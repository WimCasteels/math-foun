{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 8: Labo - Oplossingen\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 1: Basiskansrekening - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 1a - Twee dobbelstenen\n",
    "\n",
    "# Alle mogelijke uitkomsten\n",
    "outcomes = [(i, j) for i in range(1, 7) for j in range(1, 7)]\n",
    "n_total = len(outcomes)  # 36\n",
    "\n",
    "# 1. P(totaal = 7)\n",
    "sum_7 = [(i, j) for i, j in outcomes if i + j == 7]\n",
    "p_sum_7 = len(sum_7) / n_total\n",
    "print(f\"1. P(totaal = 7) = {len(sum_7)}/36 = {p_sum_7:.4f}\")\n",
    "print(f\"   Combinaties: {sum_7}\")\n",
    "\n",
    "# 2. P(totaal = 12)\n",
    "sum_12 = [(i, j) for i, j in outcomes if i + j == 12]\n",
    "p_sum_12 = len(sum_12) / n_total\n",
    "print(f\"\\n2. P(totaal = 12) = {len(sum_12)}/36 = {p_sum_12:.4f}\")\n",
    "\n",
    "# 3. P(totaal > 9)\n",
    "sum_gt_9 = [(i, j) for i, j in outcomes if i + j > 9]\n",
    "p_sum_gt_9 = len(sum_gt_9) / n_total\n",
    "print(f\"\\n3. P(totaal > 9) = {len(sum_gt_9)}/36 = {p_sum_gt_9:.4f}\")\n",
    "\n",
    "# 4. P(minstens één 6)\n",
    "at_least_one_6 = [(i, j) for i, j in outcomes if i == 6 or j == 6]\n",
    "p_at_least_6 = len(at_least_one_6) / n_total\n",
    "print(f\"\\n4. P(minstens één 6) = {len(at_least_one_6)}/36 = {p_at_least_6:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulatie verificatie\n",
    "n_sims = 100000\n",
    "die1 = np.random.randint(1, 7, n_sims)\n",
    "die2 = np.random.randint(1, 7, n_sims)\n",
    "totals = die1 + die2\n",
    "\n",
    "print(\"Simulatie verificatie (100,000 worpen):\")\n",
    "print(f\"  P(totaal = 7): theorie={p_sum_7:.4f}, sim={np.mean(totals == 7):.4f}\")\n",
    "print(f\"  P(totaal = 12): theorie={p_sum_12:.4f}, sim={np.mean(totals == 12):.4f}\")\n",
    "print(f\"  P(totaal > 9): theorie={p_sum_gt_9:.4f}, sim={np.mean(totals > 9):.4f}\")\n",
    "print(f\"  P(minstens één 6): theorie={p_at_least_6:.4f}, sim={np.mean((die1==6)|(die2==6)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 1b - Heatmap\n",
    "prob_matrix = np.ones((6, 6)) / 36\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(prob_matrix, cmap='Blues')\n",
    "plt.colorbar(label='Kans')\n",
    "\n",
    "# Annoteer met totalen\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        plt.text(j, i, f'{i+j+2}', ha='center', va='center', fontsize=12)\n",
    "\n",
    "plt.xlabel('Dobbelsteen 2', fontsize=12)\n",
    "plt.ylabel('Dobbelsteen 1', fontsize=12)\n",
    "plt.xticks(range(6), range(1, 7))\n",
    "plt.yticks(range(6), range(1, 7))\n",
    "plt.title('Totalen van twee dobbelstenen (elke cel heeft kans 1/36)', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 2: Discrete Verdelingen - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 2a - Binomiaal verdeling\n",
    "n = 100  # chips\n",
    "p = 0.02  # defectrate\n",
    "\n",
    "# 1. P(X = 2)\n",
    "p_exact_2 = stats.binom.pmf(2, n, p)\n",
    "print(f\"1. P(exact 2 defect) = {p_exact_2:.4f}\")\n",
    "\n",
    "# 2. P(X < 5) = P(X ≤ 4)\n",
    "p_less_5 = stats.binom.cdf(4, n, p)\n",
    "print(f\"2. P(minder dan 5 defect) = {p_less_5:.4f}\")\n",
    "\n",
    "# 3. P(X > 5) = 1 - P(X ≤ 5)\n",
    "p_more_5 = 1 - stats.binom.cdf(5, n, p)\n",
    "print(f\"3. P(meer dan 5 defect) = {p_more_5:.4f}\")\n",
    "\n",
    "# Visualisatie\n",
    "k = np.arange(0, 15)\n",
    "pmf = stats.binom.pmf(k, n, p)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(k, pmf, color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Aantal defecte chips', fontsize=12)\n",
    "plt.ylabel('Kans', fontsize=12)\n",
    "plt.title(f'Binomiaal(n={n}, p={p})', fontsize=14)\n",
    "plt.xticks(k)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 2b - Categorical sample\n",
    "def categorical_sample(probs, n_samples=1):\n",
    "    \"\"\"Trek samples uit een categorische verdeling.\"\"\"\n",
    "    probs = np.array(probs)\n",
    "    assert np.isclose(np.sum(probs), 1.0), \"Kansen moeten sommeren tot 1\"\n",
    "    \n",
    "    # Cumulatieve kansen\n",
    "    cumsum = np.cumsum(probs)\n",
    "    \n",
    "    # Trek uniforme random getallen\n",
    "    u = np.random.random(n_samples)\n",
    "    \n",
    "    # Vind de categorie\n",
    "    samples = np.searchsorted(cumsum, u)\n",
    "    return samples\n",
    "\n",
    "# Test\n",
    "probs = [0.1, 0.3, 0.4, 0.2]\n",
    "samples = categorical_sample(probs, 10000)\n",
    "\n",
    "print(\"Categorische verdeling test:\")\n",
    "print(f\"Theoretische kansen: {probs}\")\n",
    "print(f\"Gemeten frequenties: {[np.mean(samples == i) for i in range(4)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 3: Normale Verdeling - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 3a - IQ scores\n",
    "mu, sigma = 100, 15\n",
    "\n",
    "# 1. P(IQ > 130)\n",
    "p_above_130 = 1 - stats.norm.cdf(130, mu, sigma)\n",
    "print(f\"1. P(IQ > 130) = {p_above_130:.4f} = {p_above_130*100:.2f}%\")\n",
    "\n",
    "# 2. P(85 < IQ < 115)\n",
    "p_between = stats.norm.cdf(115, mu, sigma) - stats.norm.cdf(85, mu, sigma)\n",
    "print(f\"2. P(85 < IQ < 115) = {p_between:.4f} = {p_between*100:.2f}%\")\n",
    "\n",
    "# 3. IQ waarboven 1% scoort (99e percentiel)\n",
    "iq_99 = stats.norm.ppf(0.99, mu, sigma)\n",
    "print(f\"3. 99e percentiel IQ = {iq_99:.1f}\")\n",
    "\n",
    "# 4. Verwacht aantal met IQ > 145 uit 1000 mensen\n",
    "p_above_145 = 1 - stats.norm.cdf(145, mu, sigma)\n",
    "expected = 1000 * p_above_145\n",
    "print(f\"4. P(IQ > 145) = {p_above_145:.6f}\")\n",
    "print(f\"   Verwacht uit 1000: {expected:.2f} mensen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 3b - Normale PDF implementatie\n",
    "def normal_pdf(x, mu, sigma):\n",
    "    \"\"\"Bereken de PDF van de normale verdeling.\"\"\"\n",
    "    coefficient = 1 / np.sqrt(2 * np.pi * sigma**2)\n",
    "    exponent = -((x - mu)**2) / (2 * sigma**2)\n",
    "    return coefficient * np.exp(exponent)\n",
    "\n",
    "# Test\n",
    "x_test = np.linspace(-3, 3, 100)\n",
    "my_pdf = normal_pdf(x_test, 0, 1)\n",
    "scipy_pdf = stats.norm.pdf(x_test, 0, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_test, my_pdf, 'b-', linewidth=2, label='Mijn implementatie')\n",
    "plt.plot(x_test, scipy_pdf, 'r--', linewidth=2, label='Scipy')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Vergelijking normale PDF implementaties')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum verschil: {np.max(np.abs(my_pdf - scipy_pdf)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 4: Centrale Limietstelling - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 4a - CLT met exponentiële verdeling\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "n_samples = 10000\n",
    "lam = 1  # λ = 1\n",
    "\n",
    "for ax, n in zip(axes.flatten(), [1, 2, 5, 10, 30, 100]):\n",
    "    # Trek n exponentiële samples en neem gemiddelde\n",
    "    samples = np.random.exponential(1/lam, (n_samples, n))\n",
    "    means = np.mean(samples, axis=1)\n",
    "    \n",
    "    ax.hist(means, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    \n",
    "    # Theoretische normale benadering\n",
    "    # E[Exp(λ)] = 1/λ, Var[Exp(λ)] = 1/λ²\n",
    "    mu_theory = 1/lam\n",
    "    sigma_theory = (1/lam) / np.sqrt(n)\n",
    "    x = np.linspace(0, 3, 100)\n",
    "    ax.plot(x, stats.norm.pdf(x, mu_theory, sigma_theory), 'r-', linewidth=2)\n",
    "    \n",
    "    ax.set_title(f'n = {n}')\n",
    "    ax.set_xlim(0, 3)\n",
    "\n",
    "plt.suptitle('CLT: gemiddelde van n exponentiële variabelen', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 5: Conditionele Kans - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 5a - Spamfilter met Bayes\n",
    "P_spam = 0.3\n",
    "P_not_spam = 1 - P_spam\n",
    "P_gratis_given_spam = 0.8\n",
    "P_gratis_given_not_spam = 0.1\n",
    "\n",
    "# P(gratis) = P(gratis|spam)P(spam) + P(gratis|not spam)P(not spam)\n",
    "P_gratis = P_gratis_given_spam * P_spam + P_gratis_given_not_spam * P_not_spam\n",
    "\n",
    "# Bayes: P(spam|gratis) = P(gratis|spam)P(spam) / P(gratis)\n",
    "P_spam_given_gratis = (P_gratis_given_spam * P_spam) / P_gratis\n",
    "\n",
    "# P(not spam|gratis) = 1 - P(spam|gratis)\n",
    "P_not_spam_given_gratis = 1 - P_spam_given_gratis\n",
    "\n",
    "print(\"Spamfilter analyse:\")\n",
    "print(f\"P(gratis) = {P_gratis:.4f}\")\n",
    "print(f\"\\n1. P(spam | 'gratis') = {P_spam_given_gratis:.4f} = {P_spam_given_gratis*100:.1f}%\")\n",
    "print(f\"2. P(niet spam | 'gratis') = {P_not_spam_given_gratis:.4f} = {P_not_spam_given_gratis*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 5b - Simulatie verificatie\n",
    "n_emails = 100000\n",
    "\n",
    "# Genereer emails\n",
    "is_spam = np.random.random(n_emails) < P_spam\n",
    "\n",
    "# Genereer of email \"gratis\" bevat\n",
    "has_gratis = np.zeros(n_emails, dtype=bool)\n",
    "has_gratis[is_spam] = np.random.random(np.sum(is_spam)) < P_gratis_given_spam\n",
    "has_gratis[~is_spam] = np.random.random(np.sum(~is_spam)) < P_gratis_given_not_spam\n",
    "\n",
    "# Bereken P(spam | gratis) empirisch\n",
    "emails_with_gratis = has_gratis\n",
    "spam_with_gratis = is_spam & has_gratis\n",
    "\n",
    "P_spam_given_gratis_sim = np.sum(spam_with_gratis) / np.sum(emails_with_gratis)\n",
    "\n",
    "print(\"\\nSimulatie verificatie:\")\n",
    "print(f\"P(spam | 'gratis'): theorie={P_spam_given_gratis:.4f}, sim={P_spam_given_gratis_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 6: Softmax en Kansen - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 6a - Softmax implementatie\n",
    "def softmax(x):\n",
    "    \"\"\"Numeriek stabiele softmax.\"\"\"\n",
    "    x = np.array(x)\n",
    "    exp_x = np.exp(x - np.max(x))  # Numerieke stabiliteit\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Test met verschillende inputs\n",
    "test_inputs = [\n",
    "    [1, 2, 3],\n",
    "    [0, 0, 0],\n",
    "    [1000, 1001, 1002],  # Grote waarden\n",
    "    [-1000, -999, -998],  # Kleine waarden\n",
    "]\n",
    "\n",
    "print(\"Softmax eigenschappen:\")\n",
    "for x in test_inputs:\n",
    "    p = softmax(x)\n",
    "    print(f\"\\nInput: {x}\")\n",
    "    print(f\"Output: {p}\")\n",
    "    print(f\"  Alle positief: {np.all(p > 0)}\")\n",
    "    print(f\"  Som = {np.sum(p):.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 6b - Temperatuur effect\n",
    "logits = np.array([1.0, 2.0, 0.5, 0.3])\n",
    "temperatures = [0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, T in zip(axes, temperatures):\n",
    "    probs = softmax(logits / T)\n",
    "    ax.bar(range(len(probs)), probs, color='steelblue', edgecolor='black')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Klasse')\n",
    "    ax.set_ylabel('Kans')\n",
    "    ax.set_title(f'T = {T}')\n",
    "    ax.set_xticks(range(len(probs)))\n",
    "\n",
    "plt.suptitle(f'Softmax temperatuur effect (logits = {list(logits)})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observaties:\")\n",
    "print(\"- T < 1: Scherpere verdeling (meer confident)\")\n",
    "print(\"- T = 1: Standaard softmax\")\n",
    "print(\"- T > 1: Vlakkere verdeling (meer uniform)\")\n",
    "print(\"- T → 0: One-hot (argmax)\")\n",
    "print(\"- T → ∞: Uniform verdeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 7: Classifier Calibratie - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 7a - Calibratie analyse\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Laad MNIST\n",
    "print(\"MNIST laden...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "X, y = mnist.data[:10000] / 255.0, mnist.target[:10000].astype(int)\n",
    "X_train, X_test = X[:8000], X[8000:]\n",
    "y_train, y_test = y[:8000], y[8000:]\n",
    "\n",
    "# Train model\n",
    "print(\"Model trainen...\")\n",
    "model = MLPClassifier(hidden_layer_sizes=(128,), max_iter=20, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Verkrijg kansen\n",
    "probs = model.predict_proba(X_test)\n",
    "predictions = model.predict(X_test)\n",
    "confidence = np.max(probs, axis=1)\n",
    "correct = (predictions == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibratie plot en ECE\n",
    "n_bins = 10\n",
    "bins = np.linspace(0, 1, n_bins + 1)\n",
    "bin_indices = np.digitize(confidence, bins) - 1\n",
    "bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n",
    "\n",
    "bin_accuracies = []\n",
    "bin_confidences = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(n_bins):\n",
    "    mask = (bin_indices == i)\n",
    "    if np.sum(mask) > 0:\n",
    "        bin_accuracies.append(np.mean(correct[mask]))\n",
    "        bin_confidences.append(np.mean(confidence[mask]))\n",
    "        bin_counts.append(np.sum(mask))\n",
    "    else:\n",
    "        bin_accuracies.append(0)\n",
    "        bin_confidences.append((bins[i] + bins[i+1]) / 2)\n",
    "        bin_counts.append(0)\n",
    "\n",
    "# ECE (Expected Calibration Error)\n",
    "ece = 0\n",
    "total_samples = len(y_test)\n",
    "for i in range(n_bins):\n",
    "    if bin_counts[i] > 0:\n",
    "        ece += (bin_counts[i] / total_samples) * abs(bin_accuracies[i] - bin_confidences[i])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(bin_centers, bin_accuracies, width=0.08, alpha=0.7, label='Accuracy per bin')\n",
    "plt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect calibration')\n",
    "plt.xlabel('Confidence', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title(f'Calibratie Plot (ECE = {ece:.4f})', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Expected Calibration Error (ECE): {ece:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 8: Sampling - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 8a - Inverse transform sampling\n",
    "def sample_exponential(lam, n_samples):\n",
    "    \"\"\"Trek samples uit Exp(λ) met inverse transform sampling.\"\"\"\n",
    "    u = np.random.uniform(0, 1, n_samples)\n",
    "    # X = -ln(1-U)/λ, maar ln(U) heeft dezelfde verdeling als ln(1-U)\n",
    "    x = -np.log(u) / lam\n",
    "    return x\n",
    "\n",
    "# Test\n",
    "lam = 2\n",
    "samples = sample_exponential(lam, 10000)\n",
    "\n",
    "# Vergelijk met theoretische PDF\n",
    "x = np.linspace(0, 4, 100)\n",
    "pdf_theory = lam * np.exp(-lam * x)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(samples, bins=50, density=True, alpha=0.7, label='Samples')\n",
    "plt.plot(x, pdf_theory, 'r-', linewidth=2, label=f'Exp(λ={lam}) PDF')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Dichtheid')\n",
    "plt.title('Inverse Transform Sampling voor Exponentiële Verdeling')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Theoretisch gemiddelde: {1/lam}\")\n",
    "print(f\"Sample gemiddelde: {np.mean(samples):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonusoefening: Naive Bayes - Oplossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Leer P(klasse) en P(feature|klasse).\"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        self.n_classes = len(self.classes)\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        # P(klasse)\n",
    "        self.class_prior = np.array([np.mean(y == c) for c in self.classes])\n",
    "        \n",
    "        # P(feature=1|klasse) met Laplace smoothing\n",
    "        self.feature_prob = np.zeros((self.n_classes, self.n_features))\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            # Laplace smoothing: (count + 1) / (total + 2)\n",
    "            self.feature_prob[i] = (np.sum(X_c, axis=0) + 1) / (len(X_c) + 2)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Bereken P(klasse|features) voor elke sample.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        log_probs = np.zeros((n_samples, self.n_classes))\n",
    "        \n",
    "        for i, c in enumerate(self.classes):\n",
    "            # Log P(klasse)\n",
    "            log_prior = np.log(self.class_prior[i])\n",
    "            \n",
    "            # Log P(features|klasse) = sum of log P(feature_j|klasse)\n",
    "            # P(feature=1|klasse) als feature=1, P(feature=0|klasse) als feature=0\n",
    "            log_likelihood = np.sum(\n",
    "                X * np.log(self.feature_prob[i]) + \n",
    "                (1 - X) * np.log(1 - self.feature_prob[i]),\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            log_probs[:, i] = log_prior + log_likelihood\n",
    "        \n",
    "        # Normaliseer naar kansen\n",
    "        log_probs -= np.max(log_probs, axis=1, keepdims=True)  # Stabiliteit\n",
    "        probs = np.exp(log_probs)\n",
    "        probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Voorspel de klasse.\"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.classes[np.argmax(probs, axis=1)]\n",
    "\n",
    "# Test op binarized MNIST\n",
    "X_binary = (X > 0.5).astype(float)\n",
    "X_train_bin, X_test_bin = X_binary[:8000], X_binary[8000:]\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.fit(X_train_bin, y_train)\n",
    "predictions = nb.predict(X_test_bin)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f\"Naive Bayes accuracy op MNIST: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 8 Oplossingen | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
