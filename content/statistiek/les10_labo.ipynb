{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 10: Labo - Oefeningen\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---\n",
    "\n",
    "In dit labo oefen je met Maximum Likelihood Estimation en Cross-Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 1: MLE voor Bernoulli\n",
    "\n",
    "*Geschatte tijd: 15 minuten*\n",
    "\n",
    "### Opdracht 1a\n",
    "\n",
    "Je gooit een (mogelijk oneerlijke) munt 20 keer en krijgt 14 keer kop.\n",
    "\n",
    "1. Schrijf de likelihood functie L(p)\n",
    "2. Schrijf de log-likelihood functie\n",
    "3. Vind de MLE van p analytisch (met de hand)\n",
    "4. Verifieer numeriek door de log-likelihood te plotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code hier\n",
    "n = 20  # aantal worpen\n",
    "k = 14  # aantal keer kop\n",
    "\n",
    "# Plot de log-likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 1b\n",
    "\n",
    "Implementeer een functie die de MLE van p vindt voor willekeurige Bernoulli data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_mle(data):\n",
    "    \"\"\"\n",
    "    Vind de MLE van p voor Bernoulli data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: array van 0's en 1's\n",
    "    \n",
    "    Returns:\n",
    "    - p_mle: MLE van de succeskans\n",
    "    \"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 2: MLE voor Normale Verdeling\n",
    "\n",
    "*Geschatte tijd: 20 minuten*\n",
    "\n",
    "### Opdracht 2a\n",
    "\n",
    "Implementeer functies die de MLE van μ en σ² vinden voor normaal verdeelde data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_mle(data):\n",
    "    \"\"\"\n",
    "    Vind de MLE van μ en σ² voor normaal verdeelde data.\n",
    "    \n",
    "    Returns:\n",
    "    - mu_mle, sigma2_mle\n",
    "    \"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Test met gesimuleerde data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 2b\n",
    "\n",
    "De MLE van σ² is biased (verwacht waarde ≠ echte waarde). Demonstreer dit met simulaties en vergelijk met de unbiased schatter (met N-1 in de noemer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code hier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 3: Cross-Entropy Loss\n",
    "\n",
    "*Geschatte tijd: 20 minuten*\n",
    "\n",
    "### Opdracht 3a\n",
    "\n",
    "Implementeer de cross-entropy loss voor multi-class classificatie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Bereken cross-entropy loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_pred: (n_samples, n_classes) - softmax outputs\n",
    "    - y_true: (n_samples,) - class indices\n",
    "    \n",
    "    Returns:\n",
    "    - loss: scalar\n",
    "    \"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 3b\n",
    "\n",
    "Implementeer ook de gradiënt van cross-entropy loss naar de logits (vóór softmax).\n",
    "\n",
    "Hint: Voor softmax + cross-entropy is de gradiënt elegant: ∂L/∂z = softmax(z) - y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_gradient(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Bereken gradiënt van CE loss naar logits.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_pred: (n_samples, n_classes) - softmax outputs\n",
    "    - y_true: (n_samples,) - class indices\n",
    "    \n",
    "    Returns:\n",
    "    - gradient: (n_samples, n_classes)\n",
    "    \"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Test met numerieke gradiënt verificatie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 4: Binary Cross-Entropy\n",
    "\n",
    "*Geschatte tijd: 15 minuten*\n",
    "\n",
    "### Opdracht 4a\n",
    "\n",
    "Implementeer binary cross-entropy loss en zijn gradiënt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_pred: (n_samples,) - predicted probabilities\n",
    "    - y_true: (n_samples,) - true labels (0 or 1)\n",
    "    \"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "def binary_cross_entropy_gradient(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Gradiënt van BCE naar y_pred.\n",
    "    \"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 5: MLE = Minimaliseren van Loss\n",
    "\n",
    "*Geschatte tijd: 20 minuten*\n",
    "\n",
    "### Opdracht 5a\n",
    "\n",
    "Demonstreer dat het minimaliseren van MSE loss equivalent is aan MLE onder Gaussische aannames.\n",
    "\n",
    "Genereer regressie data en vind de optimale parameters via:\n",
    "1. Directe MSE minimalisatie\n",
    "2. MLE (maximaliseer Gaussische log-likelihood)\n",
    "\n",
    "Verifieer dat beide dezelfde parameters geven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code hier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 6: Softmax Temperature\n",
    "\n",
    "*Geschatte tijd: 15 minuten*\n",
    "\n",
    "### Opdracht 6a\n",
    "\n",
    "Implementeer softmax met temperature parameter en onderzoek het effect op:\n",
    "1. De verdeling (entropy)\n",
    "2. De gradiënten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_temperature(z, T=1.0):\n",
    "    \"\"\"\n",
    "    Softmax met temperature.\n",
    "    \"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "def entropy(probs):\n",
    "    \"\"\"\n",
    "    Bereken de entropy van een verdeling.\n",
    "    \"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Onderzoek effect van T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 7: Logistische Regressie\n",
    "\n",
    "*Geschatte tijd: 25 minuten*\n",
    "\n",
    "### Opdracht 7a\n",
    "\n",
    "Implementeer logistische regressie from scratch met gradient descent en cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        # Jouw code hier\n",
    "        pass\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        pass\n",
    "    \n",
    "    def compute_loss(self, y_true):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, X, y_true):\n",
    "        pass\n",
    "    \n",
    "    def update(self, lr):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y, lr=0.1, n_epochs=100):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        pass\n",
    "\n",
    "# Test op sklearn digits dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 8: Vergelijk Loss Functions\n",
    "\n",
    "*Geschatte tijd: 20 minuten*\n",
    "\n",
    "### Opdracht 8a\n",
    "\n",
    "Train een neuraal netwerk voor classificatie met zowel MSE als cross-entropy loss. Vergelijk:\n",
    "1. Convergentiesnelheid\n",
    "2. Finale accuracy\n",
    "3. Gradiënt stabiliteit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code hier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonusoefening: Label Smoothing\n",
    "\n",
    "*Geschatte tijd: 20 minuten*\n",
    "\n",
    "### Bonus\n",
    "\n",
    "Label smoothing is een regularisatie techniek waarbij we de harde labels (one-hot) vervangen door \"zachte\" labels:\n",
    "\n",
    "y_smooth = (1 - ε) · y_onehot + ε / K\n",
    "\n",
    "waarbij ε de smoothing factor is en K het aantal klassen.\n",
    "\n",
    "Implementeer cross-entropy loss met label smoothing en vergelijk de training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_with_label_smoothing(y_pred, y_true, n_classes, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Cross-entropy loss met label smoothing.\n",
    "    \"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Klaar!\n",
    "\n",
    "Je hebt nu de theoretische basis van loss functions begrepen. MLE verklaart waarom we cross-entropy gebruiken voor classificatie en MSE voor regressie.\n",
    "\n",
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 10 Labo | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
