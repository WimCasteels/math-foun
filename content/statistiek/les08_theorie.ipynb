{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 8: Kansrekening en Kansdichtheid\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 Recap en Motivatie\n",
    "\n",
    "In Deel 1 en 2 hebben we geleerd hoe neurale netwerken data verwerken (lineaire algebra) en hoe ze leren (calculus). We kunnen nu een netwerk trainen dat met ~97% nauwkeurigheid cijfers herkent.\n",
    "\n",
    "Maar wat betekent de output van een neuraal netwerk eigenlijk? Als we softmax gebruiken, krijgen we getallen tussen 0 en 1 die optellen tot 1. Dit zijn **kansen**! Het netwerk voorspelt de kans dat een input tot elke klasse behoort.\n",
    "\n",
    "In Deel 3 duiken we in de statistiek en kansrekening die ten grondslag ligt aan machine learning:\n",
    "\n",
    "- **Les 8**: Kansrekening - de taal van onzekerheid\n",
    "- **Les 9**: Verwachtingswaarde en variantie - samenvattende statistieken\n",
    "- **Les 10**: Maximum Likelihood - de theoretische basis van training\n",
    "\n",
    "Dit geeft ons een dieper begrip van waarom we bepaalde loss functies gebruiken en hoe we de output van een model moeten interpreteren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Leerdoelen\n",
    "\n",
    "Na deze les begrijp je de basisprincipes van kansrekening. Je kunt werken met discrete en continue kansverdelingen. Je begrijpt conditionele kans en de regel van Bayes. Je kunt de normale verdeling toepassen. Je begrijpt hoe softmax output als kansverdeling werkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Basisprincipes van Kansrekening\n",
    "\n",
    "### Wat is kans?\n",
    "\n",
    "Kans is een maat voor onzekerheid. Het geeft aan hoe waarschijnlijk een bepaalde uitkomst is. Kansen liggen altijd tussen 0 (onmogelijk) en 1 (zeker).\n",
    "\n",
    "### Kansruimte\n",
    "\n",
    "Een kansruimte bestaat uit:\n",
    "- **Uitkomstenruimte Ω**: alle mogelijke uitkomsten\n",
    "- **Gebeurtenissen**: deelverzamelingen van Ω\n",
    "- **Kansfunctie P**: wijst aan elke gebeurtenis een kans toe\n",
    "\n",
    "### Axioma's van Kolmogorov\n",
    "\n",
    "1. P(A) ≥ 0 voor elke gebeurtenis A\n",
    "2. P(Ω) = 1 (iets moet gebeuren)\n",
    "3. P(A ∪ B) = P(A) + P(B) als A en B disjunct zijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorbeeld: eerlijke dobbelsteen\n",
    "# Uitkomstenruimte Ω = {1, 2, 3, 4, 5, 6}\n",
    "\n",
    "outcomes = [1, 2, 3, 4, 5, 6]\n",
    "probabilities = [1/6] * 6  # Eerlijke dobbelsteen\n",
    "\n",
    "print(\"Eerlijke dobbelsteen:\")\n",
    "for outcome, prob in zip(outcomes, probabilities):\n",
    "    print(f\"  P(X = {outcome}) = {prob:.4f}\")\n",
    "\n",
    "print(f\"\\nSom van kansen: {sum(probabilities)}\")\n",
    "\n",
    "# Visualisatie\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(outcomes, probabilities, color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Uitkomst', fontsize=12)\n",
    "plt.ylabel('Kans', fontsize=12)\n",
    "plt.title('Kansverdeling van een eerlijke dobbelsteen', fontsize=14)\n",
    "plt.ylim(0, 0.3)\n",
    "plt.xticks(outcomes)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulatie: gooi de dobbelsteen 10000 keer\n",
    "n_throws = 10000\n",
    "throws = np.random.randint(1, 7, n_throws)\n",
    "\n",
    "# Tel frequenties\n",
    "counts = [np.sum(throws == i) for i in range(1, 7)]\n",
    "frequencies = [c / n_throws for c in counts]\n",
    "\n",
    "print(f\"Simulatie van {n_throws} worpen:\")\n",
    "for outcome, freq in zip(outcomes, frequencies):\n",
    "    print(f\"  Frequentie van {outcome}: {freq:.4f} (theoretisch: {1/6:.4f})\")\n",
    "\n",
    "# Wet van grote aantallen: frequentie nadert kans\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Cumulatieve frequentie van \"6\" na elke worp\n",
    "is_six = (throws == 6).astype(float)\n",
    "cumulative_freq = np.cumsum(is_six) / np.arange(1, n_throws + 1)\n",
    "\n",
    "plt.plot(cumulative_freq, 'b-', alpha=0.7)\n",
    "plt.axhline(y=1/6, color='r', linestyle='--', label='Theoretische kans = 1/6')\n",
    "plt.xlabel('Aantal worpen', fontsize=12)\n",
    "plt.ylabel('Frequentie van 6', fontsize=12)\n",
    "plt.title('Wet van Grote Aantallen: frequentie convergeert naar kans', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Discrete Kansverdelingen\n",
    "\n",
    "Een **discrete kansverdeling** beschrijft de kansen van een variabele die alleen bepaalde waarden kan aannemen (zoals gehele getallen).\n",
    "\n",
    "### Kansfunctie (PMF)\n",
    "\n",
    "De probability mass function P(X = x) geeft de kans dat X gelijk is aan x.\n",
    "\n",
    "Eigenschappen:\n",
    "- P(X = x) ≥ 0 voor alle x\n",
    "- Σ P(X = x) = 1\n",
    "\n",
    "### Belangrijke discrete verdelingen\n",
    "\n",
    "**Bernoulli verdeling**: één experiment met twee uitkomsten (succes/faling)\n",
    "- P(X = 1) = p, P(X = 0) = 1-p\n",
    "\n",
    "**Binomiaal verdeling**: aantal successen in n onafhankelijke Bernoulli trials\n",
    "- P(X = k) = C(n,k) · p^k · (1-p)^(n-k)\n",
    "\n",
    "**Categorische verdeling**: één experiment met K mogelijke uitkomsten\n",
    "- Dit is wat softmax output representeert!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binomiaal verdeling: aantal keer kop bij 10 muntworpen\n",
    "n = 10  # aantal worpen\n",
    "p = 0.5  # kans op kop\n",
    "\n",
    "k_values = np.arange(0, n + 1)\n",
    "binomial_probs = stats.binom.pmf(k_values, n, p)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(k_values, binomial_probs, color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Aantal keer kop (k)', fontsize=12)\n",
    "plt.ylabel('P(X = k)', fontsize=12)\n",
    "plt.title(f'Binomiaal verdeling: n={n}, p={p}', fontsize=14)\n",
    "plt.xticks(k_values)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"P(X = 5) = {stats.binom.pmf(5, n, p):.4f}\")\n",
    "print(f\"P(X ≥ 7) = {1 - stats.binom.cdf(6, n, p):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorische verdeling: softmax output!\n",
    "# Dit is precies wat een classifier voorspelt\n",
    "\n",
    "# Stel: netwerk output voor een afbeelding van een \"7\"\n",
    "logits = np.array([0.1, -0.5, 0.3, 0.2, -0.1, 0.5, -0.3, 2.5, 0.1, 0.0])\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "probs = softmax(logits)\n",
    "\n",
    "print(\"Softmax output = categorische kansverdeling:\")\n",
    "for digit, prob in enumerate(probs):\n",
    "    bar = '█' * int(prob * 50)\n",
    "    print(f\"  P(digit = {digit}) = {prob:.4f} {bar}\")\n",
    "\n",
    "print(f\"\\nSom van kansen: {np.sum(probs):.6f}\")\n",
    "print(f\"Voorspelling: {np.argmax(probs)} (hoogste kans)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisatie\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = ['steelblue'] * 10\n",
    "colors[7] = 'orangered'  # Highlight de voorspelling\n",
    "\n",
    "plt.bar(range(10), probs, color=colors, edgecolor='black')\n",
    "plt.xlabel('Digit', fontsize=12)\n",
    "plt.ylabel('Kans', fontsize=12)\n",
    "plt.title('Softmax output als categorische kansverdeling', fontsize=14)\n",
    "plt.xticks(range(10))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"De softmax output is een geldige kansverdeling:\")\n",
    "print(\"- Alle waarden ≥ 0 ✓\")\n",
    "print(\"- Som = 1 ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Continue Kansverdelingen\n",
    "\n",
    "Een **continue kansverdeling** beschrijft de kansen van een variabele die elke waarde in een interval kan aannemen.\n",
    "\n",
    "### Kansdichtheidsfunctie (PDF)\n",
    "\n",
    "Voor continue variabelen gebruiken we een probability density function f(x). De kans dat X in een interval [a, b] valt is:\n",
    "\n",
    "P(a ≤ X ≤ b) = ∫[a,b] f(x) dx\n",
    "\n",
    "Let op: f(x) zelf is geen kans! Het is een dichtheid. P(X = exact x) = 0 voor continue variabelen.\n",
    "\n",
    "### Cumulatieve Distributiefunctie (CDF)\n",
    "\n",
    "F(x) = P(X ≤ x) = ∫[-∞,x] f(t) dt\n",
    "\n",
    "De CDF geeft de kans dat X kleiner of gelijk is aan x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniforme verdeling op [0, 1]\n",
    "x = np.linspace(-0.5, 1.5, 1000)\n",
    "\n",
    "# PDF\n",
    "pdf_uniform = np.where((x >= 0) & (x <= 1), 1, 0)\n",
    "\n",
    "# CDF\n",
    "cdf_uniform = np.clip(x, 0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(x, pdf_uniform, 'b-', linewidth=2)\n",
    "axes[0].fill_between(x, pdf_uniform, alpha=0.3)\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('f(x)', fontsize=12)\n",
    "axes[0].set_title('PDF: Uniforme verdeling U(0,1)', fontsize=12)\n",
    "axes[0].set_ylim(-0.1, 1.5)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(x, cdf_uniform, 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel('F(x) = P(X ≤ x)', fontsize=12)\n",
    "axes[1].set_title('CDF: Uniforme verdeling U(0,1)', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"P(0.3 ≤ X ≤ 0.7) = 0.7 - 0.3 = 0.4\")\n",
    "print(\"Dit is de oppervlakte onder de PDF tussen 0.3 en 0.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 De Normale Verdeling\n",
    "\n",
    "De **normale verdeling** (Gaussische verdeling) is de belangrijkste verdeling in statistiek en machine learning.\n",
    "\n",
    "### Formule\n",
    "\n",
    "f(x) = (1 / √(2πσ²)) · exp(-(x-μ)² / (2σ²))\n",
    "\n",
    "waarbij:\n",
    "- μ (mu) = gemiddelde (centrum van de verdeling)\n",
    "- σ (sigma) = standaarddeviatie (breedte van de verdeling)\n",
    "\n",
    "### Waarom zo belangrijk?\n",
    "\n",
    "1. **Centrale Limietstelling**: het gemiddelde van veel onafhankelijke variabelen is normaal verdeeld\n",
    "2. **Maximum entropie**: gegeven gemiddelde en variantie, is de normale verdeling de \"meest onzekere\"\n",
    "3. **Analytisch handig**: integralen en afgeleiden zijn bekend\n",
    "\n",
    "### In Machine Learning\n",
    "\n",
    "- Weight initialisatie (vaak normaal verdeeld)\n",
    "- Gaussian noise voor regularisatie\n",
    "- Variational autoencoders\n",
    "- Gaussian processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normale verdeling met verschillende parameters\n",
    "x = np.linspace(-6, 6, 1000)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Verschillende μ en σ\n",
    "params = [(0, 1, 'Standaard: μ=0, σ=1'),\n",
    "          (0, 0.5, 'Smaller: μ=0, σ=0.5'),\n",
    "          (0, 2, 'Wider: μ=0, σ=2'),\n",
    "          (2, 1, 'Shifted: μ=2, σ=1')]\n",
    "\n",
    "for mu, sigma, label in params:\n",
    "    pdf = stats.norm.pdf(x, mu, sigma)\n",
    "    plt.plot(x, pdf, linewidth=2, label=label)\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Normale verdelingen met verschillende parameters', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De 68-95-99.7 regel\n",
    "mu, sigma = 0, 1\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "pdf = stats.norm.pdf(x, mu, sigma)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(x, pdf, 'b-', linewidth=2)\n",
    "\n",
    "# Vul gebieden in\n",
    "ax.fill_between(x, pdf, where=(x >= -1) & (x <= 1), alpha=0.3, color='green', label='68% (±1σ)')\n",
    "ax.fill_between(x, pdf, where=((x >= -2) & (x < -1)) | ((x > 1) & (x <= 2)), alpha=0.3, color='yellow', label='95% (±2σ)')\n",
    "ax.fill_between(x, pdf, where=((x >= -3) & (x < -2)) | ((x > 2) & (x <= 3)), alpha=0.3, color='red', label='99.7% (±3σ)')\n",
    "\n",
    "ax.set_xlabel('x (in standaarddeviaties)', fontsize=12)\n",
    "ax.set_ylabel('f(x)', fontsize=12)\n",
    "ax.set_title('De 68-95-99.7 regel voor de normale verdeling', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"68-95-99.7 regel:\")\n",
    "print(f\"  P(-1 ≤ X ≤ 1) = {stats.norm.cdf(1) - stats.norm.cdf(-1):.4f} ≈ 68%\")\n",
    "print(f\"  P(-2 ≤ X ≤ 2) = {stats.norm.cdf(2) - stats.norm.cdf(-2):.4f} ≈ 95%\")\n",
    "print(f\"  P(-3 ≤ X ≤ 3) = {stats.norm.cdf(3) - stats.norm.cdf(-3):.4f} ≈ 99.7%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrale Limietstelling demonstratie\n",
    "# Gemiddelde van n uniforme variabelen wordt normaal verdeeld\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "n_samples = 10000\n",
    "\n",
    "for ax, n in zip(axes.flatten(), [1, 2, 5, 10, 30, 100]):\n",
    "    # Genereer n uniforme samples en neem het gemiddelde\n",
    "    samples = np.random.uniform(0, 1, (n_samples, n))\n",
    "    means = np.mean(samples, axis=1)\n",
    "    \n",
    "    ax.hist(means, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    \n",
    "    # Theoretische normale verdeling\n",
    "    mu = 0.5  # E[U(0,1)] = 0.5\n",
    "    sigma = np.sqrt(1/12) / np.sqrt(n)  # Var[U(0,1)] = 1/12\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normale benadering')\n",
    "    \n",
    "    ax.set_title(f'n = {n}')\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "plt.suptitle('Centrale Limietstelling: gemiddelde van n uniforme variabelen', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Hoe meer variabelen we middelen, hoe meer het gemiddelde\")\n",
    "print(\"lijkt op een normale verdeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Conditionele Kans en Bayes\n",
    "\n",
    "### Conditionele kans\n",
    "\n",
    "De kans op A gegeven dat B is gebeurd:\n",
    "\n",
    "P(A|B) = P(A ∩ B) / P(B)\n",
    "\n",
    "### Regel van Bayes\n",
    "\n",
    "De regel van Bayes laat ons toe om \"om te keren\":\n",
    "\n",
    "P(A|B) = P(B|A) · P(A) / P(B)\n",
    "\n",
    "In ML termen:\n",
    "- P(A) = **prior**: wat we geloofden vóór we data zagen\n",
    "- P(B|A) = **likelihood**: hoe waarschijnlijk is de data gegeven onze hypothese\n",
    "- P(A|B) = **posterior**: wat we geloven ná het zien van de data\n",
    "\n",
    "### Waarom belangrijk voor ML?\n",
    "\n",
    "Classificatie kan gezien worden als: gegeven de input x, wat is P(klasse|x)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorbeeld: medische test\n",
    "# Een test voor een ziekte heeft:\n",
    "# - 99% sensitiviteit: P(positief | ziek) = 0.99\n",
    "# - 95% specificiteit: P(negatief | gezond) = 0.95\n",
    "# De ziekte komt voor bij 1% van de bevolking: P(ziek) = 0.01\n",
    "\n",
    "# Vraag: als je positief test, wat is de kans dat je echt ziek bent?\n",
    "# Dit is P(ziek | positief)\n",
    "\n",
    "P_ziek = 0.01\n",
    "P_gezond = 1 - P_ziek\n",
    "P_pos_gegeven_ziek = 0.99  # Sensitiviteit\n",
    "P_neg_gegeven_gezond = 0.95  # Specificiteit\n",
    "P_pos_gegeven_gezond = 1 - P_neg_gegeven_gezond  # False positive rate\n",
    "\n",
    "# P(positief) = P(pos|ziek)·P(ziek) + P(pos|gezond)·P(gezond)\n",
    "P_positief = P_pos_gegeven_ziek * P_ziek + P_pos_gegeven_gezond * P_gezond\n",
    "\n",
    "# Bayes: P(ziek|positief) = P(pos|ziek)·P(ziek) / P(positief)\n",
    "P_ziek_gegeven_pos = (P_pos_gegeven_ziek * P_ziek) / P_positief\n",
    "\n",
    "print(\"Medische test voorbeeld:\")\n",
    "print(f\"  P(ziek) = {P_ziek} (1% van bevolking)\")\n",
    "print(f\"  P(positief | ziek) = {P_pos_gegeven_ziek} (sensitiviteit)\")\n",
    "print(f\"  P(negatief | gezond) = {P_neg_gegeven_gezond} (specificiteit)\")\n",
    "print()\n",
    "print(f\"  P(positief) = {P_positief:.4f}\")\n",
    "print(f\"  P(ziek | positief) = {P_ziek_gegeven_pos:.4f} = {P_ziek_gegeven_pos*100:.1f}%\")\n",
    "print()\n",
    "print(\"Verrassend! Zelfs met een positieve test is de kans op ziekte slechts ~17%.\")\n",
    "print(\"Dit komt door de lage base rate (1% is ziek).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisatie met natuurlijke frequenties\n",
    "# Stel 10000 mensen worden getest\n",
    "\n",
    "n_people = 10000\n",
    "n_sick = int(n_people * P_ziek)  # 100 ziek\n",
    "n_healthy = n_people - n_sick    # 9900 gezond\n",
    "\n",
    "n_sick_pos = int(n_sick * P_pos_gegeven_ziek)  # 99 ziek en positief\n",
    "n_healthy_pos = int(n_healthy * P_pos_gegeven_gezond)  # 495 gezond maar positief\n",
    "\n",
    "print(f\"Van {n_people} mensen:\")\n",
    "print(f\"  {n_sick} zijn ziek, {n_healthy} zijn gezond\")\n",
    "print(f\"  {n_sick_pos} zieken testen positief\")\n",
    "print(f\"  {n_healthy_pos} gezonden testen positief (vals positief!)\")\n",
    "print(f\"  Totaal positief: {n_sick_pos + n_healthy_pos}\")\n",
    "print(f\"  Kans ziek gegeven positief: {n_sick_pos}/{n_sick_pos + n_healthy_pos} = {n_sick_pos/(n_sick_pos + n_healthy_pos):.4f}\")\n",
    "\n",
    "# Visualisatie\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "categories = ['Ziek\\nPositief', 'Ziek\\nNegatief', 'Gezond\\nPositief', 'Gezond\\nNegatief']\n",
    "counts = [n_sick_pos, n_sick - n_sick_pos, n_healthy_pos, n_healthy - n_healthy_pos]\n",
    "colors = ['red', 'lightcoral', 'orange', 'lightgreen']\n",
    "\n",
    "ax.bar(categories, counts, color=colors, edgecolor='black')\n",
    "for i, (cat, count) in enumerate(zip(categories, counts)):\n",
    "    ax.text(i, count + 100, str(count), ha='center', fontsize=11)\n",
    "\n",
    "ax.set_ylabel('Aantal mensen', fontsize=12)\n",
    "ax.set_title('Uitkomsten van 10,000 tests', fontsize=14)\n",
    "ax.set_ylim(0, 10000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Toepassing: Classifier Output als Kans\n",
    "\n",
    "In neurale netwerken voor classificatie:\n",
    "- De softmax output is een kansverdeling over klassen\n",
    "- We trainen het netwerk om P(klasse | input) te modelleren\n",
    "- Cross-entropy loss meet hoe goed deze kansverdeling past bij de werkelijke labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuleer classifier output voor MNIST\n",
    "# Goed gekalibreerde vs slecht gekalibreerde classifier\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Simuleer 1000 voorspellingen\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# True labels (uniform verdeeld over 10 klassen)\n",
    "true_labels = np.random.randint(0, 10, n_samples)\n",
    "\n",
    "# Goed model: hoge confidence voor correcte klasse\n",
    "good_logits = np.random.randn(n_samples, 10) * 0.5\n",
    "good_logits[np.arange(n_samples), true_labels] += 3  # Boost correcte klasse\n",
    "good_probs = softmax(good_logits)\n",
    "\n",
    "# Slecht model: lage confidence, meer uniform\n",
    "bad_logits = np.random.randn(n_samples, 10) * 0.5\n",
    "bad_logits[np.arange(n_samples), true_labels] += 0.5  # Kleine boost\n",
    "bad_probs = softmax(bad_logits)\n",
    "\n",
    "# Confidence van het model (max probability)\n",
    "good_confidence = np.max(good_probs, axis=1)\n",
    "bad_confidence = np.max(bad_probs, axis=1)\n",
    "\n",
    "# Correctheid\n",
    "good_correct = (np.argmax(good_probs, axis=1) == true_labels)\n",
    "bad_correct = (np.argmax(bad_probs, axis=1) == true_labels)\n",
    "\n",
    "print(f\"Goed model: {np.mean(good_correct)*100:.1f}% accuracy, gem. confidence: {np.mean(good_confidence):.3f}\")\n",
    "print(f\"Slecht model: {np.mean(bad_correct)*100:.1f}% accuracy, gem. confidence: {np.mean(bad_confidence):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration plot: is de confidence gerelateerd aan de accuracy?\n",
    "def calibration_plot(probs, labels, model_name, ax):\n",
    "    confidence = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    correct = (predictions == labels)\n",
    "    \n",
    "    # Bin confidence in 10 bins\n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    bin_indices = np.digitize(confidence, bins) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, 9)\n",
    "    \n",
    "    bin_accuracies = []\n",
    "    bin_confidences = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        mask = (bin_indices == i)\n",
    "        if np.sum(mask) > 0:\n",
    "            bin_accuracies.append(np.mean(correct[mask]))\n",
    "            bin_confidences.append(np.mean(confidence[mask]))\n",
    "        else:\n",
    "            bin_accuracies.append(0)\n",
    "            bin_confidences.append((bins[i] + bins[i+1]) / 2)\n",
    "    \n",
    "    ax.bar(np.arange(10) * 0.1 + 0.05, bin_accuracies, width=0.08, alpha=0.7, label='Accuracy')\n",
    "    ax.plot([0, 1], [0, 1], 'r--', label='Perfect calibration')\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'{model_name}')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "calibration_plot(good_probs, true_labels, 'Goed Model', axes[0])\n",
    "calibration_plot(bad_probs, true_labels, 'Slecht Model', axes[1])\n",
    "plt.suptitle('Calibratie: komt confidence overeen met accuracy?', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Een goed gekalibreerd model: als het 80% confident is, is het ~80% van de tijd correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8 Samenvatting\n",
    "\n",
    "### Kernconcepten\n",
    "\n",
    "**Kansrekening** is de wiskundige taal van onzekerheid. Kansen liggen tussen 0 en 1, en sommeren tot 1.\n",
    "\n",
    "**Discrete verdelingen** beschrijven variabelen met aftelbare uitkomsten. De categorische verdeling is precies wat softmax output representeert.\n",
    "\n",
    "**Continue verdelingen** gebruiken een dichtheidsfunctie. De normale verdeling is de belangrijkste, dankzij de centrale limietstelling.\n",
    "\n",
    "**Bayes' regel** laat ons toe om kansen te updaten op basis van nieuwe informatie. Dit is de basis van probabilistisch redeneren.\n",
    "\n",
    "### Link naar neurale netwerken\n",
    "\n",
    "- Softmax output = categorische kansverdeling\n",
    "- Het netwerk leert P(klasse | input) te modelleren\n",
    "- Calibratie: komt de confidence overeen met de werkelijke accuracy?\n",
    "\n",
    "### Volgende les\n",
    "\n",
    "In les 9 leren we over verwachtingswaarde en variantie: hoe vatten we een verdeling samen in enkele getallen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 8 van 12 | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
