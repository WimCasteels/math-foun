{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 10: Labo - Oplossingen\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 1: Likelihood Berekenen - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 1a - Bernoulli likelihood\n",
    "data = np.array([1, 1, 0, 1, 1, 1, 0, 1, 1, 1])  # 8 keer succes\n",
    "\n",
    "def bernoulli_likelihood(p, data):\n",
    "    \"\"\"L(p | data) = Π p^x_i * (1-p)^(1-x_i)\"\"\"\n",
    "    likelihood = np.prod(p**data * (1-p)**(1-data))\n",
    "    return likelihood\n",
    "\n",
    "def bernoulli_log_likelihood(p, data):\n",
    "    \"\"\"log L(p | data) = Σ [x_i*log(p) + (1-x_i)*log(1-p)]\"\"\"\n",
    "    ll = np.sum(data * np.log(p) + (1-data) * np.log(1-p))\n",
    "    return ll\n",
    "\n",
    "# Test voor p = 0.5 en p = 0.8\n",
    "print(f\"Data: {data}\")\n",
    "print(f\"Aantal successen: {np.sum(data)} van {len(data)}\")\n",
    "print(f\"\\nLikelihood(p=0.5) = {bernoulli_likelihood(0.5, data):.6f}\")\n",
    "print(f\"Likelihood(p=0.8) = {bernoulli_likelihood(0.8, data):.6f}\")\n",
    "print(f\"\\nLog-likelihood(p=0.5) = {bernoulli_log_likelihood(0.5, data):.4f}\")\n",
    "print(f\"Log-likelihood(p=0.8) = {bernoulli_log_likelihood(0.8, data):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 1b - Plot likelihood\n",
    "p_values = np.linspace(0.01, 0.99, 100)\n",
    "likelihoods = [bernoulli_likelihood(p, data) for p in p_values]\n",
    "log_likelihoods = [bernoulli_log_likelihood(p, data) for p in p_values]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Likelihood\n",
    "axes[0].plot(p_values, likelihoods, 'b-', linewidth=2)\n",
    "axes[0].axvline(x=0.8, color='red', linestyle='--', label='MLE = 0.8')\n",
    "axes[0].set_xlabel('p')\n",
    "axes[0].set_ylabel('L(p | data)')\n",
    "axes[0].set_title('Likelihood')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-likelihood\n",
    "axes[1].plot(p_values, log_likelihoods, 'b-', linewidth=2)\n",
    "axes[1].axvline(x=0.8, color='red', linestyle='--', label='MLE = 0.8')\n",
    "axes[1].set_xlabel('p')\n",
    "axes[1].set_ylabel('log L(p | data)')\n",
    "axes[1].set_title('Log-Likelihood')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"MLE: p = {np.sum(data) / len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 2: MLE voor Normale Verdeling - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 2a - Analytische MLE\n",
    "data = np.array([2.3, 4.1, 3.5, 5.2, 3.8, 4.6, 3.2, 4.8, 3.9, 4.2])\n",
    "\n",
    "# Analytische oplossingen:\n",
    "# μ_MLE = (1/n) Σ x_i = sample mean\n",
    "# σ²_MLE = (1/n) Σ (x_i - μ)² = sample variance (met n, niet n-1)\n",
    "\n",
    "mu_mle = np.mean(data)\n",
    "sigma2_mle = np.mean((data - mu_mle)**2)  # Let op: /n niet /(n-1)\n",
    "sigma_mle = np.sqrt(sigma2_mle)\n",
    "\n",
    "print(f\"Data: {data}\")\n",
    "print(f\"\\nAnalytische MLE:\")\n",
    "print(f\"  μ_MLE = {mu_mle:.4f}\")\n",
    "print(f\"  σ²_MLE = {sigma2_mle:.4f}\")\n",
    "print(f\"  σ_MLE = {sigma_mle:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 2b - Numerieke verificatie\n",
    "def neg_log_likelihood_normal(params, data):\n",
    "    mu, sigma = params\n",
    "    if sigma <= 0:\n",
    "        return np.inf\n",
    "    n = len(data)\n",
    "    nll = n/2 * np.log(2*np.pi) + n * np.log(sigma) + np.sum((data - mu)**2) / (2*sigma**2)\n",
    "    return nll\n",
    "\n",
    "result = minimize(neg_log_likelihood_normal, x0=[0, 1], args=(data,), method='Nelder-Mead')\n",
    "mu_num, sigma_num = result.x\n",
    "\n",
    "print(f\"Numerieke optimalisatie:\")\n",
    "print(f\"  μ = {mu_num:.4f}\")\n",
    "print(f\"  σ = {sigma_num:.4f}\")\n",
    "print(f\"\\nVerschil met analytisch:\")\n",
    "print(f\"  |μ_num - μ_anal| = {abs(mu_num - mu_mle):.6f}\")\n",
    "print(f\"  |σ_num - σ_anal| = {abs(sigma_num - sigma_mle):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 3: MSE = Gaussian MLE - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 3a - Bewijs dat MSE en Gaussian NLL hetzelfde minimum geven\n",
    "\n",
    "# Genereer data\n",
    "np.random.seed(42)\n",
    "n = 30\n",
    "x = np.linspace(0, 10, n)\n",
    "true_w, true_b = 2, 1\n",
    "y = true_w * x + true_b + np.random.normal(0, 1, n)\n",
    "\n",
    "def mse_loss(params, x, y):\n",
    "    w, b = params\n",
    "    pred = w * x + b\n",
    "    return np.mean((y - pred)**2)\n",
    "\n",
    "def gaussian_nll(params, x, y, sigma=1):\n",
    "    w, b = params\n",
    "    pred = w * x + b\n",
    "    n = len(y)\n",
    "    nll = n/2 * np.log(2*np.pi*sigma**2) + np.sum((y - pred)**2) / (2*sigma**2)\n",
    "    return nll\n",
    "\n",
    "# Optimaliseer beide\n",
    "result_mse = minimize(mse_loss, x0=[0, 0], args=(x, y))\n",
    "result_nll = minimize(gaussian_nll, x0=[0, 0], args=(x, y))\n",
    "\n",
    "print(f\"True: w = {true_w}, b = {true_b}\")\n",
    "print(f\"\\nMSE optimum: w = {result_mse.x[0]:.4f}, b = {result_mse.x[1]:.4f}\")\n",
    "print(f\"NLL optimum: w = {result_nll.x[0]:.4f}, b = {result_nll.x[1]:.4f}\")\n",
    "print(f\"\\nZe zijn identiek omdat MSE ∝ Gaussian NLL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 4: Cross-Entropy = Categorical MLE - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 4a - Bewijs equivalentie\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=-1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(logits, y_true):\n",
    "    \"\"\"Cross-entropy: -Σ y_k * log(p_k)\"\"\"\n",
    "    probs = softmax(logits)\n",
    "    n = len(y_true)\n",
    "    return -np.mean(np.log(probs[np.arange(n), y_true] + 1e-10))\n",
    "\n",
    "def categorical_nll(logits, y_true):\n",
    "    \"\"\"NLL: -Σ log p(y_i | x_i)\"\"\"\n",
    "    probs = softmax(logits)\n",
    "    n = len(y_true)\n",
    "    # p(y | x) voor categorische verdeling = p_k waar k de juiste klasse is\n",
    "    return -np.mean(np.log(probs[np.arange(n), y_true] + 1e-10))\n",
    "\n",
    "# Test\n",
    "np.random.seed(42)\n",
    "logits = np.random.randn(5, 3)  # 5 samples, 3 klassen\n",
    "y_true = np.array([0, 1, 2, 1, 0])\n",
    "\n",
    "ce = cross_entropy_loss(logits, y_true)\n",
    "nll = categorical_nll(logits, y_true)\n",
    "\n",
    "print(f\"Cross-Entropy: {ce:.6f}\")\n",
    "print(f\"Categorical NLL: {nll:.6f}\")\n",
    "print(f\"\\nZe zijn identiek!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 5: MLE voor Classifier - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 5a - Logistische regressie via MLE\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Data\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
    "                           n_informative=2, n_clusters_per_class=1,\n",
    "                           random_state=42)\n",
    "\n",
    "class LogisticRegressionMLE:\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def negative_log_likelihood(self, params, X, y):\n",
    "        W = params[:-1]\n",
    "        b = params[-1]\n",
    "        z = X @ W + b\n",
    "        p = self.sigmoid(z)\n",
    "        # NLL = -Σ [y*log(p) + (1-y)*log(1-p)]\n",
    "        nll = -np.mean(y * np.log(p + 1e-10) + (1-y) * np.log(1-p + 1e-10))\n",
    "        return nll\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        initial = np.zeros(n_features + 1)\n",
    "        result = minimize(self.negative_log_likelihood, initial, args=(X, y), method='BFGS')\n",
    "        self.W = result.x[:-1]\n",
    "        self.b = result.x[-1]\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(X @ self.W + self.b)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n",
    "\n",
    "# Train\n",
    "model = LogisticRegressionMLE()\n",
    "model.fit(X, y)\n",
    "\n",
    "accuracy = np.mean(model.predict(X) == y)\n",
    "print(f\"Training accuracy: {accuracy:.4f}\")\n",
    "print(f\"Parameters: W = {model.W}, b = {model.b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(X[:,0].min()-1, X[:,0].max()+1, 100),\n",
    "                     np.linspace(X[:,1].min()-1, X[:,1].max()+1, 100))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = model.predict_proba(grid).reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.7)\n",
    "plt.colorbar(label='P(y=1)')\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', edgecolors='k', label='Class 0')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='red', edgecolors='k', label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Logistische Regressie via MLE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 6: L2 Regularisatie als Prior - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 6a - Ridge regressie = MAP met Gaussian prior\n",
    "\n",
    "# Data met noise\n",
    "np.random.seed(42)\n",
    "n = 15\n",
    "x = np.linspace(0, 1, n)\n",
    "y_true = np.sin(2 * np.pi * x)\n",
    "y = y_true + np.random.normal(0, 0.3, n)\n",
    "\n",
    "# Polynomiale features\n",
    "degree = 9\n",
    "X = np.column_stack([x**i for i in range(degree + 1)])\n",
    "\n",
    "def ridge_map(X, y, lam):\n",
    "    \"\"\"MAP met Gaussian prior = Ridge regression.\"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    I = np.eye(n_features)\n",
    "    I[0, 0] = 0  # Don't regularize bias\n",
    "    w = np.linalg.solve(X.T @ X + lam * I, X.T @ y)\n",
    "    return w\n",
    "\n",
    "# Vergelijk verschillende λ\n",
    "lambdas = [0, 1e-6, 1e-3, 1]\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "X_plot = np.column_stack([x_plot**i for i in range(degree + 1)])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, lam in zip(axes.flatten(), lambdas):\n",
    "    w = ridge_map(X, y, lam)\n",
    "    y_pred = X_plot @ w\n",
    "    \n",
    "    ax.scatter(x, y, color='blue', s=50, label='Data')\n",
    "    ax.plot(x_plot, y_pred, 'r-', linewidth=2, label='Fit')\n",
    "    ax.plot(x_plot, np.sin(2*np.pi*x_plot), 'g--', alpha=0.5, label='True')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'λ = {lam}')\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Ridge Regression = MAP met Gaussian Prior', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 7: MLE vs MAP - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 7a - Vergelijk MLE en MAP\n",
    "\n",
    "# Klein dataset → overfitting risico\n",
    "np.random.seed(42)\n",
    "n_train = 10\n",
    "n_test = 100\n",
    "\n",
    "x_train = np.random.uniform(0, 1, n_train)\n",
    "y_train = np.sin(2*np.pi*x_train) + np.random.normal(0, 0.3, n_train)\n",
    "\n",
    "x_test = np.linspace(0, 1, n_test)\n",
    "y_test = np.sin(2*np.pi*x_test)\n",
    "\n",
    "degree = 9\n",
    "X_train = np.column_stack([x_train**i for i in range(degree + 1)])\n",
    "X_test = np.column_stack([x_test**i for i in range(degree + 1)])\n",
    "\n",
    "# MLE (λ = 0)\n",
    "w_mle = np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n",
    "y_mle = X_test @ w_mle\n",
    "mse_mle = np.mean((y_test - y_mle)**2)\n",
    "\n",
    "# MAP (λ = 0.01)\n",
    "lam = 0.01\n",
    "I = np.eye(degree + 1)\n",
    "I[0, 0] = 0\n",
    "w_map = np.linalg.solve(X_train.T @ X_train + lam * I, X_train.T @ y_train)\n",
    "y_map = X_test @ w_map\n",
    "mse_map = np.mean((y_test - y_map)**2)\n",
    "\n",
    "print(f\"Test MSE (MLE): {mse_mle:.4f}\")\n",
    "print(f\"Test MSE (MAP): {mse_map:.4f}\")\n",
    "print(f\"\\nMAP is beter door regularisatie!\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_train, y_train, color='blue', s=80, label='Training data', zorder=5)\n",
    "plt.plot(x_test, y_test, 'g--', linewidth=2, label='True function')\n",
    "plt.plot(x_test, y_mle, 'r-', linewidth=2, label=f'MLE (MSE={mse_mle:.3f})')\n",
    "plt.plot(x_test, y_map, 'b-', linewidth=2, label=f'MAP (MSE={mse_map:.3f})')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('MLE vs MAP: Regularisatie voorkomt overfitting')\n",
    "plt.legend()\n",
    "plt.ylim(-2, 2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonusoefening: Fisher Information - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus - Fisher information voor Bernoulli\n",
    "# I(p) = E[(d/dp log p(x|p))²] = 1 / (p(1-p))\n",
    "\n",
    "# De variance van de MLE schatter is asymptotisch 1/I(p)\n",
    "# Dus Var(p_MLE) ≈ p(1-p) / n\n",
    "\n",
    "def fisher_information_bernoulli(p):\n",
    "    return 1 / (p * (1 - p))\n",
    "\n",
    "# Simuleer om te verifiëren\n",
    "true_p = 0.7\n",
    "sample_sizes = [10, 50, 100, 500, 1000]\n",
    "n_experiments = 1000\n",
    "\n",
    "print(\"Verificatie: Var(p_MLE) ≈ p(1-p)/n\")\n",
    "print(f\"True p = {true_p}\")\n",
    "print(f\"Theoretical factor p(1-p) = {true_p * (1-true_p):.4f}\")\n",
    "print()\n",
    "\n",
    "for n in sample_sizes:\n",
    "    estimates = []\n",
    "    for _ in range(n_experiments):\n",
    "        data = np.random.binomial(1, true_p, n)\n",
    "        p_mle = np.mean(data)\n",
    "        estimates.append(p_mle)\n",
    "    \n",
    "    var_empirical = np.var(estimates)\n",
    "    var_theoretical = true_p * (1 - true_p) / n\n",
    "    \n",
    "    print(f\"n={n:4d}: Var(empirical)={var_empirical:.6f}, Var(theory)={var_theoretical:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer Fisher information\n",
    "p_values = np.linspace(0.01, 0.99, 100)\n",
    "fisher_info = fisher_information_bernoulli(p_values)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(p_values, fisher_info, 'b-', linewidth=2)\n",
    "plt.xlabel('p', fontsize=12)\n",
    "plt.ylabel('Fisher Information I(p)', fontsize=12)\n",
    "plt.title('Fisher Information voor Bernoulli: I(p) = 1/(p(1-p))', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Fisher information is hoog bij p ≈ 0 of p ≈ 1\")\n",
    "print(\"Dit betekent dat de MLE schatter daar preciezer is.\")\n",
    "print(\"Bij p = 0.5 is Fisher info minimaal → meeste onzekerheid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 10 Oplossingen | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
