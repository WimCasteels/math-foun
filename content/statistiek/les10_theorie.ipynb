{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 10: Maximum Likelihood en Cross-Entropy\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.0 Recap en Motivatie\n",
    "\n",
    "We hebben nu alle bouwstenen:\n",
    "- **Lineaire algebra**: hoe data door een netwerk stroomt\n",
    "- **Calculus**: hoe we gradiënten berekenen en optimaliseren\n",
    "- **Kansrekening**: hoe we outputs interpreteren als kansen\n",
    "\n",
    "Nu verbinden we alles. De centrale vraag is: **waarom gebruiken we cross-entropy loss?**\n",
    "\n",
    "Het antwoord komt uit de statistiek: **Maximum Likelihood Estimation (MLE)**. We trainen een neuraal netwerk om de parameters te vinden die de waargenomen data het meest waarschijnlijk maken.\n",
    "\n",
    "En het blijkt dat het maximaliseren van de likelihood wiskundig equivalent is aan het minimaliseren van de cross-entropy loss!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Leerdoelen\n",
    "\n",
    "Na deze les begrijp je het principe van Maximum Likelihood Estimation. Je begrijpt de link tussen likelihood en cross-entropy. Je kunt uitleggen waarom we cross-entropy gebruiken voor classificatie. Je begrijpt de link tussen MSE en Gaussische likelihood. Je kunt de theoretische basis van loss functions uitleggen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Maximum Likelihood Estimation\n",
    "\n",
    "### Het idee\n",
    "\n",
    "Gegeven data D en een model met parameters θ, zoek de parameters die de data het meest waarschijnlijk maken:\n",
    "\n",
    "θ* = argmax_θ P(D | θ)\n",
    "\n",
    "De functie P(D | θ) noemen we de **likelihood**.\n",
    "\n",
    "### Voorbeeld: Schat de kans op \"kop\"\n",
    "\n",
    "Stel je gooit een munt 10 keer en krijgt 7 keer kop. Wat is de meest waarschijnlijke waarde van p (kans op kop)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: 7 keer kop, 3 keer munt uit 10 worpen\n",
    "n = 10\n",
    "k = 7  # Aantal keer kop\n",
    "\n",
    "# Likelihood functie: P(k kop | p) = C(n,k) * p^k * (1-p)^(n-k)\n",
    "def likelihood(p, k, n):\n",
    "    return stats.binom.pmf(k, n, p)\n",
    "\n",
    "# Plot de likelihood voor verschillende waarden van p\n",
    "p_values = np.linspace(0.01, 0.99, 100)\n",
    "likelihoods = [likelihood(p, k, n) for p in p_values]\n",
    "\n",
    "# Vind het maximum\n",
    "p_mle = k / n  # Analytische oplossing voor binomiaal\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(p_values, likelihoods, 'b-', linewidth=2)\n",
    "plt.axvline(x=p_mle, color='r', linestyle='--', label=f'MLE: p = {p_mle}')\n",
    "plt.xlabel('p (kans op kop)', fontsize=12)\n",
    "plt.ylabel('Likelihood P(data | p)', fontsize=12)\n",
    "plt.title(f'Likelihood functie voor {k} kop uit {n} worpen', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum Likelihood Estimate: p = {p_mle}\")\n",
    "print(f\"Dit is simpelweg k/n = {k}/{n}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Likelihood\n",
    "\n",
    "In de praktijk werken we met de **log-likelihood** omdat:\n",
    "1. Producten worden sommen (makkelijker te berekenen)\n",
    "2. Numeriek stabieler (geen underflow)\n",
    "3. Log is monotoon, dus maximum blijft hetzelfde\n",
    "\n",
    "log P(D | θ) = Σᵢ log P(xᵢ | θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-likelihood\n",
    "def log_likelihood(p, k, n):\n",
    "    if p <= 0 or p >= 1:\n",
    "        return -np.inf\n",
    "    return k * np.log(p) + (n - k) * np.log(1 - p)\n",
    "\n",
    "log_likelihoods = [log_likelihood(p, k, n) for p in p_values]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(p_values, log_likelihoods, 'b-', linewidth=2)\n",
    "plt.axvline(x=p_mle, color='r', linestyle='--', label=f'MLE: p = {p_mle}')\n",
    "plt.xlabel('p (kans op kop)', fontsize=12)\n",
    "plt.ylabel('Log-Likelihood', fontsize=12)\n",
    "plt.title('Log-Likelihood functie', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 MLE voor een Normaal Verdeeld Model\n",
    "\n",
    "Stel we hebben data {x₁, x₂, ..., xₙ} die we modelleren als N(μ, σ²). Wat zijn de MLE's van μ en σ²?\n",
    "\n",
    "De log-likelihood is:\n",
    "\n",
    "log L = -n/2 · log(2πσ²) - 1/(2σ²) · Σᵢ(xᵢ - μ)²\n",
    "\n",
    "Door de afgeleiden naar μ en σ² gelijk aan nul te stellen, krijgen we:\n",
    "- μ_MLE = (1/n) · Σᵢ xᵢ = steekproefgemiddelde\n",
    "- σ²_MLE = (1/n) · Σᵢ(xᵢ - μ)² = steekproefvariantie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereer data uit N(5, 2²)\n",
    "true_mu = 5\n",
    "true_sigma = 2\n",
    "n_samples = 100\n",
    "\n",
    "data = np.random.normal(true_mu, true_sigma, n_samples)\n",
    "\n",
    "# MLE schattingen\n",
    "mu_mle = np.mean(data)\n",
    "sigma2_mle = np.var(data)  # N in de noemer (MLE), niet N-1\n",
    "sigma_mle = np.sqrt(sigma2_mle)\n",
    "\n",
    "print(f\"Werkelijke waarden: μ = {true_mu}, σ = {true_sigma}\")\n",
    "print(f\"MLE schattingen: μ = {mu_mle:.4f}, σ = {sigma_mle:.4f}\")\n",
    "\n",
    "# Visualisatie\n",
    "x = np.linspace(data.min() - 2, data.max() + 2, 100)\n",
    "pdf_true = stats.norm.pdf(x, true_mu, true_sigma)\n",
    "pdf_mle = stats.norm.pdf(x, mu_mle, sigma_mle)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(data, bins=20, density=True, alpha=0.5, label='Data')\n",
    "plt.plot(x, pdf_true, 'g-', linewidth=2, label=f'Werkelijk: N({true_mu}, {true_sigma}²)')\n",
    "plt.plot(x, pdf_mle, 'r--', linewidth=2, label=f'MLE: N({mu_mle:.2f}, {sigma_mle:.2f}²)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Dichtheid')\n",
    "plt.title('MLE voor Normale Verdeling')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Van MLE naar Cross-Entropy Loss\n",
    "\n",
    "### Classificatie als MLE\n",
    "\n",
    "In classificatie modelleert ons netwerk P(y | x; θ), de kans op klasse y gegeven input x en parameters θ.\n",
    "\n",
    "Voor een dataset {(x₁, y₁), ..., (xₙ, yₙ)} is de likelihood:\n",
    "\n",
    "L(θ) = ∏ᵢ P(yᵢ | xᵢ; θ)\n",
    "\n",
    "En de log-likelihood:\n",
    "\n",
    "log L(θ) = Σᵢ log P(yᵢ | xᵢ; θ)\n",
    "\n",
    "### Negative Log-Likelihood = Cross-Entropy\n",
    "\n",
    "Maximaliseren van log-likelihood is equivalent aan minimaliseren van de **negative log-likelihood (NLL)**:\n",
    "\n",
    "NLL = -Σᵢ log P(yᵢ | xᵢ; θ)\n",
    "\n",
    "Dit is precies de **cross-entropy loss**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstratie: NLL = Cross-Entropy\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=-1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "\n",
    "# Stel: 4-klasse classificatie, 5 samples\n",
    "n_samples = 5\n",
    "n_classes = 4\n",
    "\n",
    "# True labels (one-hot)\n",
    "y_true = np.array([0, 2, 1, 3, 0])\n",
    "\n",
    "# Model voorspellingen (softmax outputs)\n",
    "logits = np.random.randn(n_samples, n_classes)\n",
    "y_pred = softmax(logits)\n",
    "\n",
    "print(\"Softmax outputs (P(klasse | x)):\")\n",
    "for i in range(n_samples):\n",
    "    print(f\"  Sample {i}: {y_pred[i]} → True label: {y_true[i]}\")\n",
    "print()\n",
    "\n",
    "# Negative Log-Likelihood\n",
    "nll = 0\n",
    "for i in range(n_samples):\n",
    "    prob_correct = y_pred[i, y_true[i]]\n",
    "    nll -= np.log(prob_correct + 1e-10)\n",
    "    print(f\"  Sample {i}: P(y={y_true[i]}) = {prob_correct:.4f}, -log(P) = {-np.log(prob_correct):.4f}\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood = {nll:.4f}\")\n",
    "print(f\"Cross-Entropy Loss (gemiddeld) = {nll/n_samples:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Cross-Entropy Loss in Detail\n",
    "\n",
    "### Definitie\n",
    "\n",
    "De cross-entropy tussen de ware verdeling p en de voorspelde verdeling q is:\n",
    "\n",
    "H(p, q) = -Σᵢ pᵢ · log(qᵢ)\n",
    "\n",
    "### Voor classificatie\n",
    "\n",
    "De ware verdeling p is een one-hot vector (1 bij de juiste klasse, 0 elders). Dus:\n",
    "\n",
    "H(p, q) = -log(q_juiste_klasse)\n",
    "\n",
    "Dit is precies wat we hierboven berekenden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss implementatie\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Bereken cross-entropy loss.\n",
    "    y_pred: (n_samples, n_classes) - softmax outputs\n",
    "    y_true: (n_samples,) - class indices\n",
    "    \"\"\"\n",
    "    n_samples = len(y_true)\n",
    "    # Pak de voorspelde kans voor de juiste klasse\n",
    "    probs_correct = y_pred[np.arange(n_samples), y_true]\n",
    "    # Negative log\n",
    "    loss = -np.mean(np.log(probs_correct + 1e-10))\n",
    "    return loss\n",
    "\n",
    "# Test\n",
    "loss = cross_entropy_loss(y_pred, y_true)\n",
    "print(f\"Cross-Entropy Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer: hoe verandert de loss met de voorspelde kans?\n",
    "\n",
    "p_correct = np.linspace(0.01, 0.99, 100)\n",
    "loss_values = -np.log(p_correct)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(p_correct, loss_values, 'b-', linewidth=2)\n",
    "plt.xlabel('P(juiste klasse)', fontsize=12)\n",
    "plt.ylabel('Loss = -log(P)', fontsize=12)\n",
    "plt.title('Cross-Entropy Loss als functie van voorspelde kans', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Markeer enkele punten\n",
    "for p in [0.1, 0.5, 0.9]:\n",
    "    plt.scatter([p], [-np.log(p)], s=100, zorder=5)\n",
    "    plt.annotate(f'P={p}, Loss={-np.log(p):.2f}', (p, -np.log(p)), \n",
    "                 textcoords='offset points', xytext=(10,10), fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Observaties:\")\n",
    "print(\"  - Loss → 0 als P(juist) → 1 (perfecte voorspelling)\")\n",
    "print(\"  - Loss → ∞ als P(juist) → 0 (foute voorspelling)\")\n",
    "print(\"  - De loss is asymmetrisch: foute voorspellingen worden zwaar bestraft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Binaire Cross-Entropy\n",
    "\n",
    "Voor binaire classificatie (twee klassen) vereenvoudigt cross-entropy tot:\n",
    "\n",
    "BCE = -[y · log(p) + (1-y) · log(1-p)]\n",
    "\n",
    "waarbij:\n",
    "- y ∈ {0, 1} is het echte label\n",
    "- p ∈ [0, 1] is de voorspelde kans op klasse 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy visualisatie\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -(y_true * np.log(y_pred + 1e-10) + (1 - y_true) * np.log(1 - y_pred + 1e-10))\n",
    "\n",
    "p = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# y = 1 (positieve klasse)\n",
    "loss_y1 = binary_cross_entropy(1, p)\n",
    "axes[0].plot(p, loss_y1, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Voorspelde P(y=1)', fontsize=12)\n",
    "axes[0].set_ylabel('BCE Loss', fontsize=12)\n",
    "axes[0].set_title('True label y = 1', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# y = 0 (negatieve klasse)\n",
    "loss_y0 = binary_cross_entropy(0, p)\n",
    "axes[1].plot(p, loss_y0, 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Voorspelde P(y=1)', fontsize=12)\n",
    "axes[1].set_ylabel('BCE Loss', fontsize=12)\n",
    "axes[1].set_title('True label y = 0', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretatie:\")\n",
    "print(\"  Links (y=1): Loss laag als we hoge P(y=1) voorspellen\")\n",
    "print(\"  Rechts (y=0): Loss laag als we lage P(y=1) voorspellen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 MSE Loss en Gaussische Likelihood\n",
    "\n",
    "Voor regressie gebruiken we vaak Mean Squared Error (MSE). Dit komt ook uit MLE!\n",
    "\n",
    "Als we aannemen dat de data normaal verdeeld is rond de voorspelling:\n",
    "\n",
    "y ~ N(f(x; θ), σ²)\n",
    "\n",
    "Dan is de negative log-likelihood:\n",
    "\n",
    "NLL = (1/2σ²) · Σᵢ(yᵢ - f(xᵢ; θ))² + constante\n",
    "\n",
    "Dit is proportioneel aan de **MSE loss**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstratie: MSE = NLL onder Gaussische aanname\n",
    "\n",
    "# Genereer regressie data\n",
    "np.random.seed(42)\n",
    "n_points = 50\n",
    "X = np.linspace(0, 10, n_points)\n",
    "y_true = 2 * X + 3  # Echte relatie\n",
    "noise_std = 2\n",
    "y = y_true + np.random.normal(0, noise_std, n_points)\n",
    "\n",
    "# Simpel lineair model: y_pred = w*x + b\n",
    "# Vind MLE (= minimaliseer MSE)\n",
    "\n",
    "def mse_loss(params, X, y):\n",
    "    w, b = params\n",
    "    y_pred = w * X + b\n",
    "    return np.mean((y - y_pred)**2)\n",
    "\n",
    "def gaussian_nll(params, X, y, sigma=1):\n",
    "    w, b = params\n",
    "    y_pred = w * X + b\n",
    "    n = len(y)\n",
    "    nll = (1/(2*sigma**2)) * np.sum((y - y_pred)**2) + (n/2) * np.log(2*np.pi*sigma**2)\n",
    "    return nll\n",
    "\n",
    "# Grid search voor w en b\n",
    "w_range = np.linspace(0, 4, 50)\n",
    "b_range = np.linspace(0, 6, 50)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "\n",
    "MSE = np.zeros_like(W)\n",
    "NLL = np.zeros_like(W)\n",
    "\n",
    "for i in range(len(b_range)):\n",
    "    for j in range(len(w_range)):\n",
    "        MSE[i,j] = mse_loss([W[i,j], B[i,j]], X, y)\n",
    "        NLL[i,j] = gaussian_nll([W[i,j], B[i,j]], X, y, sigma=noise_std)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "c1 = axes[0].contour(W, B, MSE, levels=20)\n",
    "axes[0].clabel(c1, inline=True, fontsize=8)\n",
    "axes[0].scatter([2], [3], color='red', s=100, marker='*', label='Optimum', zorder=5)\n",
    "axes[0].set_xlabel('w')\n",
    "axes[0].set_ylabel('b')\n",
    "axes[0].set_title('MSE Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "c2 = axes[1].contour(W, B, NLL, levels=20)\n",
    "axes[1].clabel(c2, inline=True, fontsize=8)\n",
    "axes[1].scatter([2], [3], color='red', s=100, marker='*', label='Optimum', zorder=5)\n",
    "axes[1].set_xlabel('w')\n",
    "axes[1].set_ylabel('b')\n",
    "axes[1].set_title('Gaussian NLL')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE en Gaussian NLL hebben dezelfde optimale parameters!\")\n",
    "print(\"Ze zijn proportioneel aan elkaar (verschillen alleen in schaal en constante).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Vergelijking: MSE vs Cross-Entropy\n",
    "\n",
    "| Eigenschap | MSE Loss | Cross-Entropy Loss |\n",
    "|------------|----------|--------------------|\n",
    "| Aanname | Gaussische ruis | Categorische verdeling |\n",
    "| Gebruik | Regressie | Classificatie |\n",
    "| Output | Continu | Kansen (softmax) |\n",
    "| Gradiënt | Proportioneel aan fout | Sterker voor foute voorspellingen |\n",
    "| MLE basis | N(y_pred, σ²) | Categorisch(softmax(z)) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergelijk gradiënten\n",
    "\n",
    "# Voor classificatie: vergelijk MSE vs CE gradiënt\n",
    "# Bij MSE: ∂L/∂z ∝ (y_pred - y_true)\n",
    "# Bij CE: ∂L/∂z = y_pred - y_true (voor softmax + CE)\n",
    "\n",
    "# Stel: true label is klasse 0, model voorspelt [p, 1-p]\n",
    "p_values = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# MSE gradient (vereenvoudigd)\n",
    "mse_grad = 2 * (p_values - 1)  # Als we p willen verhogen naar 1\n",
    "\n",
    "# CE gradient\n",
    "ce_grad = p_values - 1  # softmax + CE geeft dit elegante resultaat\n",
    "\n",
    "# -log(p) derivative\n",
    "nll_grad = -1 / p_values  # Gradiënt van -log(p) naar p\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(p_values, np.abs(mse_grad), 'b-', linewidth=2, label='|MSE gradient|')\n",
    "plt.plot(p_values, np.abs(ce_grad), 'r-', linewidth=2, label='|CE gradient| (softmax)')\n",
    "plt.xlabel('Voorspelde kans voor juiste klasse', fontsize=12)\n",
    "plt.ylabel('|Gradiënt|', fontsize=12)\n",
    "plt.title('Gradiënt sterkte: MSE vs Cross-Entropy', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observatie:\")\n",
    "print(\"  CE met softmax geeft een constante gradiënt ongeacht hoe zeker het model is.\")\n",
    "print(\"  Dit zorgt voor stabielere training bij classificatie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9 Praktische Implementatie\n",
    "\n",
    "Laten we een complete classificatie pipeline implementeren met cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete classificatie met CE loss\n",
    "\n",
    "class LogisticRegression:\n",
    "    \"\"\"Logistische regressie met gradient descent.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, n_classes):\n",
    "        self.W = np.random.randn(n_features, n_classes) * 0.01\n",
    "        self.b = np.zeros(n_classes)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z = X @ self.W + self.b\n",
    "        self.probs = self.softmax(self.z)\n",
    "        return self.probs\n",
    "    \n",
    "    def compute_loss(self, y_true):\n",
    "        n = len(y_true)\n",
    "        log_probs = np.log(self.probs[np.arange(n), y_true] + 1e-10)\n",
    "        return -np.mean(log_probs)\n",
    "    \n",
    "    def backward(self, X, y_true):\n",
    "        n = len(y_true)\n",
    "        # Gradiënt van softmax + CE: y_pred - y_true\n",
    "        dz = self.probs.copy()\n",
    "        dz[np.arange(n), y_true] -= 1\n",
    "        dz /= n\n",
    "        \n",
    "        self.dW = X.T @ dz\n",
    "        self.db = np.sum(dz, axis=0)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.W -= lr * self.dW\n",
    "        self.b -= lr * self.db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test op MNIST (subset)\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Normaliseer\n",
    "X = X / 16.0\n",
    "\n",
    "# Split\n",
    "n_train = 1200\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "print(f\"Training: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Train\n",
    "model = LogisticRegression(n_features=64, n_classes=10)\n",
    "\n",
    "lr = 1.0\n",
    "n_epochs = 100\n",
    "losses = []\n",
    "accs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.forward(X_train)\n",
    "    loss = model.compute_loss(y_train)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    model.backward(X_train, y_train)\n",
    "    model.update(lr)\n",
    "    \n",
    "    acc = model.accuracy(X_test, y_test)\n",
    "    accs.append(acc)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss:.4f}, Test Acc = {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nFinale test accuracy: {accs[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Cross-Entropy Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(accs, 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('Test Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.10 Samenvatting\n",
    "\n",
    "### Kernconcepten\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE)** vindt de parameters die de waargenomen data het meest waarschijnlijk maken. We maximaliseren P(data | parameters).\n",
    "\n",
    "**Log-likelihood** wordt gebruikt om producten te vervangen door sommen. Maximaliseren van log-likelihood = minimaliseren van negative log-likelihood.\n",
    "\n",
    "**Cross-Entropy Loss = Negative Log-Likelihood** voor classificatie. Het volgt direct uit het MLE principe toegepast op categorische verdelingen.\n",
    "\n",
    "**MSE Loss = Gaussian NLL** voor regressie. Het volgt uit MLE onder de aanname van normaal verdeelde ruis.\n",
    "\n",
    "### De grote lijn\n",
    "\n",
    "We hebben nu het complete plaatje:\n",
    "\n",
    "1. **Forward pass** (lineaire algebra): data → logits → softmax → kansen\n",
    "2. **Loss** (kansrekening): cross-entropy meet hoe goed de voorspelde verdeling is\n",
    "3. **Backward pass** (calculus): bereken gradiënten via backprop\n",
    "4. **Update** (optimalisatie): gradient descent past parameters aan\n",
    "\n",
    "### Einde Deel 3\n",
    "\n",
    "Met deze les sluiten we de theoretische basis af. We hebben de wiskundige fundamenten van neurale netwerken volledig behandeld. In Deel 4 passen we alles toe in praktische projecten!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 10 van 12 | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
