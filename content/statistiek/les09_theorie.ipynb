{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 9: Verwachtingswaarde en Variantie\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.0 Recap en Motivatie\n",
    "\n",
    "In les 8 hebben we kansverdelingen geleerd: manieren om onzekerheid te beschrijven. Maar een volledige verdeling kan complex zijn. Vaak willen we een verdeling samenvatten in slechts enkele getallen.\n",
    "\n",
    "De twee belangrijkste samenvattende maten zijn:\n",
    "- **Verwachtingswaarde (gemiddelde)**: het \"centrum\" van de verdeling\n",
    "- **Variantie (spreiding)**: hoe breed de verdeling is\n",
    "\n",
    "In machine learning zijn deze concepten overal:\n",
    "- Batch normalization gebruikt gemiddelde en variantie\n",
    "- De MSE loss is gebaseerd op verwachtingswaarde\n",
    "- Regularisatie beïnvloedt de variantie van het model\n",
    "- Weight initialisatie is ontworpen om variantie te controleren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Leerdoelen\n",
    "\n",
    "Na deze les kun je de verwachtingswaarde berekenen voor discrete en continue verdelingen. Je begrijpt variantie en standaarddeviatie als maten voor spreiding. Je kunt covariantie en correlatie berekenen. Je begrijpt hoe deze concepten worden gebruikt in neurale netwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Verwachtingswaarde\n",
    "\n",
    "### Definitie\n",
    "\n",
    "De verwachtingswaarde (of expected value) E[X] is het \"gewogen gemiddelde\" van alle mogelijke uitkomsten, gewogen naar hun kans.\n",
    "\n",
    "**Discreet:**\n",
    "E[X] = Σ x · P(X = x)\n",
    "\n",
    "**Continu:**\n",
    "E[X] = ∫ x · f(x) dx\n",
    "\n",
    "### Intuïtie\n",
    "\n",
    "Als je het experiment oneindig vaak zou herhalen en het gemiddelde zou nemen van alle uitkomsten, krijg je de verwachtingswaarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorbeeld: verwachtingswaarde van een dobbelsteen\n",
    "outcomes = np.array([1, 2, 3, 4, 5, 6])\n",
    "probabilities = np.array([1/6] * 6)\n",
    "\n",
    "# E[X] = Σ x · P(X = x)\n",
    "expected_value = np.sum(outcomes * probabilities)\n",
    "\n",
    "print(f\"Dobbelsteen: E[X] = {expected_value}\")\n",
    "print(f\"Berekening: (1+2+3+4+5+6)/6 = {sum(outcomes)/6}\")\n",
    "\n",
    "# Verificatie met simulatie\n",
    "n_throws = 100000\n",
    "throws = np.random.randint(1, 7, n_throws)\n",
    "sample_mean = np.mean(throws)\n",
    "\n",
    "print(f\"\\nSimulatie ({n_throws} worpen): gemiddelde = {sample_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niet-eerlijke dobbelsteen: hogere kans op 6\n",
    "unfair_probs = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.5])  # 50% kans op 6!\n",
    "\n",
    "E_unfair = np.sum(outcomes * unfair_probs)\n",
    "print(f\"Niet-eerlijke dobbelsteen: E[X] = {E_unfair}\")\n",
    "print(f\"Hoger dan eerlijk (3.5) omdat 6 waarschijnlijker is.\")\n",
    "\n",
    "# Visualisatie\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].bar(outcomes, probabilities, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(x=3.5, color='red', linestyle='--', linewidth=2, label=f'E[X] = 3.5')\n",
    "axes[0].set_xlabel('Uitkomst')\n",
    "axes[0].set_ylabel('Kans')\n",
    "axes[0].set_title('Eerlijke dobbelsteen')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].bar(outcomes, unfair_probs, color='steelblue', edgecolor='black')\n",
    "axes[1].axvline(x=E_unfair, color='red', linestyle='--', linewidth=2, label=f'E[X] = {E_unfair}')\n",
    "axes[1].set_xlabel('Uitkomst')\n",
    "axes[1].set_ylabel('Kans')\n",
    "axes[1].set_title('Niet-eerlijke dobbelsteen')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwachtingswaarde van continue verdeling: Normaal\n",
    "mu, sigma = 5, 2\n",
    "\n",
    "# E[X] = μ voor normale verdeling\n",
    "print(f\"Normale verdeling N({mu}, {sigma}²): E[X] = {mu}\")\n",
    "\n",
    "# Verificatie met samples\n",
    "samples = np.random.normal(mu, sigma, 100000)\n",
    "print(f\"Sample gemiddelde: {np.mean(samples):.4f}\")\n",
    "\n",
    "# Visualisatie\n",
    "x = np.linspace(-2, 12, 1000)\n",
    "pdf = stats.norm.pdf(x, mu, sigma)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, pdf, 'b-', linewidth=2)\n",
    "plt.fill_between(x, pdf, alpha=0.3)\n",
    "plt.axvline(x=mu, color='red', linestyle='--', linewidth=2, label=f'E[X] = μ = {mu}')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Verwachtingswaarde van N(5, 4)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenschappen van Verwachtingswaarde\n",
    "\n",
    "1. **Lineariteit**: E[aX + b] = a·E[X] + b\n",
    "2. **Additiviteit**: E[X + Y] = E[X] + E[Y] (altijd waar!)\n",
    "3. **Product**: E[XY] = E[X]·E[Y] alleen als X en Y onafhankelijk zijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstratie van lineariteit: E[2X + 3]\n",
    "X = np.random.normal(5, 2, 100000)\n",
    "\n",
    "E_X = np.mean(X)\n",
    "E_2X_plus_3 = np.mean(2*X + 3)\n",
    "\n",
    "print(\"Lineariteit: E[aX + b] = a·E[X] + b\")\n",
    "print(f\"E[X] = {E_X:.4f}\")\n",
    "print(f\"E[2X + 3] = {E_2X_plus_3:.4f}\")\n",
    "print(f\"2·E[X] + 3 = {2*E_X + 3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Variantie\n",
    "\n",
    "### Definitie\n",
    "\n",
    "De variantie Var(X) meet hoe ver de waarden gemiddeld afwijken van de verwachtingswaarde:\n",
    "\n",
    "Var(X) = E[(X - E[X])²] = E[X²] - (E[X])²\n",
    "\n",
    "### Standaarddeviatie\n",
    "\n",
    "De standaarddeviatie σ = √Var(X) is in dezelfde eenheid als X en is intuïtiever.\n",
    "\n",
    "### Intuïtie\n",
    "\n",
    "- Kleine variantie: waarden clusteren dicht bij het gemiddelde\n",
    "- Grote variantie: waarden zijn meer verspreid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variantie van een dobbelsteen\n",
    "outcomes = np.array([1, 2, 3, 4, 5, 6])\n",
    "probabilities = np.array([1/6] * 6)\n",
    "\n",
    "E_X = np.sum(outcomes * probabilities)\n",
    "E_X2 = np.sum(outcomes**2 * probabilities)\n",
    "\n",
    "# Var(X) = E[X²] - (E[X])²\n",
    "Var_X = E_X2 - E_X**2\n",
    "Std_X = np.sqrt(Var_X)\n",
    "\n",
    "print(f\"Dobbelsteen:\")\n",
    "print(f\"  E[X] = {E_X}\")\n",
    "print(f\"  E[X²] = {E_X2:.4f}\")\n",
    "print(f\"  Var(X) = {Var_X:.4f}\")\n",
    "print(f\"  σ = {Std_X:.4f}\")\n",
    "\n",
    "# Verificatie\n",
    "throws = np.random.randint(1, 7, 100000)\n",
    "print(f\"\\nSample variantie: {np.var(throws):.4f}\")\n",
    "print(f\"Sample std: {np.std(throws):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergelijk verdelingen met verschillende varianties\n",
    "mu = 0\n",
    "sigmas = [0.5, 1, 2, 4]\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for sigma in sigmas:\n",
    "    pdf = stats.norm.pdf(x, mu, sigma)\n",
    "    plt.plot(x, pdf, linewidth=2, label=f'σ = {sigma} (Var = {sigma**2})')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Normale verdelingen met verschillende varianties', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Grotere variantie = bredere, plattere verdeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenschappen van Variantie\n",
    "\n",
    "1. **Constante**: Var(aX + b) = a²·Var(X) (constante b verdwijnt!)\n",
    "2. **Som (onafhankelijk)**: Var(X + Y) = Var(X) + Var(Y) als X, Y onafhankelijk\n",
    "3. **Niet-negativiteit**: Var(X) ≥ 0, met gelijkheid alleen als X constant is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstratie: Var(aX + b) = a²·Var(X)\n",
    "X = np.random.normal(5, 2, 100000)  # Var(X) = 4\n",
    "\n",
    "a, b = 3, 10\n",
    "Y = a*X + b\n",
    "\n",
    "print(\"Eigenschap: Var(aX + b) = a²·Var(X)\")\n",
    "print(f\"Var(X) = {np.var(X):.4f}\")\n",
    "print(f\"Var({a}X + {b}) = {np.var(Y):.4f}\")\n",
    "print(f\"{a}²·Var(X) = {a**2 * np.var(X):.4f}\")\n",
    "print(\"\\nMerk op: de constante b heeft geen effect op de variantie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Covariantie en Correlatie\n",
    "\n",
    "### Covariantie\n",
    "\n",
    "De covariantie meet hoe twee variabelen samen variëren:\n",
    "\n",
    "Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]·E[Y]\n",
    "\n",
    "- Cov > 0: X en Y bewegen samen omhoog/omlaag\n",
    "- Cov < 0: als X omhoog gaat, gaat Y omlaag\n",
    "- Cov = 0: geen lineair verband\n",
    "\n",
    "### Correlatie\n",
    "\n",
    "De correlatie is de genormaliseerde covariantie:\n",
    "\n",
    "ρ(X, Y) = Cov(X, Y) / (σ_X · σ_Y)\n",
    "\n",
    "De correlatie ligt altijd tussen -1 en 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstratie van correlatie\n",
    "n = 500\n",
    "\n",
    "# Positief gecorreleerd\n",
    "X1 = np.random.randn(n)\n",
    "Y1 = X1 + np.random.randn(n) * 0.3  # Y = X + noise\n",
    "\n",
    "# Negatief gecorreleerd\n",
    "X2 = np.random.randn(n)\n",
    "Y2 = -X2 + np.random.randn(n) * 0.3\n",
    "\n",
    "# Ongecorreleerd\n",
    "X3 = np.random.randn(n)\n",
    "Y3 = np.random.randn(n)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (X, Y), title in zip(axes, [(X1,Y1), (X2,Y2), (X3,Y3)], \n",
    "                               ['Positief', 'Negatief', 'Geen']):\n",
    "    corr = np.corrcoef(X, Y)[0, 1]\n",
    "    ax.scatter(X, Y, alpha=0.5)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title(f'{title} gecorreleerd\\nρ = {corr:.3f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariantie matrix\n",
    "# Voor meerdere variabelen gebruiken we een covariantie matrix\n",
    "\n",
    "# Genereer gecorreleerde data\n",
    "mean = [0, 0]\n",
    "cov_matrix = [[1, 0.8],   # Var(X)=1, Cov(X,Y)=0.8\n",
    "              [0.8, 1]]   # Cov(Y,X)=0.8, Var(Y)=1\n",
    "\n",
    "data = np.random.multivariate_normal(mean, cov_matrix, 1000)\n",
    "X, Y = data[:, 0], data[:, 1]\n",
    "\n",
    "# Bereken sample covariantie matrix\n",
    "sample_cov = np.cov(data.T)\n",
    "sample_corr = np.corrcoef(data.T)\n",
    "\n",
    "print(\"Populatie covariantie matrix:\")\n",
    "print(np.array(cov_matrix))\n",
    "print(\"\\nSample covariantie matrix:\")\n",
    "print(sample_cov)\n",
    "print(\"\\nSample correlatie matrix:\")\n",
    "print(sample_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Toepassing: Batch Normalization\n",
    "\n",
    "Batch Normalization is een techniek die gemiddelde en variantie gebruikt om training te stabiliseren.\n",
    "\n",
    "Voor een batch activaties x:\n",
    "1. Bereken batch gemiddelde: μ_B = (1/m) Σ x_i\n",
    "2. Bereken batch variantie: σ²_B = (1/m) Σ (x_i - μ_B)²\n",
    "3. Normaliseer: x̂_i = (x_i - μ_B) / √(σ²_B + ε)\n",
    "4. Scale en shift: y_i = γ·x̂_i + β\n",
    "\n",
    "Dit zorgt ervoor dat activaties gemiddelde ≈ 0 en variantie ≈ 1 hebben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm:\n",
    "    \"\"\"Simpele Batch Normalization implementatie.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, epsilon=1e-5):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = np.ones(n_features)  # Scale parameter\n",
    "        self.beta = np.zeros(n_features)   # Shift parameter\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Normaliseer een batch.\"\"\"\n",
    "        # x shape: (batch_size, n_features)\n",
    "        self.mu = np.mean(x, axis=0)\n",
    "        self.var = np.var(x, axis=0)\n",
    "        \n",
    "        # Normaliseer\n",
    "        self.x_norm = (x - self.mu) / np.sqrt(self.var + self.epsilon)\n",
    "        \n",
    "        # Scale en shift\n",
    "        out = self.gamma * self.x_norm + self.beta\n",
    "        return out\n",
    "\n",
    "# Demonstratie\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simuleer activaties met groot gemiddelde en variantie\n",
    "batch_size = 64\n",
    "n_features = 4\n",
    "activations = np.random.randn(batch_size, n_features) * 10 + 50  # Grote mean, grote var\n",
    "\n",
    "print(\"Vóór BatchNorm:\")\n",
    "print(f\"  Gemiddelde per feature: {np.mean(activations, axis=0)}\")\n",
    "print(f\"  Variantie per feature: {np.var(activations, axis=0)}\")\n",
    "\n",
    "bn = BatchNorm(n_features)\n",
    "normalized = bn.forward(activations)\n",
    "\n",
    "print(\"\\nNa BatchNorm:\")\n",
    "print(f\"  Gemiddelde per feature: {np.mean(normalized, axis=0)}\")\n",
    "print(f\"  Variantie per feature: {np.var(normalized, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer het effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Voor BatchNorm\n",
    "axes[0].hist(activations.flatten(), bins=30, alpha=0.7, density=True)\n",
    "axes[0].axvline(x=np.mean(activations), color='red', linestyle='--', label=f'μ = {np.mean(activations):.1f}')\n",
    "axes[0].set_xlabel('Activatie')\n",
    "axes[0].set_ylabel('Dichtheid')\n",
    "axes[0].set_title('Vóór BatchNorm')\n",
    "axes[0].legend()\n",
    "\n",
    "# Na BatchNorm\n",
    "axes[1].hist(normalized.flatten(), bins=30, alpha=0.7, density=True)\n",
    "axes[1].axvline(x=np.mean(normalized), color='red', linestyle='--', label=f'μ ≈ {np.mean(normalized):.2f}')\n",
    "axes[1].set_xlabel('Activatie')\n",
    "axes[1].set_ylabel('Dichtheid')\n",
    "axes[1].set_title('Na BatchNorm')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 Toepassing: Xavier/He Initialisatie\n",
    "\n",
    "De variantie van weight initialisatie is cruciaal voor goede training.\n",
    "\n",
    "### Xavier Initialisatie (voor tanh/sigmoid)\n",
    "W ~ N(0, 2/(n_in + n_out))\n",
    "\n",
    "### He Initialisatie (voor ReLU)\n",
    "W ~ N(0, 2/n_in)\n",
    "\n",
    "Het doel is om de variantie van activaties constant te houden door de lagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstratie: effect van weight initialisatie op variantie\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def forward_pass(x, weights):\n",
    "    \"\"\"Forward pass door meerdere lagen met ReLU.\"\"\"\n",
    "    activations = [x]\n",
    "    for W in weights:\n",
    "        x = relu(x @ W)\n",
    "        activations.append(x)\n",
    "    return activations\n",
    "\n",
    "# Netwerk: 256 -> 256 -> 256 -> 256 -> 256 (5 lagen)\n",
    "n_layers = 5\n",
    "n_neurons = 256\n",
    "batch_size = 100\n",
    "\n",
    "# Input\n",
    "x = np.random.randn(batch_size, n_neurons)\n",
    "\n",
    "# Slechte initialisatie: te kleine variantie\n",
    "weights_small = [np.random.randn(n_neurons, n_neurons) * 0.01 for _ in range(n_layers)]\n",
    "acts_small = forward_pass(x, weights_small)\n",
    "\n",
    "# Slechte initialisatie: te grote variantie\n",
    "weights_large = [np.random.randn(n_neurons, n_neurons) * 1.0 for _ in range(n_layers)]\n",
    "acts_large = forward_pass(x, weights_large)\n",
    "\n",
    "# He initialisatie: juiste variantie\n",
    "weights_he = [np.random.randn(n_neurons, n_neurons) * np.sqrt(2/n_neurons) for _ in range(n_layers)]\n",
    "acts_he = forward_pass(x, weights_he)\n",
    "\n",
    "# Plot varianties\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "layers = range(n_layers + 1)\n",
    "ax.plot(layers, [np.var(a) for a in acts_small], 'r-o', label='Te kleine var (0.01)')\n",
    "ax.plot(layers, [np.var(a) for a in acts_large], 'b-o', label='Te grote var (1.0)')\n",
    "ax.plot(layers, [np.var(a) for a in acts_he], 'g-o', label='He initialisatie')\n",
    "\n",
    "ax.set_xlabel('Laag', fontsize=12)\n",
    "ax.set_ylabel('Variantie van activaties', fontsize=12)\n",
    "ax.set_title('Effect van weight initialisatie op activatie variantie', fontsize=14)\n",
    "ax.set_yscale('log')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Te kleine variantie → activaties krimpen naar 0 (vanishing)\")\n",
    "print(\"Te grote variantie → activaties exploderen (exploding)\")\n",
    "print(\"He initialisatie → stabiele variantie door alle lagen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7 Samenvatting\n",
    "\n",
    "### Kernconcepten\n",
    "\n",
    "**Verwachtingswaarde E[X]** is het gewogen gemiddelde van alle mogelijke uitkomsten. Het is lineair: E[aX + b] = a·E[X] + b.\n",
    "\n",
    "**Variantie Var(X)** meet de spreiding rond het gemiddelde. Het is kwadratisch: Var(aX + b) = a²·Var(X).\n",
    "\n",
    "**Covariantie en correlatie** meten hoe twee variabelen samen variëren. Correlatie is genormaliseerd naar [-1, 1].\n",
    "\n",
    "### Toepassingen in ML\n",
    "\n",
    "- **Batch Normalization**: normaliseert activaties naar gemiddelde 0 en variantie 1\n",
    "- **Weight initialisatie**: kiest variantie om stabiele training te garanderen\n",
    "- **Loss functions**: MSE is gebaseerd op verwachte kwadratische fout\n",
    "\n",
    "### Volgende les\n",
    "\n",
    "In les 10 leren we over Maximum Likelihood Estimation: de theoretische basis voor het trainen van modellen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 9 van 12 | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
