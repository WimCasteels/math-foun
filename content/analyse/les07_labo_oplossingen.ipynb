{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 7: Labo - Oplossingen\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 1: Computational Graph Tekenen - Oplossingen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. f(x, y) = (x + y) * (x - y)\n",
    "\n",
    "```\n",
    "x ──┬──[+]── a ──┐\n",
    "    │            [*]── f\n",
    "y ──┴──[-]── b ──┘\n",
    "\n",
    "Tussenresultaten:\n",
    "- a = x + y\n",
    "- b = x - y\n",
    "- f = a * b\n",
    "```\n",
    "\n",
    "### 2. g(x, w, b) = σ(wx + b)\n",
    "\n",
    "```\n",
    "x ──[*w]── z1 ──[+b]── z2 ──[σ]── g\n",
    "\n",
    "Tussenresultaten:\n",
    "- z1 = w * x\n",
    "- z2 = z1 + b\n",
    "- g = σ(z2)\n",
    "```\n",
    "\n",
    "### 3. h(x, W1, b1, W2, b2) = W2 @ ReLU(W1 @ x + b1) + b2\n",
    "\n",
    "```\n",
    "x ──[@W1]── z1 ──[+b1]── z2 ──[ReLU]── a1 ──[@W2]── z3 ──[+b2]── h\n",
    "\n",
    "Tussenresultaten:\n",
    "- z1 = W1 @ x\n",
    "- z2 = z1 + b1\n",
    "- a1 = ReLU(z2)\n",
    "- z3 = W2 @ a1\n",
    "- h = z3 + b2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 2: Lokale Gradiënten - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 2a\n",
    "class Add:\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # z = x + y\n",
    "        # ∂z/∂x = 1, ∂z/∂y = 1\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "\n",
    "class Multiply:\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # z = x * y\n",
    "        # ∂z/∂x = y, ∂z/∂y = x\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy\n",
    "\n",
    "class Power:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x ** self.n\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # z = x^n\n",
    "        # ∂z/∂x = n * x^(n-1)\n",
    "        dx = dout * self.n * self.x ** (self.n - 1)\n",
    "        return dx\n",
    "\n",
    "# Test\n",
    "add = Add()\n",
    "z = add.forward(3, 5)\n",
    "dx, dy = add.backward(1)\n",
    "print(f\"Add: 3 + 5 = {z}, ∂z/∂x = {dx}, ∂z/∂y = {dy}\")\n",
    "\n",
    "mul = Multiply()\n",
    "z = mul.forward(3, 5)\n",
    "dx, dy = mul.backward(1)\n",
    "print(f\"Mul: 3 * 5 = {z}, ∂z/∂x = {dx}, ∂z/∂y = {dy}\")\n",
    "\n",
    "pow3 = Power(3)\n",
    "z = pow3.forward(2)\n",
    "dx = pow3.backward(1)\n",
    "print(f\"Pow: 2^3 = {z}, ∂z/∂x = {dx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 2b\n",
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # σ'(x) = σ(x)(1 - σ(x))\n",
    "        return dout * self.out * (1 - self.out)\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # tanh'(x) = 1 - tanh²(x)\n",
    "        return dout * (1 - self.out ** 2)\n",
    "\n",
    "# Test\n",
    "sig = Sigmoid()\n",
    "y = sig.forward(1.0)\n",
    "dx = sig.backward(1.0)\n",
    "print(f\"Sigmoid(1) = {y:.4f}, σ'(1) = {dx:.4f}\")\n",
    "\n",
    "# Numerieke verificatie\n",
    "h = 1e-5\n",
    "sig2 = Sigmoid()\n",
    "num_grad = (sig2.forward(1 + h) - sig.forward(1 - h)) / (2*h)\n",
    "print(f\"Numerieke gradiënt: {num_grad:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 3: Backprop met de Hand - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 3a\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Gegeven\n",
    "x = 2\n",
    "w1, b1 = 0.5, 0.1\n",
    "w2, b2 = -0.3, 0.2\n",
    "y_target = 0.5\n",
    "\n",
    "print(\"=== FORWARD PASS ===\")\n",
    "# h = ReLU(w1*x + b1)\n",
    "z1 = w1 * x + b1\n",
    "print(f\"z1 = w1*x + b1 = {w1}*{x} + {b1} = {z1}\")\n",
    "\n",
    "h = max(0, z1)  # ReLU\n",
    "print(f\"h = ReLU(z1) = {h}\")\n",
    "\n",
    "# y = sigmoid(w2*h + b2)\n",
    "z2 = w2 * h + b2\n",
    "print(f\"z2 = w2*h + b2 = {w2}*{h} + {b2} = {z2}\")\n",
    "\n",
    "y = sigmoid(z2)\n",
    "print(f\"y = σ(z2) = {y:.6f}\")\n",
    "\n",
    "# L = (y - y_target)²\n",
    "L = (y - y_target) ** 2\n",
    "print(f\"L = (y - y_target)² = ({y:.6f} - {y_target})² = {L:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== BACKWARD PASS ===\")\n",
    "\n",
    "# ∂L/∂y = 2(y - y_target)\n",
    "dL_dy = 2 * (y - y_target)\n",
    "print(f\"∂L/∂y = 2(y - y_target) = 2*({y:.6f} - {y_target}) = {dL_dy:.6f}\")\n",
    "\n",
    "# ∂L/∂z2 = ∂L/∂y * ∂y/∂z2 = ∂L/∂y * σ'(z2) = ∂L/∂y * y*(1-y)\n",
    "dy_dz2 = y * (1 - y)\n",
    "dL_dz2 = dL_dy * dy_dz2\n",
    "print(f\"∂y/∂z2 = σ(z2)*(1-σ(z2)) = {y:.6f}*(1-{y:.6f}) = {dy_dz2:.6f}\")\n",
    "print(f\"∂L/∂z2 = {dL_dz2:.6f}\")\n",
    "\n",
    "# ∂L/∂w2 = ∂L/∂z2 * ∂z2/∂w2 = ∂L/∂z2 * h\n",
    "dL_dw2 = dL_dz2 * h\n",
    "print(f\"∂L/∂w2 = ∂L/∂z2 * h = {dL_dz2:.6f} * {h} = {dL_dw2:.6f}\")\n",
    "\n",
    "# ∂L/∂b2 = ∂L/∂z2 * 1\n",
    "dL_db2 = dL_dz2\n",
    "print(f\"∂L/∂b2 = {dL_db2:.6f}\")\n",
    "\n",
    "# ∂L/∂h = ∂L/∂z2 * ∂z2/∂h = ∂L/∂z2 * w2\n",
    "dL_dh = dL_dz2 * w2\n",
    "print(f\"∂L/∂h = ∂L/∂z2 * w2 = {dL_dz2:.6f} * {w2} = {dL_dh:.6f}\")\n",
    "\n",
    "# ∂L/∂z1 = ∂L/∂h * ∂h/∂z1 = ∂L/∂h * ReLU'(z1)\n",
    "# ReLU'(z1) = 1 als z1 > 0, anders 0\n",
    "relu_grad = 1 if z1 > 0 else 0\n",
    "dL_dz1 = dL_dh * relu_grad\n",
    "print(f\"ReLU'(z1) = {relu_grad} (want z1 = {z1} > 0)\")\n",
    "print(f\"∂L/∂z1 = {dL_dz1:.6f}\")\n",
    "\n",
    "# ∂L/∂w1 = ∂L/∂z1 * x\n",
    "dL_dw1 = dL_dz1 * x\n",
    "print(f\"∂L/∂w1 = ∂L/∂z1 * x = {dL_dz1:.6f} * {x} = {dL_dw1:.6f}\")\n",
    "\n",
    "# ∂L/∂b1 = ∂L/∂z1 * 1\n",
    "dL_db1 = dL_dz1\n",
    "print(f\"∂L/∂b1 = {dL_db1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerieke verificatie\n",
    "def full_forward(x, w1, b1, w2, b2, y_target):\n",
    "    z1 = w1 * x + b1\n",
    "    h = max(0, z1)\n",
    "    z2 = w2 * h + b2\n",
    "    y = sigmoid(z2)\n",
    "    L = (y - y_target) ** 2\n",
    "    return L\n",
    "\n",
    "h = 1e-5\n",
    "print(\"\\n=== NUMERIEKE VERIFICATIE ===\")\n",
    "\n",
    "num_dw1 = (full_forward(x, w1+h, b1, w2, b2, y_target) - full_forward(x, w1-h, b1, w2, b2, y_target)) / (2*h)\n",
    "print(f\"∂L/∂w1: analytisch={dL_dw1:.6f}, numeriek={num_dw1:.6f}\")\n",
    "\n",
    "num_dw2 = (full_forward(x, w1, b1, w2+h, b2, y_target) - full_forward(x, w1, b1, w2-h, b2, y_target)) / (2*h)\n",
    "print(f\"∂L/∂w2: analytisch={dL_dw2:.6f}, numeriek={num_dw2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 4: Gradient Checking - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 4a\n",
    "def gradient_check(f, x, analytic_grad, h=1e-5):\n",
    "    \"\"\"\n",
    "    Vergelijk analytische gradiënt met numerieke gradiënt.\n",
    "    \"\"\"\n",
    "    # Numerieke gradiënt (central difference)\n",
    "    numeric_grad = (f(x + h) - f(x - h)) / (2 * h)\n",
    "    \n",
    "    # Relatieve fout\n",
    "    diff = abs(analytic_grad - numeric_grad)\n",
    "    denom = max(abs(analytic_grad), abs(numeric_grad), 1e-8)\n",
    "    relative_error = diff / denom\n",
    "    \n",
    "    return relative_error, numeric_grad\n",
    "\n",
    "# Test met f(x) = x²\n",
    "f = lambda x: x**2\n",
    "x = 3.0\n",
    "analytic = 2 * x  # f'(x) = 2x\n",
    "\n",
    "error, numeric = gradient_check(f, x, analytic)\n",
    "print(f\"f(x) = x², x = {x}\")\n",
    "print(f\"Analytische gradiënt: {analytic}\")\n",
    "print(f\"Numerieke gradiënt: {numeric:.6f}\")\n",
    "print(f\"Relatieve fout: {error:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 4b - Sigmoid verificatie\n",
    "sig = Sigmoid()\n",
    "\n",
    "for x_test in [-2, 0, 1, 3]:\n",
    "    # Forward en backward\n",
    "    y = sig.forward(x_test)\n",
    "    analytic = sig.backward(1.0)  # dout = 1\n",
    "    \n",
    "    # Numerieke check\n",
    "    f = lambda x: sigmoid(x)\n",
    "    error, numeric = gradient_check(f, x_test, analytic)\n",
    "    \n",
    "    print(f\"x={x_test:2}: analytic={analytic:.6f}, numeric={numeric:.6f}, error={error:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 5: Layer Class met Backward - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 5a\n",
    "class LinearLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Xavier initialisatie\n",
    "        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b = np.zeros(output_dim)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X  # Cache voor backward\n",
    "        return X @ self.W + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N = self.X.shape[0]\n",
    "        \n",
    "        # Gradiënten naar parameters\n",
    "        self.dW = self.X.T @ dout / N\n",
    "        self.db = np.mean(dout, axis=0)\n",
    "        \n",
    "        # Gradiënt naar input\n",
    "        dX = dout @ self.W.T\n",
    "        return dX\n",
    "\n",
    "# Test\n",
    "layer = LinearLayer(3, 2)\n",
    "X = np.random.randn(4, 3)  # 4 samples, 3 features\n",
    "out = layer.forward(X)\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "\n",
    "# Backward test\n",
    "dout = np.random.randn(4, 2)\n",
    "dX = layer.backward(dout)\n",
    "print(f\"dX shape: {dX.shape}\")\n",
    "print(f\"dW shape: {layer.dW.shape}\")\n",
    "print(f\"db shape: {layer.db.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 5b\n",
    "class ReLULayer:\n",
    "    def forward(self, X):\n",
    "        self.mask = (X > 0)\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "# Test\n",
    "relu = ReLULayer()\n",
    "X = np.array([[-1, 2, -3], [4, -5, 6]])\n",
    "out = relu.forward(X)\n",
    "print(f\"Input:\\n{X}\")\n",
    "print(f\"ReLU output:\\n{out}\")\n",
    "\n",
    "dout = np.ones_like(X)\n",
    "dX = relu.backward(dout)\n",
    "print(f\"Backward (dout=1):\\n{dX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 6: Mini-netwerk Trainen - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 6a - XOR probleem\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = ((X[:, 0] > 0) == (X[:, 1] > 0)).astype(float).reshape(-1, 1)\n",
    "\n",
    "# Netwerk voor binaire classificatie\n",
    "class XORNetwork:\n",
    "    def __init__(self):\n",
    "        self.layer1 = LinearLayer(2, 16)\n",
    "        self.relu1 = ReLULayer()\n",
    "        self.layer2 = LinearLayer(16, 8)\n",
    "        self.relu2 = ReLULayer()\n",
    "        self.layer3 = LinearLayer(8, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = self.layer1.forward(X)\n",
    "        self.a1 = self.relu1.forward(self.z1)\n",
    "        self.z2 = self.layer2.forward(self.a1)\n",
    "        self.a2 = self.relu2.forward(self.z2)\n",
    "        self.z3 = self.layer3.forward(self.a2)\n",
    "        self.out = 1 / (1 + np.exp(-self.z3))  # Sigmoid\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        # Binary cross-entropy gradient + sigmoid = y_pred - y_true\n",
    "        dout = (self.out - y_true) / len(y_true)\n",
    "        \n",
    "        # Sigmoid gradient\n",
    "        dz3 = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        da2 = self.layer3.backward(dz3)\n",
    "        dz2 = self.relu2.backward(da2)\n",
    "        da1 = self.layer2.backward(dz2)\n",
    "        dz1 = self.relu1.backward(da1)\n",
    "        self.layer1.backward(dz1)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        for layer in [self.layer1, self.layer2, self.layer3]:\n",
    "            layer.W -= lr * layer.dW\n",
    "            layer.b -= lr * layer.db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.forward(X) > 0.5).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model = XORNetwork()\n",
    "lr = 1.0\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Forward\n",
    "    out = model.forward(X)\n",
    "    \n",
    "    # Loss (binary cross-entropy)\n",
    "    loss = -np.mean(y * np.log(out + 1e-10) + (1-y) * np.log(1-out + 1e-10))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward\n",
    "    model.backward(y)\n",
    "    model.update(lr)\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        acc = np.mean(model.predict(X) == y)\n",
    "        print(f\"Epoch {epoch}: loss={loss:.4f}, acc={acc:.4f}\")\n",
    "\n",
    "print(f\"\\nFinale accuracy: {np.mean(model.predict(X) == y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer decision boundary\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = model.forward(grid).reshape(xx.shape)\n",
    "\n",
    "axes[0].contourf(xx, yy, probs, levels=20, cmap='RdBu', alpha=0.7)\n",
    "axes[0].scatter(X[y.flatten()==0, 0], X[y.flatten()==0, 1], c='blue', edgecolors='k')\n",
    "axes[0].scatter(X[y.flatten()==1, 0], X[y.flatten()==1, 1], c='red', edgecolors='k')\n",
    "axes[0].set_title('Decision Boundary')\n",
    "axes[0].set_xlabel('x1')\n",
    "axes[0].set_ylabel('x2')\n",
    "\n",
    "# Loss curve\n",
    "axes[1].plot(losses)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Training Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 7: MNIST Trainen - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 7a\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print(\"MNIST laden...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "X_data, y_data = mnist.data / 255.0, mnist.target.astype(int)\n",
    "\n",
    "X_train, X_test = X_data[:60000], X_data[60000:]\n",
    "y_train, y_test = y_data[:60000], y_data[60000:]\n",
    "\n",
    "print(f\"Training: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax en Cross-Entropy\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(probs, y_true):\n",
    "    N = probs.shape[0]\n",
    "    return -np.sum(np.log(probs[np.arange(N), y_true] + 1e-10)) / N\n",
    "\n",
    "def cross_entropy_gradient(probs, y_true):\n",
    "    N = probs.shape[0]\n",
    "    grad = probs.copy()\n",
    "    grad[np.arange(N), y_true] -= 1\n",
    "    return grad / N\n",
    "\n",
    "# MNIST Classifier\n",
    "class MNISTNet:\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        self.layer1 = LinearLayer(784, hidden_dim)\n",
    "        self.relu = ReLULayer()\n",
    "        self.layer2 = LinearLayer(hidden_dim, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = self.layer1.forward(X)\n",
    "        self.a1 = self.relu.forward(self.z1)\n",
    "        self.z2 = self.layer2.forward(self.a1)\n",
    "        self.probs = softmax(self.z2)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        dout = cross_entropy_gradient(self.probs, y_true)\n",
    "        da1 = self.layer2.backward(dout)\n",
    "        dz1 = self.relu.backward(da1)\n",
    "        self.layer1.backward(dz1)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.layer1.W -= lr * self.layer1.dW\n",
    "        self.layer1.b -= lr * self.layer1.db\n",
    "        self.layer2.W -= lr * self.layer2.dW\n",
    "        self.layer2.b -= lr * self.layer2.db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "np.random.seed(42)\n",
    "model = MNISTNet(hidden_dim=128)\n",
    "\n",
    "lr = 0.5\n",
    "batch_size = 128\n",
    "n_epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "test_accs = []\n",
    "\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    X_shuf, y_shuf = X_train[idx], y_train[idx]\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        X_b = X_shuf[start:start+batch_size]\n",
    "        y_b = y_shuf[start:start+batch_size]\n",
    "        \n",
    "        probs = model.forward(X_b)\n",
    "        loss = cross_entropy_loss(probs, y_b)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.backward(y_b)\n",
    "        model.update(lr)\n",
    "    \n",
    "    train_losses.append(epoch_loss / n_batches)\n",
    "    test_acc = model.accuracy(X_test, y_test)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}: Loss={train_losses[-1]:.4f}, Test Acc={test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nFinale test accuracy: {test_accs[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 7b - Hyperparameter experimenten\n",
    "results = []\n",
    "\n",
    "for hidden_dim in [64, 128, 256]:\n",
    "    for lr in [0.1, 0.5, 1.0]:\n",
    "        np.random.seed(42)\n",
    "        model = MNISTNet(hidden_dim=hidden_dim)\n",
    "        \n",
    "        # Train for 5 epochs\n",
    "        for epoch in range(5):\n",
    "            idx = np.random.permutation(len(X_train))\n",
    "            for batch in range(n_batches):\n",
    "                start = batch * batch_size\n",
    "                X_b = X_train[idx[start:start+batch_size]]\n",
    "                y_b = y_train[idx[start:start+batch_size]]\n",
    "                model.forward(X_b)\n",
    "                model.backward(y_b)\n",
    "                model.update(lr)\n",
    "        \n",
    "        acc = model.accuracy(X_test, y_test)\n",
    "        results.append((hidden_dim, lr, acc))\n",
    "        print(f\"hidden={hidden_dim}, lr={lr}: acc={acc:.4f}\")\n",
    "\n",
    "print(\"\\nBeste configuratie:\")\n",
    "best = max(results, key=lambda x: x[2])\n",
    "print(f\"hidden={best[0]}, lr={best[1]}: acc={best[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonusoefening: Tweede Hidden Layer - Oplossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netwerk met 2 hidden layers\n",
    "class DeepMNISTNet:\n",
    "    def __init__(self):\n",
    "        self.layer1 = LinearLayer(784, 256)\n",
    "        self.relu1 = ReLULayer()\n",
    "        self.layer2 = LinearLayer(256, 128)\n",
    "        self.relu2 = ReLULayer()\n",
    "        self.layer3 = LinearLayer(128, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = self.layer1.forward(X)\n",
    "        self.a1 = self.relu1.forward(self.z1)\n",
    "        self.z2 = self.layer2.forward(self.a1)\n",
    "        self.a2 = self.relu2.forward(self.z2)\n",
    "        self.z3 = self.layer3.forward(self.a2)\n",
    "        self.probs = softmax(self.z3)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        dout = cross_entropy_gradient(self.probs, y_true)\n",
    "        da2 = self.layer3.backward(dout)\n",
    "        dz2 = self.relu2.backward(da2)\n",
    "        da1 = self.layer2.backward(dz2)\n",
    "        dz1 = self.relu1.backward(da1)\n",
    "        self.layer1.backward(dz1)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        for layer in [self.layer1, self.layer2, self.layer3]:\n",
    "            layer.W -= lr * layer.dW\n",
    "            layer.b -= lr * layer.db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "# Train\n",
    "np.random.seed(42)\n",
    "deep_model = DeepMNISTNet()\n",
    "\n",
    "lr = 0.5\n",
    "for epoch in range(10):\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        X_b = X_train[idx[start:start+batch_size]]\n",
    "        y_b = y_train[idx[start:start+batch_size]]\n",
    "        deep_model.forward(X_b)\n",
    "        deep_model.backward(y_b)\n",
    "        deep_model.update(lr)\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        acc = deep_model.accuracy(X_test, y_test)\n",
    "        print(f\"Epoch {epoch+1}: Test Acc = {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nDiep netwerk (784→256→128→10): {deep_model.accuracy(X_test, y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 7 Oplossingen | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
