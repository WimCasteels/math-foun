{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 6: Labo - Oplossingen\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 1: Loss Functies - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 1a: MSE Loss\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"Bereken Mean Squared Error.\"\"\"\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "# Test\n",
    "y_true = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_pred = np.array([1.1, 2.2, 2.9, 4.0, 5.2])\n",
    "\n",
    "print(f\"MSE: {mse_loss(y_pred, y_true):.4f}\")\n",
    "print(f\"Verwacht: {((0.1)**2 + (0.2)**2 + (0.1)**2 + 0 + (0.2)**2) / 5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 1b: MSE Gradiënt\n",
    "def mse_gradient(y_pred, y_true):\n",
    "    \"\"\"Bereken de gradiënt van MSE naar y_pred.\"\"\"\n",
    "    n = len(y_true)\n",
    "    return (2 / n) * (y_pred - y_true)\n",
    "\n",
    "grad = mse_gradient(y_pred, y_true)\n",
    "print(f\"Gradiënt: {grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 1c: Binary Cross-Entropy\n",
    "def binary_cross_entropy(y_pred, y_true, epsilon=1e-15):\n",
    "    \"\"\"Bereken Binary Cross-Entropy Loss.\"\"\"\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Test\n",
    "y_true_class = np.array([1, 0, 1, 1, 0])\n",
    "y_pred_probs = np.array([0.9, 0.1, 0.8, 0.7, 0.3])\n",
    "\n",
    "print(f\"BCE: {binary_cross_entropy(y_pred_probs, y_true_class):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 2: 1D Gradient Descent - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 2a: 1D Gradient Descent\n",
    "def gradient_descent_1d(f, df_dx, x_init, learning_rate, n_iterations):\n",
    "    \"\"\"Voer gradient descent uit voor een 1D functie.\"\"\"\n",
    "    x = x_init\n",
    "    history = [x]\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        gradient = df_dx(x)\n",
    "        x = x - learning_rate * gradient\n",
    "        history.append(x)\n",
    "    \n",
    "    return x, np.array(history)\n",
    "\n",
    "# Functie en afgeleide\n",
    "def f(x):\n",
    "    return x**2 - 4*x + 5\n",
    "\n",
    "def df_dx(x):\n",
    "    return 2*x - 4\n",
    "\n",
    "# Test\n",
    "x_final, history = gradient_descent_1d(f, df_dx, x_init=6.0, learning_rate=0.1, n_iterations=50)\n",
    "print(f\"Gevonden minimum: x = {x_final:.6f}\")\n",
    "print(f\"Verwacht minimum: x = 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 2b: Visualisatie\n",
    "x_range = np.linspace(-1, 7, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Functie met pad\n",
    "axes[0].plot(x_range, f(x_range), 'b-', linewidth=2, label='f(x) = x² - 4x + 5')\n",
    "axes[0].plot(history, f(history), 'ro-', markersize=5, alpha=0.7, label='GD pad')\n",
    "axes[0].plot(history[0], f(history[0]), 'g^', markersize=12, label='Start')\n",
    "axes[0].plot(2, f(2), 'r*', markersize=15, label='Minimum')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].set_title('Gradient Descent Pad')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Convergentie\n",
    "axes[1].plot(f(history), 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Iteratie')\n",
    "axes[1].set_ylabel('f(x)')\n",
    "axes[1].set_title('Convergentie')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 2c: Learning rate experimenten\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0, 1.5]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for lr in learning_rates:\n",
    "    _, history = gradient_descent_1d(f, df_dx, x_init=6.0, learning_rate=lr, n_iterations=30)\n",
    "    plt.plot(f(history), label=f'lr = {lr}')\n",
    "\n",
    "plt.xlabel('Iteratie')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Effect van Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 20)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observaties:\")\n",
    "print(\"- lr=0.01: Zeer trage convergentie\")\n",
    "print(\"- lr=0.1: Goede, stabiele convergentie\")\n",
    "print(\"- lr=0.5: Snelle convergentie\")\n",
    "print(\"- lr=1.0: Exacte convergentie in 1 stap (speciaal geval voor kwadratische functie)\")\n",
    "print(\"- lr=1.5: Oscilleert, convergeert langzaam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 3: 2D Gradient Descent - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 3a: 2D Gradient Descent\n",
    "def gradient_descent_2d(f, grad_f, xy_init, learning_rate, n_iterations):\n",
    "    \"\"\"Voer gradient descent uit voor een 2D functie.\"\"\"\n",
    "    xy = np.array(xy_init, dtype=float)\n",
    "    history = [xy.copy()]\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        gradient = grad_f(xy[0], xy[1])\n",
    "        xy = xy - learning_rate * gradient\n",
    "        history.append(xy.copy())\n",
    "    \n",
    "    return xy, np.array(history)\n",
    "\n",
    "# Functie en gradiënt\n",
    "def f_2d(x, y):\n",
    "    return (x - 2)**2 + (y - 3)**2\n",
    "\n",
    "def grad_f_2d(x, y):\n",
    "    return np.array([2*(x - 2), 2*(y - 3)])\n",
    "\n",
    "# Test\n",
    "xy_final, history_2d = gradient_descent_2d(f_2d, grad_f_2d, xy_init=[-1, 6], \n",
    "                                            learning_rate=0.1, n_iterations=50)\n",
    "print(f\"Gevonden minimum: ({xy_final[0]:.4f}, {xy_final[1]:.4f})\")\n",
    "print(f\"Verwacht minimum: (2, 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 3b: Visualisatie\n",
    "x_range = np.linspace(-3, 6, 100)\n",
    "y_range = np.linspace(-1, 8, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f_2d(X, Y)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X, Y, Z, levels=30, cmap='viridis')\n",
    "plt.colorbar(label='f(x, y)')\n",
    "\n",
    "# Plot pad\n",
    "plt.plot(history_2d[:, 0], history_2d[:, 1], 'ro-', markersize=4, linewidth=1.5, label='GD pad')\n",
    "plt.plot(history_2d[0, 0], history_2d[0, 1], 'g^', markersize=12, label='Start')\n",
    "plt.plot(2, 3, 'r*', markersize=15, label='Minimum')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('2D Gradient Descent', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 4: Lineaire Regressie - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 4a: Data genereren\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "X = np.random.uniform(0, 10, n_samples)\n",
    "y = 2.5 * X + 3 + np.random.randn(n_samples) * 1.5\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, y, alpha=0.7)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetische data: y = 2.5x + 3 + ruis')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 4b: Lineaire Regressie met GD\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "        self.w = 0.0\n",
    "        self.b = 0.0\n",
    "        self.loss_history = []\n",
    "        self.w_history = []\n",
    "        self.b_history = []\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.w * X + self.b\n",
    "    \n",
    "    def compute_loss(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        n = len(y)\n",
    "        y_pred = self.predict(X)\n",
    "        errors = y_pred - y\n",
    "        \n",
    "        dw = (2 / n) * np.sum(errors * X)\n",
    "        db = (2 / n) * np.sum(errors)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y, n_epochs=100):\n",
    "        for epoch in range(n_epochs):\n",
    "            # Bereken gradiënten\n",
    "            dw, db = self.compute_gradients(X, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "            \n",
    "            # Log\n",
    "            loss = self.compute_loss(X, y)\n",
    "            self.loss_history.append(loss)\n",
    "            self.w_history.append(self.w)\n",
    "            self.b_history.append(self.b)\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Train\n",
    "model = LinearRegressionGD(learning_rate=0.01)\n",
    "model.fit(X, y, n_epochs=200)\n",
    "\n",
    "print(f\"Geleerd: w = {model.w:.4f}, b = {model.b:.4f}\")\n",
    "print(f\"Werkelijk: w = 2.5, b = 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 4c: Visualisatie\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Data met fit\n",
    "axes[0].scatter(X, y, alpha=0.7, label='Data')\n",
    "x_line = np.linspace(0, 10, 100)\n",
    "axes[0].plot(x_line, model.predict(x_line), 'r-', linewidth=2, \n",
    "             label=f'Fit: y = {model.w:.2f}x + {model.b:.2f}')\n",
    "axes[0].plot(x_line, 2.5*x_line + 3, 'g--', alpha=0.5, label='Werkelijk')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Lineaire Regressie')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss curve\n",
    "axes[1].plot(model.loss_history, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].set_title('Training Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter evolutie\n",
    "axes[2].plot(model.w_history, label='w', linewidth=2)\n",
    "axes[2].plot(model.b_history, label='b', linewidth=2)\n",
    "axes[2].axhline(y=2.5, color='blue', linestyle='--', alpha=0.5)\n",
    "axes[2].axhline(y=3, color='orange', linestyle='--', alpha=0.5)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Waarde')\n",
    "axes[2].set_title('Parameter Evolutie')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 5: Batch vs Mini-batch vs Stochastic GD - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 5a: Drie GD varianten\n",
    "\n",
    "def batch_gradient_descent(X, y, learning_rate, n_epochs):\n",
    "    \"\"\"Batch gradient descent.\"\"\"\n",
    "    w, b = 0.0, 0.0\n",
    "    loss_history = []\n",
    "    n = len(y)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Voorspelling\n",
    "        y_pred = w * X + b\n",
    "        \n",
    "        # Gradiënten over ALLE data\n",
    "        dw = (2/n) * np.sum((y_pred - y) * X)\n",
    "        db = (2/n) * np.sum(y_pred - y)\n",
    "        \n",
    "        # Update\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        loss_history.append(np.mean((y_pred - y)**2))\n",
    "    \n",
    "    return w, b, loss_history\n",
    "\n",
    "\n",
    "def minibatch_gradient_descent(X, y, learning_rate, n_epochs, batch_size=16):\n",
    "    \"\"\"Mini-batch gradient descent.\"\"\"\n",
    "    w, b = 0.0, 0.0\n",
    "    loss_history = []\n",
    "    n = len(y)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(n)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        # Itereer over mini-batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            y_pred = w * X_batch + b\n",
    "            \n",
    "            dw = (2/len(y_batch)) * np.sum((y_pred - y_batch) * X_batch)\n",
    "            db = (2/len(y_batch)) * np.sum(y_pred - y_batch)\n",
    "            \n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "        \n",
    "        # Log loss na elke epoch\n",
    "        y_pred_all = w * X + b\n",
    "        loss_history.append(np.mean((y_pred_all - y)**2))\n",
    "    \n",
    "    return w, b, loss_history\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X, y, learning_rate, n_epochs):\n",
    "    \"\"\"Stochastic gradient descent (batch_size=1).\"\"\"\n",
    "    w, b = 0.0, 0.0\n",
    "    loss_history = []\n",
    "    n = len(y)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(n)\n",
    "        \n",
    "        for i in indices:\n",
    "            xi, yi = X[i], y[i]\n",
    "            \n",
    "            y_pred = w * xi + b\n",
    "            \n",
    "            dw = 2 * (y_pred - yi) * xi\n",
    "            db = 2 * (y_pred - yi)\n",
    "            \n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "        \n",
    "        # Log loss na elke epoch\n",
    "        y_pred_all = w * X + b\n",
    "        loss_history.append(np.mean((y_pred_all - y)**2))\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 5b: Vergelijking\n",
    "n_epochs = 100\n",
    "\n",
    "w_batch, b_batch, loss_batch = batch_gradient_descent(X, y, 0.01, n_epochs)\n",
    "w_mini, b_mini, loss_mini = minibatch_gradient_descent(X, y, 0.01, n_epochs, batch_size=8)\n",
    "w_sgd, b_sgd, loss_sgd = stochastic_gradient_descent(X, y, 0.001, n_epochs)  # Kleinere lr voor SGD\n",
    "\n",
    "print(\"Resultaten:\")\n",
    "print(f\"Batch GD:      w = {w_batch:.4f}, b = {b_batch:.4f}\")\n",
    "print(f\"Mini-batch GD: w = {w_mini:.4f}, b = {b_mini:.4f}\")\n",
    "print(f\"SGD:           w = {w_sgd:.4f}, b = {b_sgd:.4f}\")\n",
    "print(f\"Werkelijk:     w = 2.5, b = 3\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_batch, 'b-', linewidth=2, label='Batch GD')\n",
    "plt.plot(loss_mini, 'g-', linewidth=2, alpha=0.7, label='Mini-batch GD')\n",
    "plt.plot(loss_sgd, 'r-', linewidth=1, alpha=0.5, label='SGD')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Vergelijking GD Varianten')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 5c - Antwoord\n",
    "\n",
    "**Stabiliteit:** Batch GD is het meest stabiel (gladde curve). SGD is het meest ruisig. Mini-batch zit ertussenin.\n",
    "\n",
    "**Snelheid:** SGD en mini-batch convergeren vaak sneller per epoch, maar zijn ruisiger. Batch GD is trager maar consistenter.\n",
    "\n",
    "**Eindresultaat:** Alle drie komen bij ongeveer dezelfde oplossing, mits goed getuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 6: Learning Rate Scheduling - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 6a: GD met decay\n",
    "def gradient_descent_with_decay(X, y, lr_init, decay, n_epochs):\n",
    "    \"\"\"Gradient descent met afnemende learning rate.\"\"\"\n",
    "    w, b = 0.0, 0.0\n",
    "    loss_history = []\n",
    "    lr_history = []\n",
    "    n = len(y)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Bereken huidige learning rate\n",
    "        lr = lr_init / (1 + decay * epoch)\n",
    "        lr_history.append(lr)\n",
    "        \n",
    "        # Voorspelling en gradiënt\n",
    "        y_pred = w * X + b\n",
    "        dw = (2/n) * np.sum((y_pred - y) * X)\n",
    "        db = (2/n) * np.sum(y_pred - y)\n",
    "        \n",
    "        # Update met afnemende lr\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "        \n",
    "        loss_history.append(np.mean((y_pred - y)**2))\n",
    "    \n",
    "    return w, b, loss_history, lr_history\n",
    "\n",
    "# Test\n",
    "w, b, loss, lr = gradient_descent_with_decay(X, y, lr_init=0.05, decay=0.01, n_epochs=200)\n",
    "print(f\"w = {w:.4f}, b = {b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 6b: Vergelijking\n",
    "w_const, b_const, loss_const = batch_gradient_descent(X, y, 0.01, 200)\n",
    "w_decay, b_decay, loss_decay, lr_decay = gradient_descent_with_decay(X, y, 0.05, 0.02, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(loss_const, 'b-', label='Constante lr=0.01')\n",
    "axes[0].plot(loss_decay, 'r-', label='Decay lr (start=0.05)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Vergelijking')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(lr_decay, 'r-')\n",
    "axes[1].axhline(y=0.01, color='b', linestyle='--', label='Constante lr')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Met decay: start snel, verfijn later. Vaak betere convergentie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 7: Logistische Regressie - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 7a: Data genereren\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "X0 = np.random.randn(n_samples // 2, 2) * 0.8 + np.array([-1, -1])\n",
    "y0 = np.zeros(n_samples // 2)\n",
    "\n",
    "X1 = np.random.randn(n_samples // 2, 2) * 0.8 + np.array([1, 1])\n",
    "y1 = np.ones(n_samples // 2)\n",
    "\n",
    "X_class = np.vstack([X0, X1])\n",
    "y_class = np.hstack([y0, y1])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_class[y_class == 0, 0], X_class[y_class == 0, 1], c='blue', label='Klasse 0')\n",
    "plt.scatter(X_class[y_class == 1, 0], X_class[y_class == 1, 1], c='red', label='Klasse 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('Binaire classificatie data')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 7b: Logistische Regressie\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "class LogisticRegressionGD:\n",
    "    def __init__(self, learning_rate=0.1):\n",
    "        self.lr = learning_rate\n",
    "        self.w = None\n",
    "        self.b = 0.0\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        z = X @ self.w + self.b\n",
    "        return sigmoid(z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "    \n",
    "    def fit(self, X, y, n_epochs=100):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Forward\n",
    "            y_pred = self.predict_proba(X)\n",
    "            \n",
    "            # Gradiënten (voor cross-entropy loss)\n",
    "            errors = y_pred - y\n",
    "            dw = (1/n_samples) * (X.T @ errors)\n",
    "            db = (1/n_samples) * np.sum(errors)\n",
    "            \n",
    "            # Update\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "            \n",
    "            # Log loss\n",
    "            eps = 1e-15\n",
    "            y_pred_clipped = np.clip(y_pred, eps, 1 - eps)\n",
    "            loss = -np.mean(y * np.log(y_pred_clipped) + (1-y) * np.log(1-y_pred_clipped))\n",
    "            self.loss_history.append(loss)\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Train\n",
    "log_model = LogisticRegressionGD(learning_rate=0.5)\n",
    "log_model.fit(X_class, y_class, n_epochs=200)\n",
    "\n",
    "# Evalueer\n",
    "predictions = log_model.predict(X_class)\n",
    "accuracy = np.mean(predictions == y_class)\n",
    "print(f\"Nauwkeurigheid: {accuracy * 100:.1f}%\")\n",
    "print(f\"Weights: {log_model.w}\")\n",
    "print(f\"Bias: {log_model.b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 7c: Decision boundary\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Decision boundary\n",
    "x_min, x_max = X_class[:, 0].min() - 1, X_class[:, 0].max() + 1\n",
    "y_min, y_max = X_class[:, 1].min() - 1, X_class[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "Z = log_model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[0].contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.6)\n",
    "axes[0].scatter(X_class[y_class == 0, 0], X_class[y_class == 0, 1], c='blue', edgecolor='k')\n",
    "axes[0].scatter(X_class[y_class == 1, 0], X_class[y_class == 1, 1], c='red', edgecolor='k')\n",
    "axes[0].contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].set_title('Decision Boundary')\n",
    "\n",
    "# Loss curve\n",
    "axes[1].plot(log_model.loss_history, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Cross-Entropy Loss')\n",
    "axes[1].set_title('Training Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 8: Momentum - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 8a: GD met momentum\n",
    "def gradient_descent_momentum_2d(f, grad_f, xy_init, learning_rate, momentum, n_iterations):\n",
    "    \"\"\"Gradient descent met momentum.\"\"\"\n",
    "    xy = np.array(xy_init, dtype=float)\n",
    "    v = np.zeros(2)  # Velocity\n",
    "    history = [xy.copy()]\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        gradient = grad_f(xy[0], xy[1])\n",
    "        v = momentum * v - learning_rate * gradient\n",
    "        xy = xy + v\n",
    "        history.append(xy.copy())\n",
    "    \n",
    "    return xy, np.array(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 8b: Vergelijking op langgerekte vallei\n",
    "def f_elongated(x, y):\n",
    "    return 0.1 * x**2 + 2 * y**2\n",
    "\n",
    "def grad_f_elongated(x, y):\n",
    "    return np.array([0.2 * x, 4 * y])\n",
    "\n",
    "# Start punt\n",
    "xy_init = [10, 1]\n",
    "n_iter = 50\n",
    "lr = 0.1\n",
    "\n",
    "# Zonder momentum\n",
    "xy_no_mom, hist_no_mom = gradient_descent_2d(f_elongated, grad_f_elongated, xy_init, lr, n_iter)\n",
    "\n",
    "# Met momentum\n",
    "xy_mom, hist_mom = gradient_descent_momentum_2d(f_elongated, grad_f_elongated, xy_init, lr, 0.9, n_iter)\n",
    "\n",
    "print(f\"Zonder momentum: ({xy_no_mom[0]:.4f}, {xy_no_mom[1]:.4f})\")\n",
    "print(f\"Met momentum:    ({xy_mom[0]:.4f}, {xy_mom[1]:.4f})\")\n",
    "\n",
    "# Visualisatie\n",
    "x_range = np.linspace(-2, 12, 100)\n",
    "y_range = np.linspace(-2, 2, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f_elongated(X, Y)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.contour(X, Y, Z, levels=30, cmap='viridis')\n",
    "plt.plot(hist_no_mom[:, 0], hist_no_mom[:, 1], 'ro-', markersize=3, label='Zonder momentum')\n",
    "plt.plot(0, 0, 'g*', markersize=15)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Zonder Momentum')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.contour(X, Y, Z, levels=30, cmap='viridis')\n",
    "plt.plot(hist_mom[:, 0], hist_mom[:, 1], 'bo-', markersize=3, label='Met momentum (0.9)')\n",
    "plt.plot(0, 0, 'g*', markersize=15)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Met Momentum')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMomentum helpt om sneller door de langgerekte vallei te navigeren!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 6 Oplossingen | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
