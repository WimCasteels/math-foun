{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 6: Gradient Descent\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Recap en Motivatie\n",
    "\n",
    "In de vorige les hebben we geleerd hoe we afgeleiden berekenen: hoe een functie verandert als we de input veranderen. We kunnen nu berekenen hoe de output van een neuron verandert als we de weights aanpassen.\n",
    "\n",
    "Maar weten hoeveel iets verandert is niet hetzelfde als weten wat de beste waarde is. Het doel van een neuraal netwerk is om de \"fout\" te minimaliseren - de discrepantie tussen wat het netwerk voorspelt en wat de werkelijke waarde is. Dit is een optimalisatieprobleem.\n",
    "\n",
    "Gradient descent is het standaard algoritme om dit te doen. Het idee is verrassend simpel: als de gradiënt je vertelt welke kant omhoog is, ga dan de andere kant op. Stap voor stap dalen we af naar het minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Leerdoelen\n",
    "\n",
    "Na deze les begrijp je het optimalisatieprobleem van neurale netwerken. Je kunt loss functions definiëren en berekenen. Je begrijpt gradient descent intuïtief en wiskundig. Je kent het effect van de learning rate. Je kent de varianten: batch, stochastic en mini-batch gradient descent. Je kunt gradient descent implementeren voor eenvoudige problemen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Loss Functions\n",
    "\n",
    "### Waarom hebben we een loss functie nodig?\n",
    "\n",
    "Een loss functie (ook wel cost functie of objective functie genoemd) is een maat voor hoe \"slecht\" het model presteert. Het is een enkel getal dat aangeeft hoever de voorspellingen van het model afwijken van de werkelijke waarden.\n",
    "\n",
    "Het doel van training is deze loss te minimaliseren. De loss is een functie van alle parameters (weights en biases) van het netwerk: L(w₁, w₂, ..., wₙ).\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "Voor regressieproblemen (waar we een continue waarde voorspellen) is Mean Squared Error de meest gebruikte loss:\n",
    "\n",
    "MSE = (1/n) Σ (voorspelling - werkelijk)²\n",
    "\n",
    "We nemen het kwadraat zodat positieve en negatieve fouten niet tegen elkaar wegvallen, en zodat grote fouten zwaarder wegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"Bereken Mean Squared Error.\"\"\"\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "# Voorbeeld\n",
    "y_true = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_pred_good = np.array([1.1, 2.0, 2.9, 4.1, 5.0])  # Goede voorspelling\n",
    "y_pred_bad = np.array([2.0, 3.5, 1.0, 5.0, 3.0])   # Slechte voorspelling\n",
    "\n",
    "print(\"Werkelijke waarden:\", y_true)\n",
    "print()\n",
    "print(\"Goede voorspelling: \", y_pred_good)\n",
    "print(f\"MSE = {mse_loss(y_pred_good, y_true):.4f}\")\n",
    "print()\n",
    "print(\"Slechte voorspelling:\", y_pred_bad)\n",
    "print(f\"MSE = {mse_loss(y_pred_bad, y_true):.4f}\")\n",
    "print()\n",
    "print(\"Lagere MSE = betere voorspelling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss\n",
    "\n",
    "Voor classificatieproblemen gebruiken we vaak Cross-Entropy Loss. Dit meet hoe ver de voorspelde kansverdeling afwijkt van de werkelijke verdeling.\n",
    "\n",
    "Voor binaire classificatie:\n",
    "L = -[y·log(p) + (1-y)·log(1-p)]\n",
    "\n",
    "waarbij y de werkelijke klasse is (0 of 1) en p de voorspelde kans is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy\n",
    "def binary_cross_entropy(y_pred, y_true, epsilon=1e-15):\n",
    "    \"\"\"Bereken Binary Cross-Entropy Loss.\"\"\"\n",
    "    # Clip om log(0) te voorkomen\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Voorbeeld: classificatie met 2 klassen\n",
    "y_true = np.array([1, 0, 1, 1, 0])  # Werkelijke klassen\n",
    "\n",
    "# Goede voorspellingen (hoge kans voor klasse 1 waar y=1)\n",
    "y_pred_good = np.array([0.9, 0.1, 0.8, 0.95, 0.2])\n",
    "\n",
    "# Slechte voorspellingen\n",
    "y_pred_bad = np.array([0.4, 0.6, 0.3, 0.5, 0.7])\n",
    "\n",
    "print(\"Werkelijke klassen:\", y_true)\n",
    "print()\n",
    "print(\"Goede voorspellingen:\", y_pred_good)\n",
    "print(f\"Cross-Entropy = {binary_cross_entropy(y_pred_good, y_true):.4f}\")\n",
    "print()\n",
    "print(\"Slechte voorspellingen:\", y_pred_bad)\n",
    "print(f\"Cross-Entropy = {binary_cross_entropy(y_pred_bad, y_true):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Het Optimalisatielandschap\n",
    "\n",
    "De loss functie kan worden gezien als een \"landschap\" over de parameterruimte. Elke combinatie van parameters geeft een bepaalde loss, en we zoeken het punt met de laagste loss.\n",
    "\n",
    "Laten we dit visualiseren voor een simpele functie met twee parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer een loss landschap\n",
    "# L(w1, w2) = (w1 - 2)² + (w2 - 3)² + 1\n",
    "# Minimum op (2, 3) met waarde 1\n",
    "\n",
    "def loss_landscape(w1, w2):\n",
    "    return (w1 - 2)**2 + (w2 - 3)**2 + 1\n",
    "\n",
    "# Maak een grid\n",
    "w1_range = np.linspace(-2, 6, 100)\n",
    "w2_range = np.linspace(-1, 7, 100)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "L = loss_landscape(W1, W2)\n",
    "\n",
    "# 3D plot\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(W1, W2, L, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('w₁')\n",
    "ax1.set_ylabel('w₂')\n",
    "ax1.set_zlabel('Loss')\n",
    "ax1.set_title('Loss landschap (3D)')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(W1, W2, L, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.plot(2, 3, 'r*', markersize=15, label='Minimum (2, 3)')\n",
    "ax2.set_xlabel('w₁')\n",
    "ax2.set_ylabel('w₂')\n",
    "ax2.set_title('Loss landschap (contour)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Het doel is om van een willekeurig startpunt naar het minimum te komen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Een complexer landschap met lokale minima\n",
    "def complex_landscape(w1, w2):\n",
    "    return np.sin(w1) * np.cos(w2) + 0.1 * (w1**2 + w2**2)\n",
    "\n",
    "w1_range = np.linspace(-4, 4, 100)\n",
    "w2_range = np.linspace(-4, 4, 100)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "L_complex = complex_landscape(W1, W2)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(W1, W2, L_complex, cmap='coolwarm', alpha=0.8)\n",
    "ax1.set_xlabel('w₁')\n",
    "ax1.set_ylabel('w₂')\n",
    "ax1.set_zlabel('Loss')\n",
    "ax1.set_title('Complex landschap met meerdere minima')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(W1, W2, L_complex, levels=30, cmap='coolwarm')\n",
    "ax2.set_xlabel('w₁')\n",
    "ax2.set_ylabel('w₂')\n",
    "ax2.set_title('Contour - let op de meerdere \"dalen\"')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"In de praktijk kan het landschap zeer complex zijn.\")\n",
    "print(\"We kunnen vastlopen in lokale minima.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Gradient Descent Algoritme\n",
    "\n",
    "### Het idee\n",
    "\n",
    "De gradiënt van de loss functie wijst in de richting van de steilste stijging. Als we de loss willen minimaliseren, moeten we dus de tegengestelde richting op: de negatieve gradiënt.\n",
    "\n",
    "### De update regel\n",
    "\n",
    "w_new = w_old - η · ∇L(w)\n",
    "\n",
    "waarbij η (eta) de learning rate is, een hyperparameter die bepaalt hoe grote stappen we nemen.\n",
    "\n",
    "### Het algoritme\n",
    "\n",
    "1. Initialiseer parameters willekeurig\n",
    "2. Bereken de gradiënt van de loss\n",
    "3. Update de parameters: w = w - η · ∇L\n",
    "4. Herhaal stap 2-3 tot convergentie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent voor een simpele 1D functie\n",
    "# f(x) = x² - 4x + 5 = (x-2)² + 1\n",
    "# Minimum op x = 2\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 4*x + 5\n",
    "\n",
    "def f_gradient(x):\n",
    "    return 2*x - 4\n",
    "\n",
    "# Gradient descent\n",
    "def gradient_descent_1d(f, f_grad, x_init, learning_rate, n_iterations):\n",
    "    x = x_init\n",
    "    history = [x]\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        grad = f_grad(x)\n",
    "        x = x - learning_rate * grad\n",
    "        history.append(x)\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Run\n",
    "x_init = 6.0\n",
    "learning_rate = 0.1\n",
    "n_iter = 20\n",
    "\n",
    "x_final, history = gradient_descent_1d(f, f_gradient, x_init, learning_rate, n_iter)\n",
    "\n",
    "print(f\"Start: x = {x_init}, f(x) = {f(x_init)}\")\n",
    "print(f\"Eind:  x = {x_final:.6f}, f(x) = {f(x_final):.6f}\")\n",
    "print(f\"Optimum: x = 2, f(x) = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisatie van het pad\n",
    "x_range = np.linspace(-1, 7, 100)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Links: functie met pad\n",
    "plt.subplot(121)\n",
    "plt.plot(x_range, f(x_range), 'b-', linewidth=2, label='f(x) = x² - 4x + 5')\n",
    "history_arr = np.array(history)\n",
    "plt.plot(history_arr, f(history_arr), 'ro-', markersize=8, alpha=0.7, label='GD pad')\n",
    "plt.plot(history_arr[0], f(history_arr[0]), 'g^', markersize=15, label='Start')\n",
    "plt.plot(2, 1, 'r*', markersize=20, label='Minimum')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent op f(x) = x² - 4x + 5')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Rechts: convergentie\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(history)), f(np.array(history)), 'b-', linewidth=2)\n",
    "plt.xlabel('Iteratie')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Convergentie van de loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Gradient Descent\n",
    "def gradient_descent_2d(loss_fn, grad_fn, w_init, learning_rate, n_iterations):\n",
    "    w = np.array(w_init, dtype=float)\n",
    "    history = [w.copy()]\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        grad = grad_fn(w[0], w[1])\n",
    "        w = w - learning_rate * np.array(grad)\n",
    "        history.append(w.copy())\n",
    "    \n",
    "    return w, history\n",
    "\n",
    "# Loss: L(w1, w2) = (w1-2)² + (w2-3)² + 1\n",
    "def loss_2d(w1, w2):\n",
    "    return (w1 - 2)**2 + (w2 - 3)**2 + 1\n",
    "\n",
    "def grad_2d(w1, w2):\n",
    "    return [2*(w1 - 2), 2*(w2 - 3)]\n",
    "\n",
    "# Run\n",
    "w_init = [-1, 6]\n",
    "w_final, history_2d = gradient_descent_2d(loss_2d, grad_2d, w_init, 0.1, 30)\n",
    "\n",
    "print(f\"Start: w = {w_init}, Loss = {loss_2d(*w_init):.4f}\")\n",
    "print(f\"Eind:  w = [{w_final[0]:.4f}, {w_final[1]:.4f}], Loss = {loss_2d(*w_final):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisatie 2D GD\n",
    "w1_range = np.linspace(-2, 6, 100)\n",
    "w2_range = np.linspace(-1, 8, 100)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "L = loss_2d(W1, W2)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(W1, W2, L, levels=30, cmap='viridis')\n",
    "plt.colorbar(label='Loss')\n",
    "\n",
    "# Plot het pad\n",
    "history_arr = np.array(history_2d)\n",
    "plt.plot(history_arr[:, 0], history_arr[:, 1], 'ro-', markersize=6, linewidth=2, label='GD pad')\n",
    "plt.plot(history_arr[0, 0], history_arr[0, 1], 'g^', markersize=15, label='Start')\n",
    "plt.plot(2, 3, 'r*', markersize=20, label='Minimum')\n",
    "\n",
    "plt.xlabel('w₁', fontsize=12)\n",
    "plt.ylabel('w₂', fontsize=12)\n",
    "plt.title('Gradient Descent in 2D', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 De Learning Rate\n",
    "\n",
    "De learning rate η is een cruciale hyperparameter. Het bepaalt hoe grote stappen we nemen in de richting van de negatieve gradiënt.\n",
    "\n",
    "- **Te groot**: We kunnen over het minimum heen schieten (overshoot), oscilleren, of zelfs divergeren.\n",
    "- **Te klein**: Convergentie is zeer traag, en we kunnen vastlopen in lokale minima.\n",
    "- **Just right**: Snelle, stabiele convergentie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect van verschillende learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "x_range = np.linspace(-1, 7, 100)\n",
    "\n",
    "for ax, lr in zip(axes.flatten(), learning_rates):\n",
    "    x_final, history = gradient_descent_1d(f, f_gradient, 6.0, lr, 20)\n",
    "    history_arr = np.array(history)\n",
    "    \n",
    "    ax.plot(x_range, f(x_range), 'b-', linewidth=2)\n",
    "    ax.plot(history_arr, f(history_arr), 'ro-', markersize=6, alpha=0.7)\n",
    "    ax.plot(2, 1, 'r*', markersize=15)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.set_title(f'Learning rate = {lr}')\n",
    "    ax.set_ylim(-1, 30)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observaties:\")\n",
    "print(\"- lr=0.01: Te langzaam, convergeert nog niet na 20 stappen\")\n",
    "print(\"- lr=0.1:  Goede balans, convergeert netjes\")\n",
    "print(\"- lr=0.5:  Snel maar begint te oscilleren\")\n",
    "print(\"- lr=0.9:  Oscilleert wild, convergeert nauwelijks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergentie curves vergelijken\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for lr in [0.01, 0.05, 0.1, 0.3, 0.5]:\n",
    "    _, history = gradient_descent_1d(f, f_gradient, 6.0, lr, 50)\n",
    "    plt.plot(f(np.array(history)), label=f'lr={lr}')\n",
    "\n",
    "plt.xlabel('Iteratie', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Effect van learning rate op convergentie', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Varianten van Gradient Descent\n",
    "\n",
    "In de praktijk hebben we niet één data punt maar een hele dataset. Er zijn drie hoofdvarianten voor hoe we de gradiënt berekenen:\n",
    "\n",
    "### Batch Gradient Descent\n",
    "Bereken de gradiënt over de **hele dataset** voor elke update. \n",
    "- ✓ Nauwkeurige gradiënt\n",
    "- ✗ Traag voor grote datasets\n",
    "- ✗ Kan vastlopen in lokale minima\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "Bereken de gradiënt op **één willekeurig sample** per update.\n",
    "- ✓ Snel per iteratie\n",
    "- ✓ Ruis helpt ontsnappen aan lokale minima\n",
    "- ✗ Zeer ruisige updates, kan oscilleren\n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "Bereken de gradiënt op een **kleine batch** (bijv. 32 samples) per update.\n",
    "- ✓ Beste van beide werelden\n",
    "- ✓ Efficiënt (GPU parallelisatie)\n",
    "- Dit is de standaard in deep learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulatie van de drie varianten voor lineaire regressie\n",
    "\n",
    "# Genereer data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 1)\n",
    "y_true = 3 * X[:, 0] + 2 + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Model: y = w*x + b\n",
    "def predict(X, w, b):\n",
    "    return X[:, 0] * w + b\n",
    "\n",
    "def mse(X, y, w, b):\n",
    "    return np.mean((predict(X, w, b) - y) ** 2)\n",
    "\n",
    "def gradient(X, y, w, b):\n",
    "    n = len(y)\n",
    "    y_pred = predict(X, w, b)\n",
    "    dw = (2/n) * np.sum((y_pred - y) * X[:, 0])\n",
    "    db = (2/n) * np.sum(y_pred - y)\n",
    "    return dw, db\n",
    "\n",
    "# Batch GD\n",
    "def batch_gd(X, y, w_init, b_init, lr, n_epochs):\n",
    "    w, b = w_init, b_init\n",
    "    history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        dw, db = gradient(X, y, w, b)\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "        history.append(mse(X, y, w, b))\n",
    "    return w, b, history\n",
    "\n",
    "# SGD\n",
    "def sgd(X, y, w_init, b_init, lr, n_epochs):\n",
    "    w, b = w_init, b_init\n",
    "    history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(len(y))\n",
    "        for i in indices:\n",
    "            Xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            dw, db = gradient(Xi, yi, w, b)\n",
    "            w -= lr * dw\n",
    "            b -= lr * db\n",
    "        history.append(mse(X, y, w, b))\n",
    "    return w, b, history\n",
    "\n",
    "# Mini-batch GD\n",
    "def minibatch_gd(X, y, w_init, b_init, lr, n_epochs, batch_size=16):\n",
    "    w, b = w_init, b_init\n",
    "    history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        indices = np.random.permutation(len(y))\n",
    "        for start in range(0, len(y), batch_size):\n",
    "            idx = indices[start:start+batch_size]\n",
    "            Xi = X[idx]\n",
    "            yi = y[idx]\n",
    "            dw, db = gradient(Xi, yi, w, b)\n",
    "            w -= lr * dw\n",
    "            b -= lr * db\n",
    "        history.append(mse(X, y, w, b))\n",
    "    return w, b, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergelijk de drie methodes\n",
    "w_init, b_init = 0.0, 0.0\n",
    "n_epochs = 50\n",
    "\n",
    "w_batch, b_batch, hist_batch = batch_gd(X, y_true, w_init, b_init, 0.1, n_epochs)\n",
    "w_sgd, b_sgd, hist_sgd = sgd(X, y_true, w_init, b_init, 0.01, n_epochs)  # Kleinere lr voor SGD\n",
    "w_mini, b_mini, hist_mini = minibatch_gd(X, y_true, w_init, b_init, 0.1, n_epochs, batch_size=16)\n",
    "\n",
    "print(\"Resultaten na 50 epochs:\")\n",
    "print(f\"Werkelijke parameters: w=3, b=2\")\n",
    "print()\n",
    "print(f\"Batch GD:     w={w_batch:.4f}, b={b_batch:.4f}, MSE={hist_batch[-1]:.4f}\")\n",
    "print(f\"SGD:          w={w_sgd:.4f}, b={b_sgd:.4f}, MSE={hist_sgd[-1]:.4f}\")\n",
    "print(f\"Mini-batch:   w={w_mini:.4f}, b={b_mini:.4f}, MSE={hist_mini[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisatie\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hist_batch, 'b-', linewidth=2, label='Batch GD')\n",
    "plt.plot(hist_sgd, 'r-', alpha=0.7, label='SGD')\n",
    "plt.plot(hist_mini, 'g-', linewidth=2, label='Mini-batch GD')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.title('Vergelijking van GD varianten', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"SGD is ruisiger maar convergeert naar dezelfde oplossing.\")\n",
    "print(\"Mini-batch is een goede balans.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Toepassing: Linear Regression met Gradient Descent\n",
    "\n",
    "Laten we een volledige lineaire regressie implementeren met gradient descent en visualiseren hoe het model leert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereer data\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "X_train = np.random.uniform(0, 10, n_samples)\n",
    "y_train = 2.5 * X_train + 3 + np.random.randn(n_samples) * 2\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, alpha=0.7)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Training data voor lineaire regressie')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineaire regressie met gradient descent\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "        self.w = 0.0\n",
    "        self.b = 0.0\n",
    "        self.history = []\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.w * X + self.b\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        return np.mean((self.predict(X) - y) ** 2)\n",
    "    \n",
    "    def fit(self, X, y, n_epochs=100, verbose=True):\n",
    "        n = len(y)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Forward\n",
    "            y_pred = self.predict(X)\n",
    "            \n",
    "            # Gradiënten\n",
    "            dw = (2/n) * np.sum((y_pred - y) * X)\n",
    "            db = (2/n) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Update\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "            \n",
    "            # Log\n",
    "            current_loss = self.loss(X, y)\n",
    "            self.history.append({\n",
    "                'epoch': epoch,\n",
    "                'loss': current_loss,\n",
    "                'w': self.w,\n",
    "                'b': self.b\n",
    "            })\n",
    "            \n",
    "            if verbose and epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch:3d}: Loss = {current_loss:.4f}, w = {self.w:.4f}, b = {self.b:.4f}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Train\n",
    "model = LinearRegressionGD(learning_rate=0.01)\n",
    "model.fit(X_train, y_train, n_epochs=100)\n",
    "\n",
    "print()\n",
    "print(f\"Geleerde parameters: w = {model.w:.4f}, b = {model.b:.4f}\")\n",
    "print(f\"Werkelijke relatie:  y = 2.5x + 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer de fit en het leerproces\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Data met fit\n",
    "axes[0].scatter(X_train, y_train, alpha=0.7, label='Data')\n",
    "x_line = np.linspace(0, 10, 100)\n",
    "axes[0].plot(x_line, model.predict(x_line), 'r-', linewidth=2, label=f'Fit: y = {model.w:.2f}x + {model.b:.2f}')\n",
    "axes[0].plot(x_line, 2.5*x_line + 3, 'g--', linewidth=2, alpha=0.5, label='Werkelijk: y = 2.5x + 3')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Lineaire Regressie Fit')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss curve\n",
    "losses = [h['loss'] for h in model.history]\n",
    "axes[1].plot(losses, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].set_title('Training Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter evolutie\n",
    "ws = [h['w'] for h in model.history]\n",
    "bs = [h['b'] for h in model.history]\n",
    "axes[2].plot(ws, label='w')\n",
    "axes[2].plot(bs, label='b')\n",
    "axes[2].axhline(y=2.5, color='blue', linestyle='--', alpha=0.5)\n",
    "axes[2].axhline(y=3, color='orange', linestyle='--', alpha=0.5)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Parameter waarde')\n",
    "axes[2].set_title('Parameter Evolutie')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 Samenvatting en Vooruitblik\n",
    "\n",
    "### Kernconcepten\n",
    "\n",
    "Een loss functie meet hoe slecht het model presteert. MSE voor regressie, Cross-Entropy voor classificatie. Het doel is de loss te minimaliseren.\n",
    "\n",
    "Gradient descent is het standaard optimalisatie-algoritme: update parameters in de richting van de negatieve gradiënt. De update regel is w = w - η·∇L.\n",
    "\n",
    "De learning rate η bepaalt de stapgrootte. Te groot = instabiliteit, te klein = trage convergentie.\n",
    "\n",
    "Mini-batch gradient descent is de standaard in deep learning: een goede balans tussen nauwkeurigheid en snelheid.\n",
    "\n",
    "### Wat ontbreekt nog?\n",
    "\n",
    "We hebben gradient descent toegepast op eenvoudige problemen waar we de gradiënt makkelijk konden berekenen. Maar in een diep neuraal netwerk met miljoenen parameters, hoe berekenen we dan de gradiënt van de loss naar elke parameter?\n",
    "\n",
    "### Volgende les\n",
    "\n",
    "In les 7 leren we backpropagation: het algoritme dat de kettingregel efficiënt toepast om gradiënten door het hele netwerk te berekenen. We zullen een volledig neuraal netwerk trainen op MNIST!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist\n",
    "\n",
    "Controleer of je het volgende begrijpt:\n",
    "\n",
    "1. Wat is een loss functie en waarom hebben we deze nodig?\n",
    "\n",
    "2. Wat is de update regel van gradient descent?\n",
    "\n",
    "3. Wat gebeurt er als de learning rate te groot of te klein is?\n",
    "\n",
    "4. Wat is het verschil tussen batch, stochastic en mini-batch GD?\n",
    "\n",
    "5. Waarom is mini-batch GD de standaard?\n",
    "\n",
    "Als je deze vragen kunt beantwoorden, ben je klaar voor les 7!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 6 van 12 | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
