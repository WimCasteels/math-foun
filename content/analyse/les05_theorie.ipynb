{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 5: Afgeleiden en de Kettingregel\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Recap en Motivatie\n",
    "\n",
    "In de vorige lessen hebben we geleerd hoe data door een neuraal netwerk stroomt. We kunnen nu een forward pass uitvoeren: gegeven een input, berekenen we de output via matrixvermenigvuldigingen en activatiefuncties. Maar tot nu toe waren onze netwerken \"dom\" - ze hadden random gewichten en maakten willekeurige voorspellingen.\n",
    "\n",
    "De grote vraag is: hoe kan een netwerk leren? Hoe passen we de gewichten aan zodat de voorspellingen beter worden?\n",
    "\n",
    "Het kernidee is verrassend eenvoudig. We meten hoe \"fout\" het netwerk is via een loss functie. Dan vragen we: als ik een bepaalde weight een klein beetje verander, wordt de fout dan groter of kleiner? En hoeveel? Als we dit voor alle weights weten, kunnen we ze allemaal een beetje aanpassen in de richting die de fout verkleint.\n",
    "\n",
    "Dit \"hoeveel verandert de fout als ik de weight verander\" is precies wat een afgeleide meet. In deze les leren we afgeleiden: de wiskundige taal van verandering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Leerdoelen\n",
    "\n",
    "Na deze les kun je de afgeleide intuïtief begrijpen als de maat voor verandering. Je kunt afgeleiden numeriek benaderen en analytisch berekenen voor basisregels. Je beheerst de kettingregel voor samengestelde functies. Je kunt partiële afgeleiden berekenen en de gradiënt vormen. Je begrijpt waarom dit essentieel is voor het trainen van neurale netwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 De Afgeleide: Intuïtie\n",
    "\n",
    "### Wat meet een afgeleide?\n",
    "\n",
    "Stel je voor dat je met de auto rijdt en je kijkt naar de snelheidsmeter. De snelheid vertelt je hoe snel je positie verandert. Als je 60 km/u rijdt, dan verander je elke seconde ongeveer 16.7 meter van positie.\n",
    "\n",
    "De snelheid is de afgeleide van positie naar tijd. Meer algemeen: de afgeleide van een functie f(x) op een punt x vertelt je hoe snel f verandert als x verandert. Het is de helling van de functie op dat punt.\n",
    "\n",
    "Wiskundig benaderen we dit door te kijken naar het verschil f(x+h) - f(x) voor een kleine stap h, gedeeld door h:\n",
    "\n",
    "f'(x) ≈ (f(x+h) - f(x)) / h\n",
    "\n",
    "Als h steeds kleiner wordt, nadert deze benadering de echte afgeleide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorbeeld: f(x) = x²\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Numerieke afgeleide\n",
    "def numerical_derivative(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "# Test op x = 3\n",
    "x = 3\n",
    "print(f\"f(x) = x² op x = {x}\")\n",
    "print(f\"f({x}) = {f(x)}\")\n",
    "print()\n",
    "\n",
    "# Numerieke benadering met verschillende h\n",
    "print(\"Numerieke afgeleide met verschillende stapgroottes:\")\n",
    "for h in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "    deriv = numerical_derivative(f, x, h)\n",
    "    print(f\"  h = {h:8.5f}: f'({x}) ≈ {deriv:.6f}\")\n",
    "\n",
    "print()\n",
    "print(f\"Analytische afgeleide: f'(x) = 2x, dus f'({x}) = {2*x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisatie: de afgeleide als helling van de raaklijn\n",
    "x_range = np.linspace(0, 5, 100)\n",
    "y_range = f(x_range)\n",
    "\n",
    "# Punt waar we de afgeleide berekenen\n",
    "x0 = 2\n",
    "y0 = f(x0)\n",
    "slope = 2 * x0  # Afgeleide van x² is 2x\n",
    "\n",
    "# Raaklijn: y - y0 = slope * (x - x0)\n",
    "tangent_y = slope * (x_range - x0) + y0\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, y_range, 'b-', linewidth=2, label='f(x) = x²')\n",
    "plt.plot(x_range, tangent_y, 'r--', linewidth=2, label=f'Raaklijn (helling = {slope})')\n",
    "plt.plot(x0, y0, 'go', markersize=10, label=f'Punt ({x0}, {y0})')\n",
    "\n",
    "plt.xlim(0, 5)\n",
    "plt.ylim(0, 20)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title(f'De afgeleide is de helling van de raaklijn\\nf\\'({x0}) = {slope}', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Op x = {x0}: de helling is {slope}, dus als x met 1 toeneemt,\")\n",
    "print(f\"neemt f(x) met ongeveer {slope} toe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De afgeleide op verschillende punten\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "\n",
    "for ax, x0 in zip(axes, [-1, 0, 2]):\n",
    "    y_range = x_range ** 2\n",
    "    y0 = x0 ** 2\n",
    "    slope = 2 * x0\n",
    "    tangent_y = slope * (x_range - x0) + y0\n",
    "    \n",
    "    ax.plot(x_range, y_range, 'b-', linewidth=2)\n",
    "    ax.plot(x_range, tangent_y, 'r--', linewidth=2)\n",
    "    ax.plot(x0, y0, 'go', markersize=10)\n",
    "    \n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-2, 8)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_title(f'x = {x0}: helling = {slope}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.suptitle('f(x) = x²: de afgeleide f\\'(x) = 2x op verschillende punten', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Basisregels voor Afgeleiden\n",
    "\n",
    "In plaats van elke afgeleide numeriek te berekenen, kunnen we analytische regels gebruiken. Deze regels zijn sneller en nauwkeuriger. De belangrijkste regels zijn:\n",
    "\n",
    "**Constante regel:** De afgeleide van een constante is nul.\n",
    "d/dx[c] = 0\n",
    "\n",
    "**Machtregel:** De afgeleide van xⁿ is n·xⁿ⁻¹.\n",
    "d/dx[xⁿ] = n·xⁿ⁻¹\n",
    "\n",
    "**Somregel:** De afgeleide van een som is de som van de afgeleiden.\n",
    "d/dx[f(x) + g(x)] = f'(x) + g'(x)\n",
    "\n",
    "**Constante factor:** Een constante factor blijft behouden.\n",
    "d/dx[c·f(x)] = c·f'(x)\n",
    "\n",
    "**Exponentiële functie:** De afgeleide van eˣ is eˣ zelf.\n",
    "d/dx[eˣ] = eˣ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorbeeld: f(x) = 3x³ + 2x² - 5x + 7\n",
    "def f(x):\n",
    "    return 3*x**3 + 2*x**2 - 5*x + 7\n",
    "\n",
    "# Analytische afgeleide: f'(x) = 9x² + 4x - 5\n",
    "def f_prime(x):\n",
    "    return 9*x**2 + 4*x - 5\n",
    "\n",
    "# Vergelijk numeriek en analytisch\n",
    "print(\"f(x) = 3x³ + 2x² - 5x + 7\")\n",
    "print(\"f'(x) = 9x² + 4x - 5\")\n",
    "print()\n",
    "\n",
    "test_points = [-2, -1, 0, 1, 2, 3]\n",
    "print(\"Vergelijking numeriek vs analytisch:\")\n",
    "print(f\"{'x':>4} | {'Numeriek':>12} | {'Analytisch':>12} | {'Verschil':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for x in test_points:\n",
    "    num = numerical_derivative(f, x)\n",
    "    ana = f_prime(x)\n",
    "    diff = abs(num - ana)\n",
    "    print(f\"{x:>4} | {num:>12.6f} | {ana:>12.6f} | {diff:>12.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De exponentiële functie: uniek!\n",
    "x_range = np.linspace(-2, 2, 100)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, np.exp(x_range), 'b-', linewidth=2, label='f(x) = eˣ')\n",
    "\n",
    "# De afgeleide is dezelfde functie!\n",
    "plt.plot(x_range, np.exp(x_range), 'r--', linewidth=2, label=\"f'(x) = eˣ (zelfde!)\")\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('De exponentiële functie eˣ is zijn eigen afgeleide!', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Dit is waarom e zo speciaal is in wiskunde en natuurkunde.\")\n",
    "print(\"Het is de enige functie (op constante factor na) die gelijk is aan zijn afgeleide.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 De Kettingregel\n",
    "\n",
    "### Het probleem\n",
    "\n",
    "Wat als we de afgeleide willen van een samengestelde functie? Bijvoorbeeld f(x) = (3x + 2)². We kunnen dit uitwerken tot 9x² + 12x + 4 en dan differentiëren, maar voor complexere functies is dat niet praktisch.\n",
    "\n",
    "### De kettingregel\n",
    "\n",
    "De kettingregel zegt: als h(x) = f(g(x)), dan is h'(x) = f'(g(x)) · g'(x).\n",
    "\n",
    "In woorden: de afgeleide van de buitenste functie (geëvalueerd op de binnenste), vermenigvuldigd met de afgeleide van de binnenste functie.\n",
    "\n",
    "### Waarom is dit cruciaal voor neurale netwerken?\n",
    "\n",
    "Een neuraal netwerk is één grote samengestelde functie! De output is iets als: softmax(W₂ · ReLU(W₁ · x + b₁) + b₂). Om de afgeleide naar W₁ te berekenen, moeten we de kettingregel meerdere keren toepassen. Dit is precies wat backpropagation doet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorbeeld: h(x) = (3x + 2)²\n",
    "# g(x) = 3x + 2 (binnenste)\n",
    "# f(u) = u² (buitenste)\n",
    "# h(x) = f(g(x))\n",
    "\n",
    "def g(x):\n",
    "    return 3*x + 2\n",
    "\n",
    "def f(u):\n",
    "    return u**2\n",
    "\n",
    "def h(x):\n",
    "    return f(g(x))  # = (3x + 2)²\n",
    "\n",
    "# Afgeleiden\n",
    "def g_prime(x):\n",
    "    return 3  # d/dx[3x + 2] = 3\n",
    "\n",
    "def f_prime(u):\n",
    "    return 2*u  # d/du[u²] = 2u\n",
    "\n",
    "def h_prime(x):\n",
    "    # Kettingregel: f'(g(x)) * g'(x)\n",
    "    return f_prime(g(x)) * g_prime(x)\n",
    "\n",
    "# Test\n",
    "print(\"h(x) = (3x + 2)²\")\n",
    "print(\"g(x) = 3x + 2  →  g'(x) = 3\")\n",
    "print(\"f(u) = u²      →  f'(u) = 2u\")\n",
    "print()\n",
    "print(\"Kettingregel: h'(x) = f'(g(x)) · g'(x) = 2(3x+2) · 3 = 6(3x+2)\")\n",
    "print()\n",
    "\n",
    "x = 2\n",
    "print(f\"Op x = {x}:\")\n",
    "print(f\"  g({x}) = {g(x)}\")\n",
    "print(f\"  h({x}) = {h(x)}\")\n",
    "print(f\"  h'({x}) analytisch = 6(3·{x}+2) = 6·{g(x)} = {6*g(x)}\")\n",
    "print(f\"  h'({x}) via kettingregel = {h_prime(x)}\")\n",
    "print(f\"  h'({x}) numeriek = {numerical_derivative(h, x):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meer complex voorbeeld: sigmoid functie\n",
    "# σ(x) = 1 / (1 + e^(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Afgeleide van sigmoid (kan je afleiden met de kettingregel)\n",
    "# σ'(x) = σ(x) · (1 - σ(x))\n",
    "def sigmoid_prime(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Visualisatie\n",
    "x_range = np.linspace(-6, 6, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(x_range, sigmoid(x_range), 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('σ(x)', fontsize=12)\n",
    "axes[0].set_title('Sigmoid functie: σ(x) = 1/(1+e⁻ˣ)', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "axes[1].plot(x_range, sigmoid_prime(x_range), 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel(\"σ'(x)\", fontsize=12)\n",
    "axes[1].set_title(\"Afgeleide: σ'(x) = σ(x)·(1-σ(x))\", fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verifieer numeriek\n",
    "x = 1\n",
    "print(f\"\\nVerificatie op x = {x}:\")\n",
    "print(f\"  σ'({x}) analytisch = {sigmoid_prime(x):.6f}\")\n",
    "print(f\"  σ'({x}) numeriek   = {numerical_derivative(sigmoid, x):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU en zijn afgeleide\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1, 0)  # 1 als x > 0, anders 0\n",
    "\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(x_range, relu(x_range), 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('ReLU(x)', fontsize=12)\n",
    "axes[0].set_title('ReLU: max(0, x)', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(x_range, relu_prime(x_range), 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel(\"ReLU'(x)\", fontsize=12)\n",
    "axes[1].set_title(\"Afgeleide: 1 als x > 0, anders 0\", fontsize=12)\n",
    "axes[1].set_ylim(-0.5, 1.5)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"De ReLU afgeleide is 'aan' (1) of 'uit' (0).\")\n",
    "print(\"Dit maakt ReLU computationeel efficiënt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Partiële Afgeleiden\n",
    "\n",
    "### Functies met meerdere variabelen\n",
    "\n",
    "Tot nu toe hadden we functies van één variabele: f(x). Maar in neurale netwerken hebben we te maken met functies van vele variabelen: de loss L hangt af van alle weights w₁, w₂, ..., wₙ.\n",
    "\n",
    "Een partiële afgeleide meet hoe de functie verandert als we één variabele veranderen terwijl we alle andere constant houden.\n",
    "\n",
    "### Notatie\n",
    "\n",
    "De partiële afgeleide van f naar x noteren we als ∂f/∂x (in plaats van df/dx). Het symbool ∂ (\"del\" of \"partial\") geeft aan dat er meerdere variabelen zijn.\n",
    "\n",
    "### De gradiënt\n",
    "\n",
    "De gradiënt van f is de vector van alle partiële afgeleiden:\n",
    "\n",
    "∇f = [∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ]\n",
    "\n",
    "De gradiënt wijst in de richting van de steilste stijging van de functie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorbeeld: f(x, y) = x² + xy + y²\n",
    "def f(x, y):\n",
    "    return x**2 + x*y + y**2\n",
    "\n",
    "# Partiële afgeleiden (analytisch)\n",
    "# ∂f/∂x = 2x + y\n",
    "# ∂f/∂y = x + 2y\n",
    "\n",
    "def df_dx(x, y):\n",
    "    return 2*x + y\n",
    "\n",
    "def df_dy(x, y):\n",
    "    return x + 2*y\n",
    "\n",
    "def gradient(x, y):\n",
    "    return np.array([df_dx(x, y), df_dy(x, y)])\n",
    "\n",
    "# Numerieke partiële afgeleiden\n",
    "def numerical_partial_x(f, x, y, h=1e-5):\n",
    "    return (f(x + h, y) - f(x, y)) / h\n",
    "\n",
    "def numerical_partial_y(f, x, y, h=1e-5):\n",
    "    return (f(x, y + h) - f(x, y)) / h\n",
    "\n",
    "# Test\n",
    "x0, y0 = 2, 3\n",
    "print(f\"f(x, y) = x² + xy + y²\")\n",
    "print(f\"\\nOp punt ({x0}, {y0}):\")\n",
    "print(f\"  f({x0}, {y0}) = {f(x0, y0)}\")\n",
    "print()\n",
    "print(f\"  ∂f/∂x analytisch = 2x + y = 2·{x0} + {y0} = {df_dx(x0, y0)}\")\n",
    "print(f\"  ∂f/∂x numeriek   = {numerical_partial_x(f, x0, y0):.4f}\")\n",
    "print()\n",
    "print(f\"  ∂f/∂y analytisch = x + 2y = {x0} + 2·{y0} = {df_dy(x0, y0)}\")\n",
    "print(f\"  ∂f/∂y numeriek   = {numerical_partial_y(f, x0, y0):.4f}\")\n",
    "print()\n",
    "print(f\"  Gradiënt: ∇f = {gradient(x0, y0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisatie: gradiënt als pijlen op een contourplot\n",
    "x_range = np.linspace(-3, 3, 50)\n",
    "y_range = np.linspace(-3, 3, 50)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f(X, Y)\n",
    "\n",
    "# Gradiënt op een grove grid\n",
    "x_arrows = np.linspace(-2.5, 2.5, 8)\n",
    "y_arrows = np.linspace(-2.5, 2.5, 8)\n",
    "X_arr, Y_arr = np.meshgrid(x_arrows, y_arrows)\n",
    "\n",
    "# Gradiënt componenten\n",
    "U = df_dx(X_arr, Y_arr)\n",
    "V = df_dy(X_arr, Y_arr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "plt.colorbar(label='f(x, y)')\n",
    "plt.quiver(X_arr, Y_arr, U, V, color='red', alpha=0.7)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('f(x, y) = x² + xy + y²\\nRode pijlen: gradiënt (richting van steilste stijging)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"De gradiënt wijst altijd loodrecht op de contourlijnen,\")\n",
    "print(\"in de richting waar de functie het snelst stijgt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Toepassing: Afgeleide van een Simpel Neuron\n",
    "\n",
    "Laten we alles samenvoegen en de afgeleiden berekenen voor een enkel neuron. Een neuron berekent:\n",
    "\n",
    "y = σ(wx + b)\n",
    "\n",
    "waarbij σ de sigmoid activatiefunctie is. We willen weten hoe de output y verandert als we w of b aanpassen. Dit zijn de partiële afgeleiden ∂y/∂w en ∂y/∂b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron: y = σ(wx + b)\n",
    "\n",
    "def neuron(x, w, b):\n",
    "    \"\"\"Bereken de output van een neuron.\"\"\"\n",
    "    z = w * x + b  # Lineaire combinatie\n",
    "    y = sigmoid(z)  # Activatie\n",
    "    return y\n",
    "\n",
    "# Afgeleiden met de kettingregel\n",
    "# y = σ(z) waar z = wx + b\n",
    "# ∂y/∂w = ∂y/∂z · ∂z/∂w = σ'(z) · x\n",
    "# ∂y/∂b = ∂y/∂z · ∂z/∂b = σ'(z) · 1\n",
    "\n",
    "def neuron_gradients(x, w, b):\n",
    "    \"\"\"Bereken de gradiënten van een neuron.\"\"\"\n",
    "    z = w * x + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # σ'(z) = σ(z)(1 - σ(z)) = y(1 - y)\n",
    "    dy_dz = y * (1 - y)\n",
    "    \n",
    "    # Kettingregel\n",
    "    dy_dw = dy_dz * x  # ∂z/∂w = x\n",
    "    dy_db = dy_dz * 1  # ∂z/∂b = 1\n",
    "    \n",
    "    return dy_dw, dy_db\n",
    "\n",
    "# Test\n",
    "x = 2.0\n",
    "w = 0.5\n",
    "b = -0.3\n",
    "\n",
    "y = neuron(x, w, b)\n",
    "dy_dw, dy_db = neuron_gradients(x, w, b)\n",
    "\n",
    "print(f\"Neuron: y = σ(wx + b)\")\n",
    "print(f\"\\nInput: x = {x}, w = {w}, b = {b}\")\n",
    "print(f\"\\nz = wx + b = {w}·{x} + {b} = {w*x + b}\")\n",
    "print(f\"y = σ(z) = {y:.6f}\")\n",
    "print()\n",
    "print(f\"Gradiënten:\")\n",
    "print(f\"  ∂y/∂w = {dy_dw:.6f}\")\n",
    "print(f\"  ∂y/∂b = {dy_db:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifieer numeriek\n",
    "h = 1e-5\n",
    "\n",
    "# Numerieke ∂y/∂w\n",
    "dy_dw_num = (neuron(x, w + h, b) - neuron(x, w, b)) / h\n",
    "\n",
    "# Numerieke ∂y/∂b\n",
    "dy_db_num = (neuron(x, w, b + h) - neuron(x, w, b)) / h\n",
    "\n",
    "print(\"Verificatie met numerieke afgeleiden:\")\n",
    "print(f\"  ∂y/∂w analytisch: {dy_dw:.6f}\")\n",
    "print(f\"  ∂y/∂w numeriek:   {dy_dw_num:.6f}\")\n",
    "print()\n",
    "print(f\"  ∂y/∂b analytisch: {dy_db:.6f}\")\n",
    "print(f\"  ∂y/∂b numeriek:   {dy_db_num:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer hoe de output verandert met w en b\n",
    "x = 2.0  # Vaste input\n",
    "\n",
    "w_range = np.linspace(-2, 2, 50)\n",
    "b_range = np.linspace(-2, 2, 50)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "\n",
    "# Bereken output voor alle combinaties\n",
    "Y = sigmoid(W * x + B)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(W, B, Y, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('w')\n",
    "ax1.set_ylabel('b')\n",
    "ax1.set_zlabel('y')\n",
    "ax1.set_title(f'Output y = σ(wx + b) met x = {x}')\n",
    "\n",
    "# Contour plot met gradiënten\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(W, B, Y, levels=15, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Bereken gradiënten op grove grid\n",
    "w_arr = np.linspace(-1.5, 1.5, 6)\n",
    "b_arr = np.linspace(-1.5, 1.5, 6)\n",
    "W_arr, B_arr = np.meshgrid(w_arr, b_arr)\n",
    "\n",
    "Z = W_arr * x + B_arr\n",
    "Y_arr = sigmoid(Z)\n",
    "dY_dZ = Y_arr * (1 - Y_arr)\n",
    "dY_dW = dY_dZ * x\n",
    "dY_dB = dY_dZ * 1\n",
    "\n",
    "ax2.quiver(W_arr, B_arr, dY_dW, dY_dB, color='red', alpha=0.7)\n",
    "ax2.set_xlabel('w')\n",
    "ax2.set_ylabel('b')\n",
    "ax2.set_title('Contour met gradiënt pijlen')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Samenvatting en Vooruitblik\n",
    "\n",
    "### Kernconcepten\n",
    "\n",
    "In deze les hebben we geleerd dat afgeleiden meten hoe snel een functie verandert. We kunnen ze numeriek benaderen of analytisch berekenen met regels zoals de machtregel en de kettingregel.\n",
    "\n",
    "De kettingregel is cruciaal voor samengestelde functies: (f ∘ g)'(x) = f'(g(x)) · g'(x). Dit is precies wat we nodig hebben voor neurale netwerken, die uit meerdere samengestelde lagen bestaan.\n",
    "\n",
    "Partiële afgeleiden meten verandering in functies met meerdere variabelen. De gradiënt is de vector van alle partiële afgeleiden en wijst in de richting van steilste stijging.\n",
    "\n",
    "### Link naar neurale netwerken\n",
    "\n",
    "We hebben gezien hoe we de afgeleiden van een enkel neuron berekenen: hoe de output verandert als we de weight of bias aanpassen. In een echt netwerk moeten we dit voor duizenden parameters doen, en de kettingregel door meerdere lagen toepassen.\n",
    "\n",
    "### Volgende les\n",
    "\n",
    "In les 6 leren we gradient descent: hoe we de gradiënt gebruiken om parameters te optimaliseren. We gaan de loss minimaliseren door steeds kleine stapjes in de richting van de negatieve gradiënt te nemen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist\n",
    "\n",
    "Controleer of je het volgende begrijpt:\n",
    "\n",
    "1. Wat meet een afgeleide?\n",
    "\n",
    "2. Hoe bereken je de afgeleide van xⁿ?\n",
    "\n",
    "3. Wat zegt de kettingregel?\n",
    "\n",
    "4. Wat is een partiële afgeleide?\n",
    "\n",
    "5. Wat is de gradiënt en welke richting wijst deze?\n",
    "\n",
    "Als je deze vragen kunt beantwoorden, ben je klaar voor les 6!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 5 van 12 | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
