{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 7: Backpropagation\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Recap en Motivatie\n",
    "\n",
    "In de vorige lessen hebben we alle puzzelstukjes verzameld:\n",
    "\n",
    "- **Les 2-4**: Lineaire algebra - hoe data door een netwerk stroomt (forward pass)\n",
    "- **Les 5**: Afgeleiden en de kettingregel - hoe we verandering meten\n",
    "- **Les 6**: Gradient descent - hoe we parameters optimaliseren\n",
    "\n",
    "Nu brengen we alles samen. We weten dat we de gradiënt van de loss naar elke parameter nodig hebben voor gradient descent. Maar in een netwerk met meerdere lagen, hoe bereken je de gradiënt naar een weight in de eerste laag?\n",
    "\n",
    "Het antwoord is **backpropagation**: een algoritme dat de kettingregel efficiënt door het hele netwerk toepast. Het is de sleutel tot het trainen van diepe neurale netwerken.\n",
    "\n",
    "Aan het einde van deze les zul je een neuraal netwerk from scratch bouwen en trainen op MNIST!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Leerdoelen\n",
    "\n",
    "Na deze les begrijp je waarom backpropagation nodig is. Je kunt een neuraal netwerk voorstellen als een computational graph. Je begrijpt hoe de kettingregel door het netwerk wordt toegepast. Je kunt backpropagation met de hand uitvoeren voor kleine netwerken. Je kunt een volledig trainingsloop implementeren. Je kunt een neuraal netwerk trainen op MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 De Computational Graph\n",
    "\n",
    "### Het netwerk als een graaf\n",
    "\n",
    "We kunnen een neuraal netwerk voorstellen als een gerichte graaf van operaties:\n",
    "\n",
    "- **Nodes**: Operaties (matrixvermenigvuldiging, optelling, activatiefuncties, loss)\n",
    "- **Edges**: Data die tussen operaties stroomt\n",
    "\n",
    "De forward pass berekent de output door van input naar output te gaan. De backward pass berekent gradiënten door van output naar input te gaan.\n",
    "\n",
    "### Voorbeeld\n",
    "\n",
    "Beschouw een simpel netwerk: y = σ(Wx + b)\n",
    "\n",
    "De computational graph is:\n",
    "```\n",
    "x → [×W] → z₁ → [+b] → z₂ → [σ] → y → [Loss] → L\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorbeeld: forward pass door een simpel netwerk\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Parameters\n",
    "W = np.array([[0.5, -0.3],\n",
    "              [0.2, 0.8]])\n",
    "b = np.array([0.1, -0.2])\n",
    "\n",
    "# Input\n",
    "x = np.array([1.0, 2.0])\n",
    "\n",
    "# Forward pass - stap voor stap\n",
    "z1 = W @ x       # Matrixvermenigvuldiging\n",
    "z2 = z1 + b      # Bias optellen\n",
    "y = sigmoid(z2)  # Activatie\n",
    "\n",
    "print(\"Forward Pass:\")\n",
    "print(f\"Input x = {x}\")\n",
    "print(f\"z₁ = W @ x = {z1}\")\n",
    "print(f\"z₂ = z₁ + b = {z2}\")\n",
    "print(f\"y = σ(z₂) = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Backpropagation: Het Idee\n",
    "\n",
    "### Het probleem\n",
    "\n",
    "We willen ∂L/∂W berekenen: hoe verandert de loss als we W veranderen? Het probleem is dat W niet direct verbonden is met L - er zitten meerdere operaties tussen.\n",
    "\n",
    "### De oplossing: kettingregel\n",
    "\n",
    "We passen de kettingregel toe, maar dan \"achteruit\" door het netwerk:\n",
    "\n",
    "∂L/∂W = ∂L/∂y · ∂y/∂z₂ · ∂z₂/∂z₁ · ∂z₁/∂W\n",
    "\n",
    "### Het algoritme\n",
    "\n",
    "1. **Forward pass**: Bereken alle tussenresultaten en sla ze op (\"cache\")\n",
    "2. **Backward pass**: Start bij ∂L/∂L = 1, werk achteruit door elke operatie\n",
    "3. Bij elke operatie: vermenigvuldig de \"upstream\" gradiënt met de \"lokale\" gradiënt\n",
    "\n",
    "upstream_gradient × local_gradient = downstream_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop voor ons simpele voorbeeld\n",
    "# Stel de \"target\" is [0.8, 0.2] en we gebruiken MSE loss\n",
    "\n",
    "y_target = np.array([0.8, 0.2])\n",
    "\n",
    "# Loss = MSE = mean((y - y_target)²)\n",
    "loss = np.mean((y - y_target) ** 2)\n",
    "print(f\"Loss = {loss:.6f}\")\n",
    "print()\n",
    "\n",
    "# Backward pass\n",
    "print(\"Backward Pass:\")\n",
    "\n",
    "# ∂L/∂y = 2(y - y_target) / n\n",
    "dL_dy = 2 * (y - y_target) / len(y)\n",
    "print(f\"∂L/∂y = {dL_dy}\")\n",
    "\n",
    "# ∂L/∂z₂ = ∂L/∂y · ∂y/∂z₂ = ∂L/∂y · σ'(z₂)\n",
    "dy_dz2 = sigmoid_derivative(z2)\n",
    "dL_dz2 = dL_dy * dy_dz2\n",
    "print(f\"∂y/∂z₂ = σ'(z₂) = {dy_dz2}\")\n",
    "print(f\"∂L/∂z₂ = {dL_dz2}\")\n",
    "\n",
    "# ∂L/∂b = ∂L/∂z₂ · ∂z₂/∂b = ∂L/∂z₂ · 1\n",
    "dL_db = dL_dz2\n",
    "print(f\"∂L/∂b = {dL_db}\")\n",
    "\n",
    "# ∂L/∂z₁ = ∂L/∂z₂ · ∂z₂/∂z₁ = ∂L/∂z₂ · 1\n",
    "dL_dz1 = dL_dz2\n",
    "print(f\"∂L/∂z₁ = {dL_dz1}\")\n",
    "\n",
    "# ∂L/∂W = ∂L/∂z₁ · ∂z₁/∂W = outer(∂L/∂z₁, x)\n",
    "dL_dW = np.outer(dL_dz1, x)\n",
    "print(f\"∂L/∂W = \\n{dL_dW}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Lokale Gradiënten van Operaties\n",
    "\n",
    "Elke operatie moet weten hoe hij zijn lokale gradiënt berekent. Dit is de afgeleide van de output naar de inputs.\n",
    "\n",
    "### Basis operaties\n",
    "\n",
    "**Optelling: z = x + y**\n",
    "- ∂z/∂x = 1\n",
    "- ∂z/∂y = 1\n",
    "\n",
    "**Vermenigvuldiging: z = x · y**\n",
    "- ∂z/∂x = y\n",
    "- ∂z/∂y = x\n",
    "\n",
    "**ReLU: z = max(0, x)**\n",
    "- ∂z/∂x = 1 als x > 0, anders 0\n",
    "\n",
    "**Sigmoid: z = σ(x)**\n",
    "- ∂z/∂x = σ(x)(1 - σ(x))\n",
    "\n",
    "**Matrix vermenigvuldiging: Z = X @ W**\n",
    "- ∂L/∂X = ∂L/∂Z @ Wᵀ\n",
    "- ∂L/∂W = Xᵀ @ ∂L/∂Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementeer operaties met forward en backward\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"ReLU activatie met forward en backward.\"\"\"\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * (self.x > 0)\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid activatie met forward en backward.\"\"\"\n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.out * (1 - self.out)\n",
    "\n",
    "# Test ReLU\n",
    "relu = ReLU()\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "y = relu.forward(x)\n",
    "print(\"ReLU Forward:\")\n",
    "print(f\"  x = {x}\")\n",
    "print(f\"  y = {y}\")\n",
    "\n",
    "# Backward: stel upstream gradient is allemaal 1\n",
    "dout = np.ones_like(y)\n",
    "dx = relu.backward(dout)\n",
    "print(f\"ReLU Backward:\")\n",
    "print(f\"  dout = {dout}\")\n",
    "print(f\"  dx = {dx}\")\n",
    "print()\n",
    "print(\"Merk op: gradiënt is 0 waar x ≤ 0 (geen leren mogelijk daar)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Backprop Door een Mini-Netwerk\n",
    "\n",
    "Laten we een volledig mini-netwerk uitwerken met concrete getallen. Dit helpt om de intuïtie te bouwen.\n",
    "\n",
    "Netwerk: input → Linear → ReLU → Linear → MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-netwerk: 2 inputs → 3 hidden → 1 output\n",
    "\n",
    "# Parameters\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(3, 2) * 0.5  # 3x2: 2 inputs, 3 hidden\n",
    "b1 = np.zeros(3)\n",
    "W2 = np.random.randn(1, 3) * 0.5  # 1x3: 3 hidden, 1 output\n",
    "b2 = np.zeros(1)\n",
    "\n",
    "print(\"Netwerk structuur: 2 → 3 → 1\")\n",
    "print(f\"W1 shape: {W1.shape}, b1 shape: {b1.shape}\")\n",
    "print(f\"W2 shape: {W2.shape}, b2 shape: {b2.shape}\")\n",
    "print()\n",
    "print(\"W1 =\")\n",
    "print(W1)\n",
    "print(f\"\\nW2 = {W2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = np.array([1.0, 2.0])  # Input\n",
    "y_target = np.array([1.0])  # Target\n",
    "\n",
    "print(\"=== FORWARD PASS ===\")\n",
    "print(f\"Input: x = {x}\")\n",
    "print(f\"Target: y_target = {y_target}\")\n",
    "print()\n",
    "\n",
    "# Laag 1\n",
    "z1 = W1 @ x + b1\n",
    "a1 = np.maximum(0, z1)  # ReLU\n",
    "print(f\"z1 = W1 @ x + b1 = {z1}\")\n",
    "print(f\"a1 = ReLU(z1) = {a1}\")\n",
    "print()\n",
    "\n",
    "# Laag 2\n",
    "z2 = W2 @ a1 + b2\n",
    "y = z2  # Geen activatie op output (regressie)\n",
    "print(f\"z2 = W2 @ a1 + b2 = {z2}\")\n",
    "print(f\"y (output) = {y}\")\n",
    "print()\n",
    "\n",
    "# Loss\n",
    "loss = 0.5 * np.sum((y - y_target) ** 2)  # 0.5 voor makkelijkere afgeleide\n",
    "print(f\"Loss = 0.5 * (y - y_target)² = {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "print(\"=== BACKWARD PASS ===\")\n",
    "print()\n",
    "\n",
    "# Start bij de loss\n",
    "# ∂L/∂y = y - y_target\n",
    "dL_dy = y - y_target\n",
    "print(f\"∂L/∂y = y - y_target = {dL_dy}\")\n",
    "print()\n",
    "\n",
    "# Laag 2 backward\n",
    "# z2 = W2 @ a1 + b2, y = z2\n",
    "# ∂L/∂z2 = ∂L/∂y (want y = z2)\n",
    "dL_dz2 = dL_dy\n",
    "print(f\"∂L/∂z2 = {dL_dz2}\")\n",
    "\n",
    "# ∂L/∂W2 = ∂L/∂z2 ⊗ a1 (outer product)\n",
    "dL_dW2 = np.outer(dL_dz2, a1)\n",
    "print(f\"∂L/∂W2 = {dL_dW2}\")\n",
    "\n",
    "# ∂L/∂b2 = ∂L/∂z2\n",
    "dL_db2 = dL_dz2\n",
    "print(f\"∂L/∂b2 = {dL_db2}\")\n",
    "\n",
    "# ∂L/∂a1 = W2ᵀ @ ∂L/∂z2\n",
    "dL_da1 = W2.T @ dL_dz2\n",
    "print(f\"∂L/∂a1 = {dL_da1}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laag 1 backward\n",
    "# a1 = ReLU(z1)\n",
    "# ∂L/∂z1 = ∂L/∂a1 * ReLU'(z1)\n",
    "relu_grad = (z1 > 0).astype(float)  # 1 waar z1 > 0, anders 0\n",
    "dL_dz1 = dL_da1 * relu_grad\n",
    "print(f\"ReLU'(z1) = {relu_grad}\")\n",
    "print(f\"∂L/∂z1 = {dL_dz1}\")\n",
    "\n",
    "# ∂L/∂W1 = ∂L/∂z1 ⊗ x\n",
    "dL_dW1 = np.outer(dL_dz1, x)\n",
    "print(f\"∂L/∂W1 = \\n{dL_dW1}\")\n",
    "\n",
    "# ∂L/∂b1 = ∂L/∂z1\n",
    "dL_db1 = dL_dz1\n",
    "print(f\"∂L/∂b1 = {dL_db1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerieke verificatie\n",
    "def forward_network(x, W1, b1, W2, b2):\n",
    "    z1 = W1 @ x + b1\n",
    "    a1 = np.maximum(0, z1)\n",
    "    z2 = W2 @ a1 + b2\n",
    "    return z2\n",
    "\n",
    "def compute_loss(x, y_target, W1, b1, W2, b2):\n",
    "    y = forward_network(x, W1, b1, W2, b2)\n",
    "    return 0.5 * np.sum((y - y_target) ** 2)\n",
    "\n",
    "# Numerieke gradiënt voor W1[0,0]\n",
    "h = 1e-5\n",
    "W1_plus = W1.copy()\n",
    "W1_plus[0, 0] += h\n",
    "W1_minus = W1.copy()\n",
    "W1_minus[0, 0] -= h\n",
    "\n",
    "numerical_grad = (compute_loss(x, y_target, W1_plus, b1, W2, b2) - \n",
    "                  compute_loss(x, y_target, W1_minus, b1, W2, b2)) / (2 * h)\n",
    "\n",
    "print(\"Verificatie:\")\n",
    "print(f\"  ∂L/∂W1[0,0] analytisch: {dL_dW1[0,0]:.6f}\")\n",
    "print(f\"  ∂L/∂W1[0,0] numeriek:   {numerical_grad:.6f}\")\n",
    "print(f\"  Match: {np.isclose(dL_dW1[0,0], numerical_grad, rtol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Vectorized Backprop\n",
    "\n",
    "In de praktijk werken we met batches van data. De wiskunde blijft hetzelfde, maar we moeten rekening houden met matrix dimensies.\n",
    "\n",
    "Voor een batch van N samples:\n",
    "- X heeft shape (N, input_dim)\n",
    "- De gradiënten worden gesommeerd/gemiddeld over de batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volledige Layer class met forward en backward\n",
    "\n",
    "class LinearLayer:\n",
    "    \"\"\"Lineaire laag: z = X @ W + b\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Xavier initialisatie\n",
    "        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b = np.zeros(output_dim)\n",
    "        \n",
    "        # Gradiënten\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X  # Cache voor backward\n",
    "        return X @ self.W + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # dout: gradiënt van loss naar output, shape (N, output_dim)\n",
    "        N = self.X.shape[0]\n",
    "        \n",
    "        # Gradiënt naar parameters\n",
    "        self.dW = self.X.T @ dout / N\n",
    "        self.db = np.mean(dout, axis=0)\n",
    "        \n",
    "        # Gradiënt naar input (voor volgende laag)\n",
    "        dX = dout @ self.W.T\n",
    "        return dX\n",
    "\n",
    "class ReLULayer:\n",
    "    \"\"\"ReLU activatie.\"\"\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.mask = (X > 0)\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "# Test\n",
    "layer = LinearLayer(2, 3)\n",
    "relu = ReLULayer()\n",
    "\n",
    "X = np.array([[1.0, 2.0],\n",
    "              [3.0, 4.0]])  # 2 samples\n",
    "\n",
    "# Forward\n",
    "z = layer.forward(X)\n",
    "a = relu.forward(z)\n",
    "print(f\"Input X shape: {X.shape}\")\n",
    "print(f\"Output a shape: {a.shape}\")\n",
    "print(f\"Output:\\n{a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 De Volledige Training Loop\n",
    "\n",
    "Nu bouwen we een volledig neuraal netwerk dat we kunnen trainen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Een simpel neuraal netwerk met 1 hidden layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.layer1 = LinearLayer(input_dim, hidden_dim)\n",
    "        self.relu = ReLULayer()\n",
    "        self.layer2 = LinearLayer(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        self.z1 = self.layer1.forward(X)\n",
    "        self.a1 = self.relu.forward(self.z1)\n",
    "        self.z2 = self.layer2.forward(self.a1)\n",
    "        return self.z2\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"Backward pass.\"\"\"\n",
    "        dz2 = dout\n",
    "        da1 = self.layer2.backward(dz2)\n",
    "        dz1 = self.relu.backward(da1)\n",
    "        dX = self.layer1.backward(dz1)\n",
    "        return dX\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"Update parameters met gradient descent.\"\"\"\n",
    "        self.layer1.W -= learning_rate * self.layer1.dW\n",
    "        self.layer1.b -= learning_rate * self.layer1.db\n",
    "        self.layer2.W -= learning_rate * self.layer2.dW\n",
    "        self.layer2.b -= learning_rate * self.layer2.db\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"Mean Squared Error loss.\"\"\"\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def mse_loss_gradient(y_pred, y_true):\n",
    "    \"\"\"Gradiënt van MSE loss.\"\"\"\n",
    "    return 2 * (y_pred - y_true) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test op een eenvoudig regressieprobleem\n",
    "np.random.seed(42)\n",
    "\n",
    "# Genereer data: y = sin(x)\n",
    "X_train = np.random.uniform(-3, 3, (200, 1))\n",
    "y_train = np.sin(X_train)\n",
    "\n",
    "# Netwerk\n",
    "nn = NeuralNetwork(input_dim=1, hidden_dim=32, output_dim=1)\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.1\n",
    "n_epochs = 500\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    y_pred = nn.forward(X_train)\n",
    "    \n",
    "    # Loss\n",
    "    loss = mse_loss(y_pred, y_train)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward\n",
    "    dout = mse_loss_gradient(y_pred, y_train)\n",
    "    nn.backward(dout)\n",
    "    \n",
    "    # Update\n",
    "    nn.update(learning_rate)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss = {loss:.6f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer de fit\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Fit\n",
    "X_test = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y_test = nn.forward(X_test)\n",
    "\n",
    "axes[0].scatter(X_train, y_train, alpha=0.3, label='Training data')\n",
    "axes[0].plot(X_test, y_test, 'r-', linewidth=2, label='NN voorspelling')\n",
    "axes[0].plot(X_test, np.sin(X_test), 'g--', linewidth=2, label='sin(x)')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Neuraal netwerk leert sin(x)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss curve\n",
    "axes[1].plot(losses)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].set_title('Training Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Toepassing: MNIST Classificatie\n",
    "\n",
    "Nu het grote moment: we trainen een neuraal netwerk op MNIST, de klassieke dataset van handgeschreven cijfers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad MNIST data\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print(\"MNIST laden...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Normaliseer naar [0, 1]\n",
    "X = X / 255.0\n",
    "\n",
    "# Split in train/test\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Input dimensie: {X_train.shape[1]}\")\n",
    "print(f\"Aantal klassen: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bekijk wat voorbeelden\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i]}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST voorbeelden', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax en Cross-Entropy voor classificatie\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax functie (numeriek stabiel).\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"Cross-entropy loss voor classificatie.\"\"\"\n",
    "    N = y_pred.shape[0]\n",
    "    # One-hot encoding\n",
    "    y_one_hot = np.zeros_like(y_pred)\n",
    "    y_one_hot[np.arange(N), y_true] = 1\n",
    "    # Cross-entropy\n",
    "    loss = -np.sum(y_one_hot * np.log(y_pred + 1e-10)) / N\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_gradient(y_pred, y_true):\n",
    "    \"\"\"Gradiënt van softmax + cross-entropy.\"\"\"\n",
    "    N = y_pred.shape[0]\n",
    "    grad = y_pred.copy()\n",
    "    grad[np.arange(N), y_true] -= 1\n",
    "    return grad / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST classifier\n",
    "\n",
    "class MNISTClassifier:\n",
    "    \"\"\"Neuraal netwerk voor MNIST classificatie.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=784, hidden_dim=128, output_dim=10):\n",
    "        self.layer1 = LinearLayer(input_dim, hidden_dim)\n",
    "        self.relu = ReLULayer()\n",
    "        self.layer2 = LinearLayer(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = self.layer1.forward(X)\n",
    "        self.a1 = self.relu.forward(self.z1)\n",
    "        self.z2 = self.layer2.forward(self.a1)\n",
    "        self.probs = softmax(self.z2)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        dout = cross_entropy_gradient(self.probs, y_true)\n",
    "        da1 = self.layer2.backward(dout)\n",
    "        dz1 = self.relu.backward(da1)\n",
    "        self.layer1.backward(dz1)\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        self.layer1.W -= learning_rate * self.layer1.dW\n",
    "        self.layer1.b -= learning_rate * self.layer1.db\n",
    "        self.layer2.W -= learning_rate * self.layer2.dW\n",
    "        self.layer2.b -= learning_rate * self.layer2.db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train het netwerk!\n",
    "np.random.seed(42)\n",
    "\n",
    "model = MNISTClassifier(hidden_dim=128)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.5\n",
    "batch_size = 128\n",
    "n_epochs = 10\n",
    "\n",
    "# Training geschiedenis\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "print(\"Training starten...\")\n",
    "print(f\"Batch size: {batch_size}, Batches per epoch: {n_batches}\")\n",
    "print()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_shuffled = X_train[indices]\n",
    "    y_shuffled = y_train[indices]\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "        \n",
    "        # Forward\n",
    "        probs = model.forward(X_batch)\n",
    "        loss = cross_entropy_loss(probs, y_batch)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Backward\n",
    "        model.backward(y_batch)\n",
    "        \n",
    "        # Update\n",
    "        model.update(learning_rate)\n",
    "    \n",
    "    # Evalueer\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    train_acc = model.accuracy(X_train[:5000], y_train[:5000])  # Subset voor snelheid\n",
    "    test_acc = model.accuracy(X_test, y_test)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}: Loss = {avg_loss:.4f}, Train acc = {train_acc:.4f}, Test acc = {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nFinale test accuracy: {test_accs[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(train_losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(train_accs, 'b-', linewidth=2, label='Train')\n",
    "axes[1].plot(test_accs, 'r-', linewidth=2, label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bekijk voorspellingen\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "\n",
    "# Random test samples\n",
    "indices = np.random.choice(len(X_test), 10, replace=False)\n",
    "\n",
    "for i, (ax, idx) in enumerate(zip(axes.flatten(), indices)):\n",
    "    img = X_test[idx].reshape(28, 28)\n",
    "    true_label = y_test[idx]\n",
    "    pred_label = model.predict(X_test[idx:idx+1])[0]\n",
    "    \n",
    "    ax.imshow(img, cmap='gray')\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    ax.set_title(f'Pred: {pred_label}, True: {true_label}', color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Model voorspellingen (groen=correct, rood=fout)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9 Samenvatting\n",
    "\n",
    "### Wat hebben we bereikt?\n",
    "\n",
    "We hebben een neuraal netwerk **from scratch** gebouwd en getraind! Geen TensorFlow, geen PyTorch - alleen NumPy en ons begrip van de wiskunde.\n",
    "\n",
    "### Kernconcepten\n",
    "\n",
    "**Backpropagation** is de efficiënte toepassing van de kettingregel door een computational graph. De forward pass berekent de output en slaat tussenresultaten op. De backward pass berekent gradiënten van de loss naar alle parameters.\n",
    "\n",
    "Elke operatie heeft een **lokale gradiënt** die bepaalt hoe de output verandert met de input. De **kettingregel** combineert deze lokale gradiënten tot de totale gradiënt.\n",
    "\n",
    "### De complete training loop\n",
    "\n",
    "1. **Forward pass**: bereken output en loss\n",
    "2. **Backward pass**: bereken gradiënten\n",
    "3. **Update**: pas parameters aan met gradient descent\n",
    "4. Herhaal voor elke batch, voor meerdere epochs\n",
    "\n",
    "### Einde Deel 2: Calculus\n",
    "\n",
    "We hebben nu alle tools om neurale netwerken te begrijpen en te trainen:\n",
    "- **Lineaire algebra** (Deel 1): data flow door het netwerk\n",
    "- **Calculus** (Deel 2): leren via gradient descent en backpropagation\n",
    "\n",
    "In Deel 3 (Statistiek en Kansrekening) zullen we dieper ingaan op loss functies, output interpretaties en model evaluatie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 7 van 12 | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
