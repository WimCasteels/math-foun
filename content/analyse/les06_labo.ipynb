{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 6: Labo - Oefeningen\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---\n",
    "\n",
    "In dit labo implementeer je gradient descent en pas je het toe op verschillende optimalisatieproblemen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 1: Loss Functies Implementeren\n",
    "\n",
    "*Geschatte tijd: 15 minuten*\n",
    "\n",
    "### Opdracht 1a\n",
    "\n",
    "Implementeer de Mean Squared Error (MSE) loss functie:\n",
    "\n",
    "MSE = (1/n) Σᵢ (yᵢ_pred - yᵢ_true)²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"Bereken Mean Squared Error.\"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "y_true = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_pred = np.array([1.1, 2.2, 2.9, 4.0, 5.2])\n",
    "\n",
    "# Verwacht: ((0.1)² + (0.2)² + (0.1)² + 0 + (0.2)²) / 5 = 0.02\n",
    "print(f\"MSE: {mse_loss(y_pred, y_true)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 1b\n",
    "\n",
    "Implementeer de gradiënt van de MSE loss ten opzichte van de voorspellingen:\n",
    "\n",
    "∂MSE/∂y_pred = (2/n) · (y_pred - y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_gradient(y_pred, y_true):\n",
    "    \"\"\"Bereken de gradiënt van MSE naar y_pred.\"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "grad = mse_gradient(y_pred, y_true)\n",
    "print(f\"Gradiënt: {grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 1c\n",
    "\n",
    "Implementeer Binary Cross-Entropy loss:\n",
    "\n",
    "BCE = -(1/n) Σᵢ [yᵢ·log(pᵢ) + (1-yᵢ)·log(1-pᵢ)]\n",
    "\n",
    "Gebruik `np.clip()` om te voorkomen dat je log(0) berekent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_pred, y_true, epsilon=1e-15):\n",
    "    \"\"\"Bereken Binary Cross-Entropy Loss.\"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "y_true_class = np.array([1, 0, 1, 1, 0])\n",
    "y_pred_probs = np.array([0.9, 0.1, 0.8, 0.7, 0.3])\n",
    "\n",
    "print(f\"BCE: {binary_cross_entropy(y_pred_probs, y_true_class)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 2: 1D Gradient Descent\n",
    "\n",
    "*Geschatte tijd: 20 minuten*\n",
    "\n",
    "### Opdracht 2a\n",
    "\n",
    "Implementeer gradient descent voor een 1D functie. Het algoritme is:\n",
    "\n",
    "```\n",
    "x = x_init\n",
    "for i in range(n_iterations):\n",
    "    gradient = df_dx(x)\n",
    "    x = x - learning_rate * gradient\n",
    "```\n",
    "\n",
    "Test op f(x) = x² - 4x + 5 (minimum bij x = 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_1d(f, df_dx, x_init, learning_rate, n_iterations):\n",
    "    \"\"\"Voer gradient descent uit voor een 1D functie.\n",
    "    \n",
    "    Returns:\n",
    "        x_final: de gevonden x waarde\n",
    "        history: lijst van alle x waarden tijdens optimalisatie\n",
    "    \"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Functie en afgeleide\n",
    "def f(x):\n",
    "    return x**2 - 4*x + 5\n",
    "\n",
    "def df_dx(x):\n",
    "    return 2*x - 4\n",
    "\n",
    "# Test\n",
    "x_final, history = gradient_descent_1d(f, df_dx, x_init=6.0, learning_rate=0.1, n_iterations=50)\n",
    "print(f\"Gevonden minimum: x = {x_final:.4f}\")\n",
    "print(f\"Verwacht minimum: x = 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 2b\n",
    "\n",
    "Visualiseer het optimalisatiepad: plot de functie en teken het pad dat gradient descent volgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code voor opdracht 2b:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 2c\n",
    "\n",
    "Experimenteer met verschillende learning rates: 0.01, 0.1, 0.5, 1.0, 1.5. Plot de convergentiecurves (loss vs iteratie) voor elke learning rate. Wat observeer je?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code voor opdracht 2c:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 3: 2D Gradient Descent\n",
    "\n",
    "*Geschatte tijd: 20 minuten*\n",
    "\n",
    "### Opdracht 3a\n",
    "\n",
    "Breid gradient descent uit naar 2D. Minimaliseer f(x, y) = (x - 2)² + (y - 3)² (minimum bij (2, 3))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_2d(f, grad_f, xy_init, learning_rate, n_iterations):\n",
    "    \"\"\"Voer gradient descent uit voor een 2D functie.\n",
    "    \n",
    "    Args:\n",
    "        f: functie f(x, y)\n",
    "        grad_f: gradiënt functie die [∂f/∂x, ∂f/∂y] teruggeeft\n",
    "        xy_init: startpunt [x, y]\n",
    "        learning_rate: stapgrootte\n",
    "        n_iterations: aantal iteraties\n",
    "    \n",
    "    Returns:\n",
    "        xy_final: gevonden minimum [x, y]\n",
    "        history: lijst van alle [x, y] waarden\n",
    "    \"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Functie en gradiënt\n",
    "def f_2d(x, y):\n",
    "    return (x - 2)**2 + (y - 3)**2\n",
    "\n",
    "def grad_f_2d(x, y):\n",
    "    return np.array([2*(x - 2), 2*(y - 3)])\n",
    "\n",
    "# Test\n",
    "xy_final, history = gradient_descent_2d(f_2d, grad_f_2d, xy_init=[-1, 6], \n",
    "                                         learning_rate=0.1, n_iterations=50)\n",
    "print(f\"Gevonden minimum: ({xy_final[0]:.4f}, {xy_final[1]:.4f})\")\n",
    "print(f\"Verwacht minimum: (2, 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 3b\n",
    "\n",
    "Visualiseer het pad op een contourplot van de functie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code voor opdracht 3b:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 4: Gradient Descent voor Lineaire Regressie\n",
    "\n",
    "*Geschatte tijd: 25 minuten*\n",
    "\n",
    "### Opdracht 4a\n",
    "\n",
    "Genereer synthetische data voor lineaire regressie: y = 2.5x + 3 + ruis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "# Genereer data\n",
    "X = np.random.uniform(0, 10, n_samples)\n",
    "y = 2.5 * X + 3 + np.random.randn(n_samples) * 1.5\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, y, alpha=0.7)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetische data: y = 2.5x + 3 + ruis')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 4b\n",
    "\n",
    "Implementeer lineaire regressie met gradient descent. Het model is y = wx + b.\n",
    "\n",
    "De gradiënten zijn:\n",
    "- ∂MSE/∂w = (2/n) Σᵢ (yᵢ_pred - yᵢ_true) · xᵢ\n",
    "- ∂MSE/∂b = (2/n) Σᵢ (yᵢ_pred - yᵢ_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "        self.w = 0.0\n",
    "        self.b = 0.0\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Voorspel y = w*X + b.\"\"\"\n",
    "        # Jouw code hier\n",
    "        pass\n",
    "    \n",
    "    def compute_loss(self, X, y):\n",
    "        \"\"\"Bereken MSE loss.\"\"\"\n",
    "        # Jouw code hier\n",
    "        pass\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        \"\"\"Bereken gradiënten ∂L/∂w en ∂L/∂b.\"\"\"\n",
    "        # Jouw code hier\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y, n_epochs=100):\n",
    "        \"\"\"Train het model met gradient descent.\"\"\"\n",
    "        # Jouw code hier\n",
    "        pass\n",
    "\n",
    "# Train\n",
    "model = LinearRegressionGD(learning_rate=0.01)\n",
    "model.fit(X, y, n_epochs=200)\n",
    "\n",
    "print(f\"Geleerd: w = {model.w:.4f}, b = {model.b:.4f}\")\n",
    "print(f\"Werkelijk: w = 2.5, b = 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 4c\n",
    "\n",
    "Visualiseer:\n",
    "1. De data met de geleerde lijn\n",
    "2. De loss curve tijdens training\n",
    "3. De evolutie van w en b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code voor opdracht 4c:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 5: Batch vs Mini-batch vs Stochastic GD\n",
    "\n",
    "*Geschatte tijd: 25 minuten*\n",
    "\n",
    "### Opdracht 5a\n",
    "\n",
    "Implementeer drie varianten van gradient descent:\n",
    "1. **Batch GD**: gebruik alle data voor elke update\n",
    "2. **Mini-batch GD**: gebruik een batch van b samples\n",
    "3. **Stochastic GD**: gebruik 1 sample per update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, learning_rate, n_epochs):\n",
    "    \"\"\"Batch gradient descent.\"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "def minibatch_gradient_descent(X, y, learning_rate, n_epochs, batch_size=16):\n",
    "    \"\"\"Mini-batch gradient descent.\"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "def stochastic_gradient_descent(X, y, learning_rate, n_epochs):\n",
    "    \"\"\"Stochastic gradient descent (batch_size=1).\"\"\"\n",
    "    # Jouw code hier\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 5b\n",
    "\n",
    "Vergelijk de drie methodes op de lineaire regressie data. Plot de loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code voor opdracht 5b:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 5c\n",
    "\n",
    "Wat observeer je? Beschrijf de verschillen in:\n",
    "- Stabiliteit van de convergentie\n",
    "- Snelheid van convergentie\n",
    "- Eindresultaat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Jouw antwoord:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 6: Learning Rate Scheduling\n",
    "\n",
    "*Geschatte tijd: 15 minuten*\n",
    "\n",
    "### Opdracht 6a\n",
    "\n",
    "Implementeer een afnemende learning rate: lr(t) = lr₀ / (1 + decay·t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_decay(X, y, lr_init, decay, n_epochs):\n",
    "    \"\"\"Gradient descent met afnemende learning rate.\"\"\"\n",
    "    # Jouw code hier\n",
    "    pass\n",
    "\n",
    "# Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 6b\n",
    "\n",
    "Vergelijk constante learning rate met afnemende learning rate. Wat is het verschil?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code voor opdracht 6b:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 7: Logistische Regressie\n",
    "\n",
    "*Geschatte tijd: 25 minuten*\n",
    "\n",
    "### Opdracht 7a\n",
    "\n",
    "Genereer data voor binaire classificatie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Klasse 0: gecentreerd rond (-1, -1)\n",
    "X0 = np.random.randn(n_samples // 2, 2) * 0.8 + np.array([-1, -1])\n",
    "y0 = np.zeros(n_samples // 2)\n",
    "\n",
    "# Klasse 1: gecentreerd rond (1, 1)\n",
    "X1 = np.random.randn(n_samples // 2, 2) * 0.8 + np.array([1, 1])\n",
    "y1 = np.ones(n_samples // 2)\n",
    "\n",
    "X_class = np.vstack([X0, X1])\n",
    "y_class = np.hstack([y0, y1])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_class[y_class == 0, 0], X_class[y_class == 0, 1], c='blue', label='Klasse 0')\n",
    "plt.scatter(X_class[y_class == 1, 0], X_class[y_class == 1, 1], c='red', label='Klasse 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('Binaire classificatie data')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 7b\n",
    "\n",
    "Implementeer logistische regressie met gradient descent.\n",
    "\n",
    "Model: p = σ(w₁x₁ + w₂x₂ + b)\n",
    "\n",
    "Loss: Binary Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "class LogisticRegressionGD:\n",
    "    def __init__(self, learning_rate=0.1):\n",
    "        self.lr = learning_rate\n",
    "        self.w = None\n",
    "        self.b = 0.0\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Voorspel kansen.\"\"\"\n",
    "        # Jouw code hier\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Voorspel klassen (0 of 1).\"\"\"\n",
    "        # Jouw code hier\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y, n_epochs=100):\n",
    "        \"\"\"Train met gradient descent.\"\"\"\n",
    "        # Jouw code hier\n",
    "        pass\n",
    "\n",
    "# Train\n",
    "log_model = LogisticRegressionGD(learning_rate=0.5)\n",
    "log_model.fit(X_class, y_class, n_epochs=200)\n",
    "\n",
    "# Evalueer\n",
    "predictions = log_model.predict(X_class)\n",
    "accuracy = np.mean(predictions == y_class)\n",
    "print(f\"Nauwkeurigheid: {accuracy * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 7c\n",
    "\n",
    "Visualiseer de decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code voor opdracht 7c:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 8: Momentum\n",
    "\n",
    "*Geschatte tijd: 20 minuten*\n",
    "\n",
    "### Opdracht 8a\n",
    "\n",
    "Implementeer gradient descent met momentum. De update regel wordt:\n",
    "\n",
    "```\n",
    "v = momentum * v - learning_rate * gradient\n",
    "w = w + v\n",
    "```\n",
    "\n",
    "Momentum (vaak 0.9) helpt om sneller te convergeren en uit lokale minima te ontsnappen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_momentum(f, grad_f, x_init, learning_rate, momentum, n_iterations):\n",
    "    \"\"\"Gradient descent met momentum.\"\"\"\n",
    "    # Jouw code hier\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht 8b\n",
    "\n",
    "Vergelijk standaard GD met GD + momentum op de functie f(x, y) = 0.1x² + 2y² (een \"langgerekte\" vallei). Start vanaf (10, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code voor opdracht 8b:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonusoefening: Rosenbrock Functie\n",
    "\n",
    "*Geschatte tijd: 20 minuten*\n",
    "\n",
    "### Bonus\n",
    "\n",
    "De Rosenbrock functie is een klassieke testfunctie voor optimalisatie:\n",
    "\n",
    "f(x, y) = (a - x)² + b(y - x²)²\n",
    "\n",
    "Met a = 1, b = 100 is het minimum bij (1, 1).\n",
    "\n",
    "Probeer deze functie te minimaliseren met gradient descent. Dit is uitdagend! Experimenteer met verschillende learning rates en momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code voor de bonusoefening:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Klaar!\n",
    "\n",
    "Je hebt de labo-oefeningen van les 6 afgerond. Je kunt nu gradient descent implementeren en toepassen op verschillende problemen.\n",
    "\n",
    "In de volgende les leren we backpropagation: hoe we de gradiënten door een heel neuraal netwerk berekenen.\n",
    "\n",
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 6 Labo | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
