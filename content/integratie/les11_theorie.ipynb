{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 11: Alles Samen - Een Complete Neural Network Library\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.0 Recap: De Reis Tot Nu Toe\n",
    "\n",
    "We hebben een indrukwekkende reis gemaakt door de wiskunde achter deep learning:\n",
    "\n",
    "### Deel 1: Lineaire Algebra\n",
    "- **Vectoren en matrices**: data representatie\n",
    "- **Matrixvermenigvuldiging**: de forward pass\n",
    "- **Lineaire transformaties**: wat lagen doen\n",
    "\n",
    "### Deel 2: Calculus\n",
    "- **Afgeleiden**: hoe verandering te meten\n",
    "- **Gradient descent**: parameters optimaliseren\n",
    "- **Backpropagation**: efficiënt gradiënten berekenen\n",
    "\n",
    "### Deel 3: Statistiek\n",
    "- **Kansverdelingen**: output interpretatie\n",
    "- **Verwachtingswaarde/variantie**: batch norm, initialisatie\n",
    "- **Maximum Likelihood**: waarom onze loss functies werken\n",
    "\n",
    "Nu is het tijd om alles samen te brengen in een **complete, werkende neural network library**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Leerdoelen\n",
    "\n",
    "Na deze les kun je een complete neural network library from scratch bouwen. Je begrijpt hoe alle wiskundige concepten samenwerken. Je kunt een flexibel netwerk ontwerpen met verschillende lagen. Je kunt het netwerk trainen en evalueren op echte data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 De Architectuur\n",
    "\n",
    "Onze library zal bestaan uit:\n",
    "\n",
    "1. **Layer** (abstracte klasse): interface voor alle lagen\n",
    "2. **Linear**: lineaire transformatie (Les 2-4)\n",
    "3. **Activations**: ReLU, Sigmoid, Tanh, Softmax (Les 5)\n",
    "4. **BatchNorm**: normalisatie (Les 9)\n",
    "5. **Loss functions**: MSE, CrossEntropy (Les 10)\n",
    "6. **Optimizers**: SGD, SGD+Momentum (Les 6)\n",
    "7. **Sequential**: container voor het netwerk\n",
    "\n",
    "Dit is vergelijkbaar met hoe PyTorch en Keras werken!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Layer Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    \"\"\"Abstracte base class voor alle lagen.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = {}  # Leerbare parameters\n",
    "        self.grads = {}   # Gradiënten van parameters\n",
    "        self.training = True\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: bereken output gegeven input.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, dout):\n",
    "        \"\"\"Backward pass: bereken gradiënten.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        self.training = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Linear Layer\n",
    "\n",
    "De kern van neurale netwerken: z = Wx + b\n",
    "\n",
    "**Forward**: z = X @ W + b\n",
    "\n",
    "**Backward**:\n",
    "- dW = X.T @ dout\n",
    "- db = sum(dout)\n",
    "- dX = dout @ W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"Lineaire laag: z = Wx + b\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, init='he'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialisatie (Les 9)\n",
    "        if init == 'he':\n",
    "            std = np.sqrt(2.0 / in_features)\n",
    "        elif init == 'xavier':\n",
    "            std = np.sqrt(2.0 / (in_features + out_features))\n",
    "        else:\n",
    "            std = 0.01\n",
    "        \n",
    "        self.params['W'] = np.random.randn(in_features, out_features) * std\n",
    "        self.params['b'] = np.zeros(out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x  # Cache voor backward\n",
    "        return x @ self.params['W'] + self.params['b']\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        n = self.x.shape[0]\n",
    "        \n",
    "        # Gradiënten voor parameters\n",
    "        self.grads['W'] = self.x.T @ dout / n\n",
    "        self.grads['b'] = np.mean(dout, axis=0)\n",
    "        \n",
    "        # Gradiënt voor input (naar vorige laag)\n",
    "        return dout @ self.params['W'].T\n",
    "\n",
    "# Test\n",
    "linear = Linear(3, 2)\n",
    "x = np.random.randn(4, 3)  # 4 samples, 3 features\n",
    "out = linear.forward(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"W shape: {linear.params['W'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Activation Functions\n",
    "\n",
    "Activatiefuncties introduceren non-lineariteit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"ReLU: f(x) = max(0, x)\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x > 0)\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"Sigmoid: f(x) = 1 / (1 + e^-x)\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.out * (1 - self.out)\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "    \"\"\"Tanh: f(x) = tanh(x)\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * (1 - self.out ** 2)\n",
    "\n",
    "\n",
    "class LeakyReLU(Layer):\n",
    "    \"\"\"Leaky ReLU: f(x) = x if x > 0 else alpha * x\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.01):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * np.where(self.x > 0, 1, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer activatiefuncties\n",
    "x = np.linspace(-3, 3, 100)\n",
    "\n",
    "activations = {\n",
    "    'ReLU': ReLU(),\n",
    "    'Sigmoid': Sigmoid(),\n",
    "    'Tanh': Tanh(),\n",
    "    'LeakyReLU': LeakyReLU(0.1)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, (name, act) in zip(axes.flatten(), activations.items()):\n",
    "    y = act.forward(x)\n",
    "    ax.plot(x, y, 'b-', linewidth=2, label='f(x)')\n",
    "    \n",
    "    # Toon ook de afgeleide\n",
    "    dy = act.backward(np.ones_like(x))\n",
    "    ax.plot(x, dy, 'r--', linewidth=2, label=\"f'(x)\")\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(name)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    \"\"\"Batch Normalization (Les 9)\"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, momentum=0.9, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Leerbare parameters\n",
    "        self.params['gamma'] = np.ones(n_features)\n",
    "        self.params['beta'] = np.zeros(n_features)\n",
    "        \n",
    "        # Running statistics\n",
    "        self.running_mean = np.zeros(n_features)\n",
    "        self.running_var = np.ones(n_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # Batch statistics\n",
    "            self.mu = np.mean(x, axis=0)\n",
    "            self.var = np.var(x, axis=0)\n",
    "            \n",
    "            # Update running statistics\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mu\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "        else:\n",
    "            self.mu = self.running_mean\n",
    "            self.var = self.running_var\n",
    "        \n",
    "        # Normalize\n",
    "        self.x_centered = x - self.mu\n",
    "        self.std = np.sqrt(self.var + self.epsilon)\n",
    "        self.x_norm = self.x_centered / self.std\n",
    "        \n",
    "        # Scale and shift\n",
    "        self.x = x\n",
    "        return self.params['gamma'] * self.x_norm + self.params['beta']\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        n = dout.shape[0]\n",
    "        \n",
    "        # Gradiënten voor gamma en beta\n",
    "        self.grads['gamma'] = np.sum(dout * self.x_norm, axis=0)\n",
    "        self.grads['beta'] = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Gradiënt voor x\n",
    "        dx_norm = dout * self.params['gamma']\n",
    "        dvar = np.sum(dx_norm * self.x_centered * -0.5 * (self.var + self.epsilon)**(-1.5), axis=0)\n",
    "        dmu = np.sum(dx_norm * -1 / self.std, axis=0) + dvar * np.mean(-2 * self.x_centered, axis=0)\n",
    "        dx = dx_norm / self.std + dvar * 2 * self.x_centered / n + dmu / n\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.7 Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(ABC):\n",
    "    \"\"\"Abstracte base class voor loss functies.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, y_pred, y_true):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        return self.forward(y_pred, y_true)\n",
    "\n",
    "\n",
    "class MSELoss(Loss):\n",
    "    \"\"\"Mean Squared Error: L = (1/n) Σ (y_pred - y_true)²\"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def backward(self):\n",
    "        n = self.y_pred.shape[0]\n",
    "        return 2 * (self.y_pred - self.y_true) / n\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    \"\"\"Cross-Entropy Loss met ingebouwde softmax.\"\"\"\n",
    "    \n",
    "    def forward(self, logits, y_true):\n",
    "        self.y_true = y_true\n",
    "        n = logits.shape[0]\n",
    "        \n",
    "        # Softmax\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        self.probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        \n",
    "        # Cross-entropy\n",
    "        correct_logprobs = -np.log(self.probs[np.arange(n), y_true] + 1e-10)\n",
    "        return np.mean(correct_logprobs)\n",
    "    \n",
    "    def backward(self):\n",
    "        n = self.probs.shape[0]\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(n), self.y_true] -= 1\n",
    "        return grad / n\n",
    "\n",
    "\n",
    "class BinaryCrossEntropyLoss(Loss):\n",
    "    \"\"\"Binary Cross-Entropy Loss.\"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
    "        self.y_true = y_true\n",
    "        return -np.mean(y_true * np.log(self.y_pred) + (1 - y_true) * np.log(1 - self.y_pred))\n",
    "    \n",
    "    def backward(self):\n",
    "        n = self.y_pred.shape[0]\n",
    "        return (self.y_pred - self.y_true) / (self.y_pred * (1 - self.y_pred) * n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.8 Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(ABC):\n",
    "    \"\"\"Abstracte base class voor optimizers.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, lr=0.01):\n",
    "        self.layers = layers\n",
    "        self.lr = lr\n",
    "    \n",
    "    @abstractmethod\n",
    "    def step(self):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.grads = {}\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, lr=0.01, momentum=0, weight_decay=0):\n",
    "        super().__init__(layers, lr)\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.velocity = {}\n",
    "    \n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            for name, param in layer.params.items():\n",
    "                if name not in layer.grads:\n",
    "                    continue\n",
    "                    \n",
    "                key = (i, name)\n",
    "                grad = layer.grads[name]\n",
    "                \n",
    "                # Weight decay (L2 regularisatie)\n",
    "                if self.weight_decay > 0:\n",
    "                    grad = grad + self.weight_decay * param\n",
    "                \n",
    "                # Momentum\n",
    "                if self.momentum > 0:\n",
    "                    if key not in self.velocity:\n",
    "                        self.velocity[key] = np.zeros_like(param)\n",
    "                    self.velocity[key] = self.momentum * self.velocity[key] - self.lr * grad\n",
    "                    layer.params[name] += self.velocity[key]\n",
    "                else:\n",
    "                    layer.params[name] -= self.lr * grad\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"Adam optimizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(layers, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}  # First moment\n",
    "        self.v = {}  # Second moment\n",
    "        self.t = 0   # Timestep\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            for name, param in layer.params.items():\n",
    "                if name not in layer.grads:\n",
    "                    continue\n",
    "                \n",
    "                key = (i, name)\n",
    "                grad = layer.grads[name]\n",
    "                \n",
    "                # Initialize moments\n",
    "                if key not in self.m:\n",
    "                    self.m[key] = np.zeros_like(param)\n",
    "                    self.v[key] = np.zeros_like(param)\n",
    "                \n",
    "                # Update moments\n",
    "                self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grad\n",
    "                self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * grad**2\n",
    "                \n",
    "                # Bias correction\n",
    "                m_hat = self.m[key] / (1 - self.beta1**self.t)\n",
    "                v_hat = self.v[key] / (1 - self.beta2**self.t)\n",
    "                \n",
    "                # Update parameters\n",
    "                layer.params[name] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.9 Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"Container voor sequentiële lagen.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        for layer in self.layers:\n",
    "            layer.eval()\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Return alle lagen met parameters.\"\"\"\n",
    "        return [layer for layer in self.layers if layer.params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.10 Alles Samen: MNIST Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print(\"MNIST laden...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "X, y = mnist.data / 255.0, mnist.target.astype(int)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "print(f\"Training: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bouw het netwerk\n",
    "model = Sequential([\n",
    "    Linear(784, 256),\n",
    "    BatchNorm(256),\n",
    "    ReLU(),\n",
    "    Linear(256, 128),\n",
    "    BatchNorm(128),\n",
    "    ReLU(),\n",
    "    Linear(128, 10)\n",
    "])\n",
    "\n",
    "# Loss en optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Model architectuur: 784 → 256 → 128 → 10\")\n",
    "print(f\"Aantal lagen: {len(model.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X, y):\n",
    "    \"\"\"Bereken accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    logits = model(X)\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    model.train()\n",
    "    return np.mean(preds == y)\n",
    "\n",
    "# Training loop\n",
    "batch_size = 128\n",
    "n_epochs = 10\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "train_losses = []\n",
    "test_accs = []\n",
    "\n",
    "print(\"Training starten...\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    X_shuffled = X_train[idx]\n",
    "    y_shuffled = y_train[idx]\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        dout = criterion.backward()\n",
    "        model.backward(dout)\n",
    "        \n",
    "        # Update\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evalueer\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    test_acc = accuracy(model, X_test, y_test)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}: Loss = {avg_loss:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nFinale test accuracy: {test_accs[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(train_losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(test_accs, 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Test Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer voorspellingen\n",
    "model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "indices = np.random.choice(len(X_test), 10, replace=False)\n",
    "\n",
    "for ax, idx in zip(axes.flatten(), indices):\n",
    "    img = X_test[idx].reshape(28, 28)\n",
    "    logits = model(X_test[idx:idx+1])\n",
    "    pred = np.argmax(logits)\n",
    "    true = y_test[idx]\n",
    "    \n",
    "    ax.imshow(img, cmap='gray')\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f'Pred: {pred}, True: {true}', color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Model Voorspellingen', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.11 Samenvatting\n",
    "\n",
    "We hebben een complete neural network library gebouwd met:\n",
    "\n",
    "| Component | Wiskundige basis | Les |\n",
    "|-----------|------------------|-----|\n",
    "| Linear layer | Matrixvermenigvuldiging | 2-4 |\n",
    "| Activations | Niet-lineaire functies + afgeleiden | 5 |\n",
    "| BatchNorm | Gemiddelde + variantie | 9 |\n",
    "| Loss functions | Maximum Likelihood | 10 |\n",
    "| Optimizers | Gradient descent | 6 |\n",
    "| Backprop | Kettingregel | 7 |\n",
    "\n",
    "**Alle wiskunde uit deze cursus komt samen in een werkend neuraal netwerk!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 11 van 12 | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
