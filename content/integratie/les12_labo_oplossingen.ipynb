{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 12: Labo - Oplossingen\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---\n",
    "\n",
    "Voorbeeldoplossingen voor de capstone projecten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Neural Network Library\n",
    "class Layer(ABC):\n",
    "    def __init__(self):\n",
    "        self.params, self.grads, self.training = {}, {}, True\n",
    "    @abstractmethod\n",
    "    def forward(self, x): pass\n",
    "    @abstractmethod\n",
    "    def backward(self, dout): pass\n",
    "    def __call__(self, x): return self.forward(x)\n",
    "\n",
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, y_pred, y_true): pass\n",
    "    @abstractmethod\n",
    "    def backward(self): pass\n",
    "    def __call__(self, y_pred, y_true): return self.forward(y_pred, y_true)\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_f, out_f):\n",
    "        super().__init__()\n",
    "        self.params['W'] = np.random.randn(in_f, out_f) * np.sqrt(2.0/in_f)\n",
    "        self.params['b'] = np.zeros(out_f)\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.params['W'] + self.params['b']\n",
    "    def backward(self, dout):\n",
    "        n = self.x.shape[0]\n",
    "        self.grads['W'] = self.x.T @ dout / n\n",
    "        self.grads['b'] = np.mean(dout, axis=0)\n",
    "        return dout @ self.params['W'].T\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def forward(self, x):\n",
    "        self.mask = x > 0\n",
    "        return np.maximum(0, x)\n",
    "    def backward(self, dout): return dout * self.mask\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, x):\n",
    "        self.out = 1/(1+np.exp(-np.clip(x,-500,500)))\n",
    "        return self.out\n",
    "    def backward(self, dout): return dout * self.out * (1-self.out)\n",
    "\n",
    "class MSELoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred, self.y_true = y_pred, y_true\n",
    "        return np.mean((y_pred - y_true)**2)\n",
    "    def backward(self):\n",
    "        return 2*(self.y_pred - self.y_true)/self.y_pred.shape[0]\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, logits, y_true):\n",
    "        self.y_true = y_true\n",
    "        n = logits.shape[0]\n",
    "        exp_l = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        self.probs = exp_l / np.sum(exp_l, axis=1, keepdims=True)\n",
    "        return -np.mean(np.log(self.probs[np.arange(n), y_true]+1e-10))\n",
    "    def backward(self):\n",
    "        n = self.probs.shape[0]\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(n), self.y_true] -= 1\n",
    "        return grad / n\n",
    "\n",
    "class BinaryCrossEntropyLoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = np.clip(y_pred, 1e-10, 1-1e-10)\n",
    "        self.y_true = y_true\n",
    "        return -np.mean(y_true*np.log(self.y_pred) + (1-y_true)*np.log(1-self.y_pred))\n",
    "    def backward(self):\n",
    "        return (self.y_pred - self.y_true)/(self.y_pred*(1-self.y_pred)*len(self.y_true))\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, layers, lr=0.001, b1=0.9, b2=0.999, eps=1e-8):\n",
    "        self.layers, self.lr, self.b1, self.b2, self.eps = layers, lr, b1, b2, eps\n",
    "        self.m, self.v, self.t = {}, {}, 0\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, l in enumerate(self.layers):\n",
    "            for n, p in l.params.items():\n",
    "                if n not in l.grads: continue\n",
    "                k = (i,n)\n",
    "                g = l.grads[n]\n",
    "                if k not in self.m: self.m[k], self.v[k] = np.zeros_like(p), np.zeros_like(p)\n",
    "                self.m[k] = self.b1*self.m[k] + (1-self.b1)*g\n",
    "                self.v[k] = self.b2*self.v[k] + (1-self.b2)*g**2\n",
    "                m_h = self.m[k]/(1-self.b1**self.t)\n",
    "                v_h = self.v[k]/(1-self.b2**self.t)\n",
    "                l.params[n] -= self.lr * m_h/(np.sqrt(v_h)+self.eps)\n",
    "    def zero_grad(self):\n",
    "        for l in self.layers: l.grads = {}\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers): self.layers = layers\n",
    "    def forward(self, x):\n",
    "        for l in self.layers: x = l.forward(x)\n",
    "        return x\n",
    "    def backward(self, d):\n",
    "        for l in reversed(self.layers): d = l.backward(d)\n",
    "        return d\n",
    "    def __call__(self, x): return self.forward(x)\n",
    "    def parameters(self): return [l for l in self.layers if l.params]\n",
    "\n",
    "print(\"Library geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Project A: Huizenprijzen Regressie - Oplossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target.reshape(-1, 1)\n",
    "\n",
    "# Preprocessing\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X = scaler_X.fit_transform(X)\n",
    "y = scaler_y.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Sequential([\n",
    "    Linear(8, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 32),\n",
    "    ReLU(),\n",
    "    Linear(32, 1)\n",
    "])\n",
    "\n",
    "criterion = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "n_epochs = 50\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    epoch_loss = 0\n",
    "    n_batches = len(X_train) // batch_size\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        X_b = X_train[idx[start:start+batch_size]]\n",
    "        y_b = y_train[idx[start:start+batch_size]]\n",
    "        \n",
    "        pred = model(X_b)\n",
    "        loss = criterion(pred, y_b)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.backward(criterion.backward())\n",
    "        optimizer.step()\n",
    "    \n",
    "    losses.append(epoch_loss / n_batches)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss = {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluatie\n",
    "y_pred = model(X_test)\n",
    "mse = np.mean((y_pred - y_test)**2)\n",
    "\n",
    "# R² score\n",
    "ss_res = np.sum((y_test - y_pred)**2)\n",
    "ss_tot = np.sum((y_test - np.mean(y_test))**2)\n",
    "r2 = 1 - ss_res / ss_tot\n",
    "\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(y_test, y_pred, alpha=0.5)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "axes[1].set_xlabel('Actual')\n",
    "axes[1].set_ylabel('Predicted')\n",
    "axes[1].set_title(f'Predicted vs Actual (R²={r2:.3f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Project B: Iris Classificatie - Oplossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Normaliseer\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Split\n",
    "idx = np.random.permutation(len(X))\n",
    "X, y = X[idx], y[idx]\n",
    "X_train, X_test = X[:120], X[120:]\n",
    "y_train, y_test = y[:120], y[120:]\n",
    "\n",
    "# Model\n",
    "model = Sequential([\n",
    "    Linear(4, 16),\n",
    "    ReLU(),\n",
    "    Linear(16, 8),\n",
    "    ReLU(),\n",
    "    Linear(8, 3)\n",
    "])\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "for epoch in range(200):\n",
    "    logits = model(X_train)\n",
    "    loss = criterion(logits, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    model.backward(criterion.backward())\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        acc = np.mean(np.argmax(model(X_test), axis=1) == y_test)\n",
    "        print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Test Acc={acc:.4f}\")\n",
    "\n",
    "# Final accuracy\n",
    "acc = np.mean(np.argmax(model(X_test), axis=1) == y_test)\n",
    "print(f\"\\nFinal Test Accuracy: {acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Project C: Spam Detectie - Oplossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "n_features = 20\n",
    "\n",
    "X_spam = np.random.exponential(0.5, (n_samples//2, n_features)) + 0.3\n",
    "X_ham = np.random.exponential(0.3, (n_samples//2, n_features))\n",
    "X = np.vstack([X_spam, X_ham])\n",
    "y = np.array([1]*(n_samples//2) + [0]*(n_samples//2)).reshape(-1, 1)\n",
    "\n",
    "idx = np.random.permutation(n_samples)\n",
    "X, y = X[idx], y[idx]\n",
    "\n",
    "X_train, X_test = X[:1600], X[1600:]\n",
    "y_train, y_test = y[:1600], y[1600:]\n",
    "\n",
    "# Normalize\n",
    "X_train = (X_train - X_train.mean(axis=0)) / (X_train.std(axis=0) + 1e-8)\n",
    "X_test = (X_test - X_train.mean(axis=0)) / (X_train.std(axis=0) + 1e-8)\n",
    "\n",
    "# Model met Sigmoid output\n",
    "model = Sequential([\n",
    "    Linear(20, 32),\n",
    "    ReLU(),\n",
    "    Linear(32, 16),\n",
    "    ReLU(),\n",
    "    Linear(16, 1),\n",
    "    Sigmoid()\n",
    "])\n",
    "\n",
    "criterion = BinaryCrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "batch_size = 64\n",
    "for epoch in range(100):\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_b = X_train[idx[i:i+batch_size]]\n",
    "        y_b = y_train[idx[i:i+batch_size]]\n",
    "        \n",
    "        pred = model(X_b)\n",
    "        loss = criterion(pred, y_b)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.backward(criterion.backward())\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        pred_test = (model(X_test) > 0.5).astype(int)\n",
    "        acc = np.mean(pred_test == y_test)\n",
    "        print(f\"Epoch {epoch+1}: Acc={acc:.4f}\")\n",
    "\n",
    "# Final\n",
    "pred_test = (model(X_test) > 0.5).astype(int)\n",
    "acc = np.mean(pred_test == y_test)\n",
    "print(f\"\\nFinal Accuracy: {acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Project D: Digit Sum - Oplossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Laad MNIST\n",
    "print(\"MNIST laden...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "X_mnist, y_mnist = mnist.data / 255.0, mnist.target.astype(int)\n",
    "\n",
    "# Genereer paren\n",
    "n_pairs = 20000\n",
    "idx1 = np.random.randint(0, len(X_mnist), n_pairs)\n",
    "idx2 = np.random.randint(0, len(X_mnist), n_pairs)\n",
    "\n",
    "X = np.hstack([X_mnist[idx1], X_mnist[idx2]])\n",
    "y = y_mnist[idx1] + y_mnist[idx2]\n",
    "\n",
    "# Split\n",
    "X_train, X_test = X[:16000], X[16000:]\n",
    "y_train, y_test = y[:16000], y[16000:]\n",
    "\n",
    "print(f\"Train: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Sequential([\n",
    "    Linear(1568, 256),\n",
    "    ReLU(),\n",
    "    Linear(256, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 19)  # 0-18\n",
    "])\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "for epoch in range(10):\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        X_b = X_train[idx[start:start+batch_size]]\n",
    "        y_b = y_train[idx[start:start+batch_size]]\n",
    "        \n",
    "        logits = model(X_b)\n",
    "        loss = criterion(logits, y_b)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.backward(criterion.backward())\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Eval\n",
    "    preds = np.argmax(model(X_test), axis=1)\n",
    "    acc = np.mean(preds == y_test)\n",
    "    print(f\"Epoch {epoch+1}: Loss={epoch_loss/n_batches:.4f}, Acc={acc:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Accuracy: {acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer voorbeelden\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    i = np.random.randint(len(X_test))\n",
    "    img1 = X_test[i, :784].reshape(28, 28)\n",
    "    img2 = X_test[i, 784:].reshape(28, 28)\n",
    "    combined = np.hstack([img1, img2])\n",
    "    \n",
    "    pred = np.argmax(model(X_test[i:i+1]))\n",
    "    true = y_test[i]\n",
    "    \n",
    "    ax.imshow(combined, cmap='gray')\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f'Pred: {pred}, True: {true}', color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Digit Sum Predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflectie Antwoorden\n",
    "\n",
    "1. **Wiskundige concepten**: Matrixvermenigvuldiging (forward pass), afgeleiden en kettingregel (backward pass), gradient descent (optimalisatie), softmax/cross-entropy (classificatie).\n",
    "\n",
    "2. **Architectuurkeuze**: Meerdere hidden layers voor voldoende capaciteit, ReLU voor snelle training en geen vanishing gradients, BatchNorm voor stabiele training.\n",
    "\n",
    "3. **Problemen**: Overfitting → oplossing: dropout, regularisatie. Vanishing gradients → oplossing: ReLU, He initialisatie. Slow convergence → oplossing: Adam optimizer.\n",
    "\n",
    "4. **Verbeteringen**: Meer experimenten met hyperparameters, learning rate scheduling, data augmentatie, diepere architecturen.\n",
    "\n",
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 12 Oplossingen | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
