{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 11: Labo - Oplossingen\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base classes\n",
    "class Layer(ABC):\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        self.training = True\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x): pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, dout): pass\n",
    "    \n",
    "    def __call__(self, x): return self.forward(x)\n",
    "    def train(self): self.training = True\n",
    "    def eval(self): self.training = False\n",
    "\n",
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, y_pred, y_true): pass\n",
    "    @abstractmethod\n",
    "    def backward(self): pass\n",
    "    def __call__(self, y_pred, y_true): return self.forward(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 1: Linear Layer - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 1a\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # He initialisatie\n",
    "        std = np.sqrt(2.0 / in_features)\n",
    "        self.params['W'] = np.random.randn(in_features, out_features) * std\n",
    "        self.params['b'] = np.zeros(out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.params['W'] + self.params['b']\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        n = self.x.shape[0]\n",
    "        self.grads['W'] = self.x.T @ dout / n\n",
    "        self.grads['b'] = np.mean(dout, axis=0)\n",
    "        return dout @ self.params['W'].T\n",
    "\n",
    "# Test\n",
    "linear = Linear(3, 2)\n",
    "x = np.random.randn(4, 3)\n",
    "out = linear.forward(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 1b - Gradient check\n",
    "def gradient_check(layer, x, eps=1e-5):\n",
    "    \"\"\"Check gradients numerically.\"\"\"\n",
    "    # Forward\n",
    "    out = layer.forward(x)\n",
    "    dout = np.ones_like(out)\n",
    "    layer.backward(dout)\n",
    "    \n",
    "    # Check W gradients\n",
    "    for i in range(layer.params['W'].shape[0]):\n",
    "        for j in range(layer.params['W'].shape[1]):\n",
    "            old_val = layer.params['W'][i, j]\n",
    "            \n",
    "            layer.params['W'][i, j] = old_val + eps\n",
    "            out_plus = np.sum(layer.forward(x))\n",
    "            \n",
    "            layer.params['W'][i, j] = old_val - eps\n",
    "            out_minus = np.sum(layer.forward(x))\n",
    "            \n",
    "            layer.params['W'][i, j] = old_val\n",
    "            \n",
    "            numerical_grad = (out_plus - out_minus) / (2 * eps) / x.shape[0]\n",
    "            analytical_grad = layer.grads['W'][i, j]\n",
    "            \n",
    "            diff = abs(numerical_grad - analytical_grad)\n",
    "            if diff > 1e-5:\n",
    "                print(f\"Gradient mismatch at W[{i},{j}]: numerical={numerical_grad:.6f}, analytical={analytical_grad:.6f}\")\n",
    "                return False\n",
    "    \n",
    "    print(\"Gradient check passed!\")\n",
    "    return True\n",
    "\n",
    "gradient_check(Linear(3, 2), np.random.randn(4, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 2: Activatiefuncties - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 2a\n",
    "class ReLU(Layer):\n",
    "    def forward(self, x):\n",
    "        self.mask = (x > 0)\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.out * (1 - self.out)\n",
    "\n",
    "# Test\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "relu = ReLU()\n",
    "print(f\"ReLU({x}) = {relu.forward(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 2b - ELU\n",
    "class ELU(Layer):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.mask = x > 0\n",
    "        return np.where(self.mask, x, self.alpha * (np.exp(x) - 1))\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # f'(x) = 1 if x > 0, else alpha * e^x\n",
    "        return dout * np.where(self.mask, 1, self.alpha * np.exp(self.x))\n",
    "\n",
    "# Visualiseer\n",
    "x = np.linspace(-3, 3, 100)\n",
    "elu = ELU(alpha=1.0)\n",
    "y = elu.forward(x)\n",
    "dy = elu.backward(np.ones_like(x))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='ELU(x)')\n",
    "plt.plot(x, dy, 'r--', linewidth=2, label=\"ELU'(x)\")\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('ELU Activation Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 3: Loss Functions - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 3a\n",
    "class MSELoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def backward(self):\n",
    "        n = self.y_pred.shape[0]\n",
    "        return 2 * (self.y_pred - self.y_true) / n\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, logits, y_true):\n",
    "        self.y_true = y_true\n",
    "        n = logits.shape[0]\n",
    "        \n",
    "        # Softmax\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        self.probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        \n",
    "        # Cross-entropy\n",
    "        correct_logprobs = -np.log(self.probs[np.arange(n), y_true] + 1e-10)\n",
    "        return np.mean(correct_logprobs)\n",
    "    \n",
    "    def backward(self):\n",
    "        n = self.probs.shape[0]\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(n), self.y_true] -= 1\n",
    "        return grad / n\n",
    "\n",
    "# Test\n",
    "logits = np.array([[2.0, 1.0, 0.1], [0.5, 2.0, 0.3]])\n",
    "y_true = np.array([0, 1])\n",
    "\n",
    "ce_loss = CrossEntropyLoss()\n",
    "loss = ce_loss.forward(logits, y_true)\n",
    "grad = ce_loss.backward()\n",
    "\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Probs:\\n{ce_loss.probs}\")\n",
    "print(f\"Gradient:\\n{grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 4: SGD Optimizer - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 4a\n",
    "class SGD:\n",
    "    def __init__(self, layers, lr=0.01, momentum=0, weight_decay=0):\n",
    "        self.layers = layers\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.velocity = {}\n",
    "    \n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            for name, param in layer.params.items():\n",
    "                if name not in layer.grads:\n",
    "                    continue\n",
    "                \n",
    "                key = (i, name)\n",
    "                grad = layer.grads[name]\n",
    "                \n",
    "                # Weight decay\n",
    "                if self.weight_decay > 0:\n",
    "                    grad = grad + self.weight_decay * param\n",
    "                \n",
    "                # Momentum\n",
    "                if self.momentum > 0:\n",
    "                    if key not in self.velocity:\n",
    "                        self.velocity[key] = np.zeros_like(param)\n",
    "                    self.velocity[key] = self.momentum * self.velocity[key] - self.lr * grad\n",
    "                    layer.params[name] += self.velocity[key]\n",
    "                else:\n",
    "                    layer.params[name] -= self.lr * grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.grads = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 5: Sequential Model - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 5a\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            layer.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        for layer in self.layers:\n",
    "            layer.training = False\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [l for l in self.layers if l.params]\n",
    "\n",
    "# Test\n",
    "model = Sequential([\n",
    "    Linear(10, 5),\n",
    "    ReLU(),\n",
    "    Linear(5, 2)\n",
    "])\n",
    "\n",
    "x = np.random.randn(3, 10)\n",
    "out = model(x)\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 6: Train MNIST - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print(\"MNIST laden...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "X, y = mnist.data / 255.0, mnist.target.astype(int)\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "print(f\"Training: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bouw model\n",
    "model = Sequential([\n",
    "    Linear(784, 256),\n",
    "    ReLU(),\n",
    "    Linear(256, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 10)\n",
    "])\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "n_epochs = 5\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        X_batch = X_train[idx[start:start+batch_size]]\n",
    "        y_batch = y_train[idx[start:start+batch_size]]\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        dout = criterion.backward()\n",
    "        model.backward(dout)\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    logits = model(X_test)\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = np.mean(preds == y_test)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss={epoch_loss/n_batches:.4f}, Test Acc={acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 7: Dropout - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdracht 7a\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p  # Dropout probability\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # Create mask: keep probability is (1-p)\n",
    "            self.mask = (np.random.random(x.shape) > self.p) / (1 - self.p)\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        if self.training:\n",
    "            return dout * self.mask\n",
    "        else:\n",
    "            return dout\n",
    "\n",
    "# Test\n",
    "dropout = Dropout(p=0.5)\n",
    "x = np.ones((3, 4))\n",
    "\n",
    "dropout.train()\n",
    "out_train = dropout.forward(x)\n",
    "print(f\"Training (met dropout):\\n{out_train}\")\n",
    "\n",
    "dropout.eval()\n",
    "out_eval = dropout.forward(x)\n",
    "print(f\"\\nEvaluation (geen dropout):\\n{out_eval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Oefening 8: Experimenten - Oplossingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergelijk activatiefuncties\n",
    "def train_and_evaluate(model, n_epochs=3):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    \n",
    "    accs = []\n",
    "    for epoch in range(n_epochs):\n",
    "        idx = np.random.permutation(len(X_train))\n",
    "        for batch in range(100):  # Subset for speed\n",
    "            start = batch * batch_size\n",
    "            X_batch = X_train[idx[start:start+batch_size]]\n",
    "            y_batch = y_train[idx[start:start+batch_size]]\n",
    "            \n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            model.backward(criterion.backward())\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        logits = model(X_test[:1000])\n",
    "        acc = np.mean(np.argmax(logits, axis=1) == y_test[:1000])\n",
    "        accs.append(acc)\n",
    "    \n",
    "    return accs\n",
    "\n",
    "results = {}\n",
    "\n",
    "# ReLU\n",
    "np.random.seed(42)\n",
    "model_relu = Sequential([Linear(784, 128), ReLU(), Linear(128, 10)])\n",
    "results['ReLU'] = train_and_evaluate(model_relu)\n",
    "\n",
    "# Sigmoid\n",
    "np.random.seed(42)\n",
    "model_sigmoid = Sequential([Linear(784, 128), Sigmoid(), Linear(128, 10)])\n",
    "results['Sigmoid'] = train_and_evaluate(model_sigmoid)\n",
    "\n",
    "# ELU\n",
    "np.random.seed(42)\n",
    "model_elu = Sequential([Linear(784, 128), ELU(), Linear(128, 10)])\n",
    "results['ELU'] = train_and_evaluate(model_elu)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "for name, accs in results.items():\n",
    "    plt.plot(accs, 'o-', label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Vergelijking Activatiefuncties')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonusoefening: Conv2D - Oplossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(Layer):\n",
    "    \"\"\"Simple 2D convolution (no padding, stride=1).\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # He initialization\n",
    "        std = np.sqrt(2.0 / (in_channels * kernel_size * kernel_size))\n",
    "        self.params['W'] = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * std\n",
    "        self.params['b'] = np.zeros(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, in_channels, H, W)\n",
    "        self.x = x\n",
    "        batch, _, H, W = x.shape\n",
    "        k = self.kernel_size\n",
    "        H_out = H - k + 1\n",
    "        W_out = W - k + 1\n",
    "        \n",
    "        out = np.zeros((batch, self.out_channels, H_out, W_out))\n",
    "        \n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                region = x[:, :, i:i+k, j:j+k]  # (batch, in_ch, k, k)\n",
    "                for f in range(self.out_channels):\n",
    "                    out[:, f, i, j] = np.sum(region * self.params['W'][f], axis=(1,2,3)) + self.params['b'][f]\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        batch, _, H, W = self.x.shape\n",
    "        k = self.kernel_size\n",
    "        H_out, W_out = dout.shape[2], dout.shape[3]\n",
    "        \n",
    "        self.grads['W'] = np.zeros_like(self.params['W'])\n",
    "        self.grads['b'] = np.sum(dout, axis=(0, 2, 3))\n",
    "        dx = np.zeros_like(self.x)\n",
    "        \n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                region = self.x[:, :, i:i+k, j:j+k]\n",
    "                for f in range(self.out_channels):\n",
    "                    self.grads['W'][f] += np.sum(region * dout[:, f, i, j][:, None, None, None], axis=0)\n",
    "                    dx[:, :, i:i+k, j:j+k] += self.params['W'][f] * dout[:, f, i, j][:, None, None, None]\n",
    "        \n",
    "        self.grads['W'] /= batch\n",
    "        return dx\n",
    "\n",
    "# Test\n",
    "conv = Conv2D(1, 2, 3)  # 1 input channel, 2 filters, 3x3 kernel\n",
    "x = np.random.randn(2, 1, 5, 5)  # 2 images, 1 channel, 5x5\n",
    "out = conv.forward(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 11 Oplossingen | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
