{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 12: Capstone Project - Van Wiskunde naar AI\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.0 Welkom bij de Finale!\n",
    "\n",
    "Gefeliciteerd! Je hebt een indrukwekkende reis gemaakt door de wiskunde van deep learning. In deze laatste les brengen we alles samen in een **capstone project** waar je zelf een volledig machine learning systeem ontwerpt, implementeert en evalueert.\n",
    "\n",
    "### Wat je hebt geleerd\n",
    "\n",
    "| Deel | Onderwerp | Toepassing in Neural Networks |\n",
    "|------|-----------|------------------------------|\n",
    "| 1 | Lineaire Algebra | Data representatie, forward pass |\n",
    "| 2 | Calculus | Optimalisatie, backpropagation |\n",
    "| 3 | Statistiek | Loss functies, output interpretatie |\n",
    "| 4 | Integratie | Complete systemen bouwen |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Leerdoelen\n",
    "\n",
    "Na deze les kun je zelfstandig een machine learning project uitvoeren. Je kunt de juiste wiskundige tools kiezen voor een probleem. Je kunt een model ontwerpen, trainen en evalueren. Je kunt je resultaten interpreteren en presenteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries geladen!\")\n",
    "print(\"Klaar voor het capstone project!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 De Complete Neural Network Library\n",
    "\n",
    "Hier is onze volledige library uit Les 11, klaar voor gebruik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BASE CLASSES =====\n",
    "class Layer(ABC):\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        self.training = True\n",
    "    @abstractmethod\n",
    "    def forward(self, x): pass\n",
    "    @abstractmethod\n",
    "    def backward(self, dout): pass\n",
    "    def __call__(self, x): return self.forward(x)\n",
    "    def train(self): self.training = True\n",
    "    def eval(self): self.training = False\n",
    "\n",
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, y_pred, y_true): pass\n",
    "    @abstractmethod\n",
    "    def backward(self): pass\n",
    "    def __call__(self, y_pred, y_true): return self.forward(y_pred, y_true)\n",
    "\n",
    "# ===== LAYERS =====\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        std = np.sqrt(2.0 / in_features)\n",
    "        self.params['W'] = np.random.randn(in_features, out_features) * std\n",
    "        self.params['b'] = np.zeros(out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.params['W'] + self.params['b']\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        n = self.x.shape[0]\n",
    "        self.grads['W'] = self.x.T @ dout / n\n",
    "        self.grads['b'] = np.mean(dout, axis=0)\n",
    "        return dout @ self.params['W'].T\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def forward(self, x):\n",
    "        self.mask = (x > 0)\n",
    "        return np.maximum(0, x)\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        return self.out\n",
    "    def backward(self, dout):\n",
    "        return dout * self.out * (1 - self.out)\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        return self.out\n",
    "    def backward(self, dout):\n",
    "        return dout * (1 - self.out ** 2)\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.mask = (np.random.random(x.shape) > self.p) / (1 - self.p)\n",
    "            return x * self.mask\n",
    "        return x\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask if self.training else dout\n",
    "\n",
    "class BatchNorm(Layer):\n",
    "    def __init__(self, n_features, momentum=0.9, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.params['gamma'] = np.ones(n_features)\n",
    "        self.params['beta'] = np.zeros(n_features)\n",
    "        self.running_mean = np.zeros(n_features)\n",
    "        self.running_var = np.ones(n_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.mu = np.mean(x, axis=0)\n",
    "            self.var = np.var(x, axis=0)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mu\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "        else:\n",
    "            self.mu = self.running_mean\n",
    "            self.var = self.running_var\n",
    "        self.x_centered = x - self.mu\n",
    "        self.std = np.sqrt(self.var + self.epsilon)\n",
    "        self.x_norm = self.x_centered / self.std\n",
    "        return self.params['gamma'] * self.x_norm + self.params['beta']\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        n = dout.shape[0]\n",
    "        self.grads['gamma'] = np.sum(dout * self.x_norm, axis=0)\n",
    "        self.grads['beta'] = np.sum(dout, axis=0)\n",
    "        dx_norm = dout * self.params['gamma']\n",
    "        dvar = np.sum(dx_norm * self.x_centered * -0.5 * (self.var + self.epsilon)**(-1.5), axis=0)\n",
    "        dmu = np.sum(dx_norm * -1 / self.std, axis=0) + dvar * np.mean(-2 * self.x_centered, axis=0)\n",
    "        return dx_norm / self.std + dvar * 2 * self.x_centered / n + dmu / n\n",
    "\n",
    "# ===== LOSS FUNCTIONS =====\n",
    "class MSELoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred, self.y_true = y_pred, y_true\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    def backward(self):\n",
    "        return 2 * (self.y_pred - self.y_true) / self.y_pred.shape[0]\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, logits, y_true):\n",
    "        self.y_true = y_true\n",
    "        n = logits.shape[0]\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        self.probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        return -np.mean(np.log(self.probs[np.arange(n), y_true] + 1e-10))\n",
    "    def backward(self):\n",
    "        n = self.probs.shape[0]\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(n), self.y_true] -= 1\n",
    "        return grad / n\n",
    "\n",
    "class BinaryCrossEntropyLoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = np.clip(y_pred, 1e-10, 1-1e-10)\n",
    "        self.y_true = y_true\n",
    "        return -np.mean(y_true * np.log(self.y_pred) + (1-y_true) * np.log(1-self.y_pred))\n",
    "    def backward(self):\n",
    "        return (self.y_pred - self.y_true) / (self.y_pred * (1 - self.y_pred) * len(self.y_true))\n",
    "\n",
    "# ===== OPTIMIZERS =====\n",
    "class SGD:\n",
    "    def __init__(self, layers, lr=0.01, momentum=0, weight_decay=0):\n",
    "        self.layers = layers\n",
    "        self.lr, self.momentum, self.weight_decay = lr, momentum, weight_decay\n",
    "        self.velocity = {}\n",
    "    \n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            for name, param in layer.params.items():\n",
    "                if name not in layer.grads: continue\n",
    "                key = (i, name)\n",
    "                grad = layer.grads[name] + self.weight_decay * param if self.weight_decay else layer.grads[name]\n",
    "                if self.momentum:\n",
    "                    if key not in self.velocity: self.velocity[key] = np.zeros_like(param)\n",
    "                    self.velocity[key] = self.momentum * self.velocity[key] - self.lr * grad\n",
    "                    layer.params[name] += self.velocity[key]\n",
    "                else:\n",
    "                    layer.params[name] -= self.lr * grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers: layer.grads = {}\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, layers, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.layers, self.lr = layers, lr\n",
    "        self.beta1, self.beta2, self.epsilon = beta1, beta2, epsilon\n",
    "        self.m, self.v, self.t = {}, {}, 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            for name, param in layer.params.items():\n",
    "                if name not in layer.grads: continue\n",
    "                key = (i, name)\n",
    "                grad = layer.grads[name]\n",
    "                if key not in self.m:\n",
    "                    self.m[key], self.v[key] = np.zeros_like(param), np.zeros_like(param)\n",
    "                self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grad\n",
    "                self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * grad**2\n",
    "                m_hat = self.m[key] / (1 - self.beta1**self.t)\n",
    "                v_hat = self.v[key] / (1 - self.beta2**self.t)\n",
    "                layer.params[name] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers: layer.grads = {}\n",
    "\n",
    "# ===== SEQUENTIAL MODEL =====\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers: x = layer.forward(x)\n",
    "        return x\n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers): dout = layer.backward(dout)\n",
    "        return dout\n",
    "    def __call__(self, x): return self.forward(x)\n",
    "    def train(self):\n",
    "        for layer in self.layers: layer.training = True\n",
    "    def eval(self):\n",
    "        for layer in self.layers: layer.training = False\n",
    "    def parameters(self):\n",
    "        return [l for l in self.layers if l.params]\n",
    "\n",
    "print(\"Neural Network Library geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Capstone Project: Fashion-MNIST Classifier\n",
    "\n",
    "We bouwen een classifier voor Fashion-MNIST: 10 categorieÃ«n kleding.\n",
    "\n",
    "Dit project demonstreert alle concepten uit de cursus:\n",
    "- **Data preprocessing**: normalisatie (Les 9)\n",
    "- **Model design**: lagen kiezen (Les 2-4, 11)\n",
    "- **Training**: loss, optimizer, backprop (Les 5-7, 10)\n",
    "- **Evaluation**: accuracy, confusion matrix (Les 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad Fashion-MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print(\"Fashion-MNIST laden...\")\n",
    "fashion = fetch_openml('Fashion-MNIST', version=1, as_frame=False, parser='auto')\n",
    "X, y = fashion.data / 255.0, fashion.target.astype(int)\n",
    "\n",
    "# Split\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "# Labels\n",
    "class_names = ['T-shirt', 'Broek', 'Trui', 'Jurk', 'Jas', \n",
    "               'Sandaal', 'Shirt', 'Sneaker', 'Tas', 'Laars']\n",
    "\n",
    "print(f\"Training: {X_train.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")\n",
    "print(f\"Klassen: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer voorbeelden\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    idx = np.random.randint(len(X_train))\n",
    "    ax.imshow(X_train[idx].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(class_names[y_train[idx]])\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Fashion-MNIST Voorbeelden', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 1: Model Ontwerpen\n",
    "\n",
    "We ontwerpen een netwerk met:\n",
    "- Input: 784 (28Ã—28 pixels)\n",
    "- Hidden layers met BatchNorm en Dropout\n",
    "- Output: 10 klassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architectuur\n",
    "model = Sequential([\n",
    "    # Layer 1: 784 â†’ 512\n",
    "    Linear(784, 512),\n",
    "    BatchNorm(512),\n",
    "    ReLU(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Layer 2: 512 â†’ 256\n",
    "    Linear(512, 256),\n",
    "    BatchNorm(256),\n",
    "    ReLU(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Layer 3: 256 â†’ 128\n",
    "    Linear(256, 128),\n",
    "    BatchNorm(128),\n",
    "    ReLU(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Output: 128 â†’ 10\n",
    "    Linear(128, 10)\n",
    "])\n",
    "\n",
    "print(\"Model: 784 â†’ 512 â†’ 256 â†’ 128 â†’ 10\")\n",
    "print(f\"Aantal lagen: {len(model.layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 2: Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "n_epochs = 15\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss en optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Helper functies\n",
    "def compute_accuracy(model, X, y, batch_size=1000):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "        logits = model(X_batch)\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        correct += np.sum(preds == y_batch)\n",
    "    model.train()\n",
    "    return correct / len(X)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Epochs: {n_epochs}\")\n",
    "print(f\"Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 3: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "n_batches = len(X_train) // batch_size\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "print(\"Training starten...\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    X_shuffled = X_train[idx]\n",
    "    y_shuffled = y_train[idx]\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        X_batch = X_shuffled[start:start+batch_size]\n",
    "        y_batch = y_shuffled[start:start+batch_size]\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        model.backward(criterion.backward())\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    train_acc = compute_accuracy(model, X_train[:5000], y_train[:5000])\n",
    "    test_acc = compute_accuracy(model, X_test, y_test)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}: Loss={avg_loss:.4f}, Train={train_acc:.4f}, Test={test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nâœ“ Training compleet!\")\n",
    "print(f\"âœ“ Beste test accuracy: {max(test_accs)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 4: Resultaten Visualiseren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(train_losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(train_accs, 'b-', linewidth=2, label='Train')\n",
    "axes[1].plot(test_accs, 'r-', linewidth=2, label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "model.eval()\n",
    "all_preds = []\n",
    "for i in range(0, len(X_test), 1000):\n",
    "    logits = model(X_test[i:i+1000])\n",
    "    all_preds.extend(np.argmax(logits, axis=1))\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "# Bereken confusion matrix\n",
    "conf_matrix = np.zeros((10, 10), dtype=int)\n",
    "for true, pred in zip(y_test, all_preds):\n",
    "    conf_matrix[true, pred] += 1\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(conf_matrix, cmap='Blues')\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        plt.text(j, i, conf_matrix[i, j], ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.xticks(range(10), class_names, rotation=45, ha='right')\n",
    "plt.yticks(range(10), class_names)\n",
    "plt.xlabel('Voorspelling')\n",
    "plt.ylabel('Werkelijk')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nAccuracy per klasse:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    class_acc = conf_matrix[i, i] / np.sum(conf_matrix[i, :])\n",
    "    print(f\"  {name:10s}: {class_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer voorspellingen\n",
    "model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(14, 9))\n",
    "indices = np.random.choice(len(X_test), 15, replace=False)\n",
    "\n",
    "for ax, idx in zip(axes.flatten(), indices):\n",
    "    img = X_test[idx].reshape(28, 28)\n",
    "    logits = model(X_test[idx:idx+1])\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    pred = np.argmax(logits)\n",
    "    true = y_test[idx]\n",
    "    \n",
    "    ax.imshow(img, cmap='gray')\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f'Pred: {class_names[pred]}\\nTrue: {class_names[true]}\\nConf: {probs[0, pred]*100:.1f}%', \n",
    "                 color=color, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Model Voorspellingen (groen=correct, rood=fout)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Reflectie: De Wiskunde Achter het Succes\n",
    "\n",
    "Laten we terugkijken op welke wiskunde we hebben gebruikt:\n",
    "\n",
    "### Lineaire Algebra (Les 1-4)\n",
    "- **Matrixvermenigvuldiging**: X @ W in elke Linear layer\n",
    "- **Vectoroptelling**: + b voor de bias\n",
    "- **Batch processing**: meerdere samples tegelijk\n",
    "\n",
    "### Calculus (Les 5-7)\n",
    "- **Afgeleiden**: elke layer heeft een backward() methode\n",
    "- **Kettingregel**: backpropagation door alle lagen\n",
    "- **Gradient descent**: optimizer.step() update de parameters\n",
    "\n",
    "### Statistiek (Les 8-10)\n",
    "- **Softmax**: zet logits om in kansen\n",
    "- **Cross-entropy**: meet verschil tussen voorspelde en echte verdeling\n",
    "- **Batch normalization**: normaliseert naar Î¼=0, Ïƒ=1\n",
    "- **Maximum Likelihood**: cross-entropy = negative log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5 Conclusie\n",
    "\n",
    "### Wat je hebt bereikt\n",
    "\n",
    "Je hebt:\n",
    "1. âœ… De wiskundige basis van deep learning geleerd\n",
    "2. âœ… Een complete neural network library from scratch gebouwd\n",
    "3. âœ… Een classifier getraind die ~88% accuracy haalt op Fashion-MNIST\n",
    "4. âœ… Begrepen waarom elke component werkt\n",
    "\n",
    "### Volgende stappen\n",
    "\n",
    "Met deze basis kun je:\n",
    "- Frameworks zoals PyTorch/TensorFlow begrijpen op een dieper niveau\n",
    "- Nieuwe architecturen implementeren\n",
    "- Papers lezen en begrijpen\n",
    "- Problemen debuggen met wiskundig inzicht\n",
    "\n",
    "### Afsluiting\n",
    "\n",
    "**Gefeliciteerd met het voltooien van Mathematical Foundations!** ðŸŽ‰\n",
    "\n",
    "Je hebt nu de wiskundige fundamenten om verder te gaan in AI en Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 12 van 12 | IT & Artificial Intelligence\n",
    "\n",
    "**Cursus Compleet!** ðŸŽ“\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
