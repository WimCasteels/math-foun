{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 3: Matrixoperaties en Lineaire Transformaties\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Recap en Motivatie\n",
    "\n",
    "In de vorige les hebben we gezien hoe één neuron werkt: het berekent het dot product van de input vector met zijn gewichten en telt daar een bias bij op. Dit is elegant voor één neuron, maar een echte netwerklaag bevat honderden of duizenden neuronen die allemaal tegelijk moeten worden berekend.\n",
    "\n",
    "Stel je voor dat we 128 neuronen hebben in een hidden layer, elk verbonden met 784 inputs. Als we elk neuron apart zouden berekenen met een for-loop, zou dit traag en inefficiënt zijn. De oplossing is matrixvermenigvuldiging: één enkele operatie die alle 128 neuronen tegelijk berekent.\n",
    "\n",
    "Dit is niet alleen eleganter, het is ook veel sneller. GPU's (Graphics Processing Units) zijn specifiek ontworpen om matrixoperaties massaal parallel uit te voeren. Dit is de reden waarom deep learning pas echt van de grond kwam toen onderzoekers ontdekten dat ze GPU's konden gebruiken voor neurale netwerken.\n",
    "\n",
    "In deze les leren we matrixvermenigvuldiging, hoe we een complete netwerklaag kunnen berekenen met één operatie, en hoe matrices geometrisch kunnen worden geïnterpreteerd als transformaties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Leerdoelen\n",
    "\n",
    "Na het doorwerken van deze les kun je matrixvermenigvuldiging uitvoeren en de dimensieregels toepassen. Je begrijpt hoe een volledige netwerklaag wordt berekend met één matrixoperatie. Je kunt lineaire transformaties geometrisch interpreteren, waaronder rotatie, schaling en projectie. Je kunt de transpose van een matrix berekenen en weet wanneer je deze nodig hebt. Tot slot herken je speciale matrices zoals de identiteitsmatrix en diagonaalmatrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importeer de benodigde libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Laad MNIST\n",
    "print(\"MNIST laden...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X_mnist, y_mnist = mnist.data, mnist.target.astype(int)\n",
    "print(f\"Geladen: {len(X_mnist)} afbeeldingen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Matrixvermenigvuldiging\n",
    "\n",
    "### De definitie\n",
    "\n",
    "Matrixvermenigvuldiging is een operatie die twee matrices combineert tot een nieuwe matrix. Anders dan element-wise vermenigvuldiging, waarbij je simpelweg overeenkomstige elementen vermenigvuldigt, combineert matrixvermenigvuldiging rijen van de eerste matrix met kolommen van de tweede.\n",
    "\n",
    "Als A een matrix is met m rijen en n kolommen, en B een matrix met n rijen en p kolommen, dan is het product C = A × B een matrix met m rijen en p kolommen. Elk element C[i,j] is het dot product van rij i van A met kolom j van B.\n",
    "\n",
    "Dit betekent dat het aantal kolommen van A gelijk moet zijn aan het aantal rijen van B. We noteren dit als: (m × n) × (n × p) = (m × p). De \"binnenste\" dimensies (beide n) moeten overeenkomen en verdwijnen in het resultaat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eenvoudig voorbeeld: 2x3 matrix maal 3x2 matrix\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])  # 2x3\n",
    "\n",
    "B = np.array([\n",
    "    [7, 8],\n",
    "    [9, 10],\n",
    "    [11, 12]\n",
    "])  # 3x2\n",
    "\n",
    "print(f\"Matrix A (shape {A.shape}):\")\n",
    "print(A)\n",
    "print()\n",
    "print(f\"Matrix B (shape {B.shape}):\")\n",
    "print(B)\n",
    "print()\n",
    "\n",
    "# Matrixvermenigvuldiging\n",
    "C = A @ B  # Of: np.matmul(A, B)\n",
    "\n",
    "print(f\"C = A @ B (shape {C.shape}):\")\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laten we C[0,0] stap voor stap berekenen\n",
    "print(\"Berekening van C[0,0]:\")\n",
    "print(f\"Rij 0 van A: {A[0]}\")\n",
    "print(f\"Kolom 0 van B: {B[:, 0]}\")\n",
    "print(f\"Dot product: {A[0,0]}×{B[0,0]} + {A[0,1]}×{B[1,0]} + {A[0,2]}×{B[2,0]}\")\n",
    "print(f\"           = {A[0,0]*B[0,0]} + {A[0,1]*B[1,0]} + {A[0,2]*B[2,0]}\")\n",
    "print(f\"           = {A[0] @ B[:, 0]}\")\n",
    "print()\n",
    "\n",
    "# En C[1,1]\n",
    "print(\"Berekening van C[1,1]:\")\n",
    "print(f\"Rij 1 van A: {A[1]}\")\n",
    "print(f\"Kolom 1 van B: {B[:, 1]}\")\n",
    "print(f\"Dot product: {A[1,0]}×{B[0,1]} + {A[1,1]}×{B[1,1]} + {A[1,2]}×{B[2,1]}\")\n",
    "print(f\"           = {A[1,0]*B[0,1]} + {A[1,1]*B[1,1]} + {A[1,2]*B[2,1]}\")\n",
    "print(f\"           = {A[1] @ B[:, 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisatie van matrixvermenigvuldiging\n",
    "fig, axes = plt.subplots(1, 5, figsize=(16, 3))\n",
    "\n",
    "# Matrix A\n",
    "axes[0].imshow(A, cmap='Blues', aspect='auto')\n",
    "axes[0].set_title(f'A\\n{A.shape[0]}×{A.shape[1]}')\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        axes[0].text(j, i, str(A[i,j]), ha='center', va='center', fontsize=14)\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "\n",
    "# ×\n",
    "axes[1].text(0.5, 0.5, '×', fontsize=30, ha='center', va='center')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Matrix B\n",
    "axes[2].imshow(B, cmap='Oranges', aspect='auto')\n",
    "axes[2].set_title(f'B\\n{B.shape[0]}×{B.shape[1]}')\n",
    "for i in range(B.shape[0]):\n",
    "    for j in range(B.shape[1]):\n",
    "        axes[2].text(j, i, str(B[i,j]), ha='center', va='center', fontsize=14)\n",
    "axes[2].set_xticks([])\n",
    "axes[2].set_yticks([])\n",
    "\n",
    "# =\n",
    "axes[3].text(0.5, 0.5, '=', fontsize=30, ha='center', va='center')\n",
    "axes[3].axis('off')\n",
    "\n",
    "# Matrix C\n",
    "axes[4].imshow(C, cmap='Greens', aspect='auto')\n",
    "axes[4].set_title(f'C = A×B\\n{C.shape[0]}×{C.shape[1]}')\n",
    "for i in range(C.shape[0]):\n",
    "    for j in range(C.shape[1]):\n",
    "        axes[4].text(j, i, str(C[i,j]), ha='center', va='center', fontsize=14)\n",
    "axes[4].set_xticks([])\n",
    "axes[4].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Dimensieregel: (2×3) × (3×2) = (2×2)\")\n",
    "print(\"De binnenste dimensies (beide 3) moeten gelijk zijn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belangrijke eigenschappen\n",
    "\n",
    "Matrixvermenigvuldiging heeft enkele belangrijke eigenschappen die je moet kennen. Ten eerste is het niet commutatief: in het algemeen geldt A × B ≠ B × A. Zelfs als beide producten gedefinieerd zijn, geven ze meestal verschillende resultaten. Dit is een belangrijk verschil met gewone vermenigvuldiging van getallen.\n",
    "\n",
    "Ten tweede is matrixvermenigvuldiging wel associatief: (A × B) × C = A × (B × C). Dit betekent dat bij het vermenigvuldigen van meerdere matrices de volgorde van berekening niet uitmaakt, zolang de volgorde van de matrices zelf niet verandert.\n",
    "\n",
    "Ten derde is het distributief over optelling: A × (B + C) = A × B + A × C. Dit is belangrijk voor het begrijpen van hoe neurale netwerken batch data verwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niet commutatief: A × B ≠ B × A\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "AB = A @ B\n",
    "BA = B @ A\n",
    "\n",
    "print(\"Matrixvermenigvuldiging is NIET commutatief:\\n\")\n",
    "print(\"A @ B =\")\n",
    "print(AB)\n",
    "print()\n",
    "print(\"B @ A =\")\n",
    "print(BA)\n",
    "print()\n",
    "print(f\"A @ B == B @ A? {np.allclose(AB, BA)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wel associatief: (A × B) × C = A × (B × C)\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "C = np.array([[9, 10], [11, 12]])\n",
    "\n",
    "links = (A @ B) @ C\n",
    "rechts = A @ (B @ C)\n",
    "\n",
    "print(\"Matrixvermenigvuldiging is WEL associatief:\\n\")\n",
    "print(\"(A @ B) @ C =\")\n",
    "print(links)\n",
    "print()\n",
    "print(\"A @ (B @ C) =\")\n",
    "print(rechts)\n",
    "print()\n",
    "print(f\"(A @ B) @ C == A @ (B @ C)? {np.allclose(links, rechts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-matrix vermenigvuldiging\n",
    "\n",
    "Een speciaal geval van matrixvermenigvuldiging is wanneer één van de operanden een vector is. Een vector kan worden gezien als een matrix met één rij (rijvector) of één kolom (kolomvector).\n",
    "\n",
    "Als x een rijvector is met shape (1, n) en W een matrix met shape (n, m), dan is x @ W een rijvector met shape (1, m). In de praktijk gebruiken we in NumPy vaak 1D arrays voor vectoren, en NumPy handelt de dimensies automatisch af."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector-matrix vermenigvuldiging\n",
    "x = np.array([1, 2, 3])  # Vector met 3 elementen\n",
    "W = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])  # 3×4 matrix\n",
    "\n",
    "print(f\"Vector x: {x} (shape {x.shape})\")\n",
    "print(f\"Matrix W (shape {W.shape}):\")\n",
    "print(W)\n",
    "print()\n",
    "\n",
    "result = x @ W\n",
    "print(f\"x @ W = {result} (shape {result.shape})\")\n",
    "print()\n",
    "print(\"Elke output is het dot product van x met een kolom van W:\")\n",
    "for j in range(W.shape[1]):\n",
    "    print(f\"  Output {j}: x · W[:,{j}] = {x} · {W[:,j]} = {x @ W[:,j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Een Netwerklaag als Matrixoperatie\n",
    "\n",
    "Nu komen we bij de kerntoepassing van matrixvermenigvuldiging in neurale netwerken. Een laag van een neuraal netwerk kan volledig worden beschreven als een matrixoperatie gevolgd door een bias-optelling.\n",
    "\n",
    "Stel we hebben een input vector x met 784 elementen (een MNIST afbeelding) en we willen deze door een laag sturen met 128 neuronen. Elk neuron heeft 784 gewichten (één voor elke input) plus een bias. In totaal hebben we dus 784 × 128 = 100.352 gewichten en 128 biases.\n",
    "\n",
    "We organiseren de gewichten in een matrix W met shape (784, 128), waarbij kolom j de gewichten bevat van neuron j. De biases vormen een vector b met shape (128,). De output van de laag is dan simpelweg: y = x @ W + b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Een enkele netwerklaag\n",
    "input_size = 784\n",
    "output_size = 128\n",
    "\n",
    "# Random gewichten en biases (in de praktijk worden deze getraind)\n",
    "np.random.seed(42)\n",
    "W = np.random.randn(input_size, output_size) * 0.01  # Kleine random waarden\n",
    "b = np.zeros(output_size)  # Biases starten vaak op 0\n",
    "\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Output size: {output_size}\")\n",
    "print(f\"Weights shape: {W.shape}\")\n",
    "print(f\"Biases shape: {b.shape}\")\n",
    "print(f\"Totaal aantal parameters: {W.size + b.size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass voor één afbeelding\n",
    "x = X_mnist[0]  # Eerste MNIST afbeelding\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# De volledige laag in één operatie!\n",
    "y = x @ W + b\n",
    "\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"\\nEerste 10 output waarden: {y[:10]}\")\n",
    "print(f\"Min: {y.min():.4f}, Max: {y.max():.4f}, Mean: {y.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergelijk met de \"langzame\" manier: neuron voor neuron\n",
    "y_slow = np.zeros(output_size)\n",
    "for j in range(output_size):\n",
    "    # Neuron j: dot product met kolom j van W, plus bias j\n",
    "    y_slow[j] = x @ W[:, j] + b[j]\n",
    "\n",
    "print(\"Vergelijking met neuron-voor-neuron berekening:\")\n",
    "print(f\"Resultaten identiek? {np.allclose(y, y_slow)}\")\n",
    "print()\n",
    "print(\"De matrixvermenigvuldiging doet exact hetzelfde,\")\n",
    "print(\"maar veel efficiënter (vooral op GPU's)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing\n",
    "\n",
    "Het echte voordeel van matrixoperaties komt naar voren bij batch processing. In plaats van één afbeelding tegelijk te verwerken, kunnen we een hele batch van bijvoorbeeld 32 afbeeldingen tegelijk door de laag sturen.\n",
    "\n",
    "Als X een matrix is met shape (32, 784), waarbij elke rij een afbeelding is, dan is X @ W een matrix met shape (32, 128), waarbij elke rij de output is voor de corresponderende afbeelding. De bias wordt via broadcasting bij elke rij opgeteld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing: meerdere afbeeldingen tegelijk\n",
    "batch_size = 32\n",
    "X_batch = X_mnist[:batch_size]  # Eerste 32 afbeeldingen\n",
    "\n",
    "print(f\"Batch input shape: {X_batch.shape}\")\n",
    "print(f\"  → {X_batch.shape[0]} afbeeldingen van {X_batch.shape[1]} pixels\")\n",
    "print()\n",
    "\n",
    "# Forward pass voor de hele batch in één keer!\n",
    "Y_batch = X_batch @ W + b\n",
    "\n",
    "print(f\"Batch output shape: {Y_batch.shape}\")\n",
    "print(f\"  → {Y_batch.shape[0]} outputs van {Y_batch.shape[1]} waarden\")\n",
    "print()\n",
    "print(\"Dimensieregel: (32×784) @ (784×128) + (128,) = (32×128)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer wat er gebeurt\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Input batch (toon subset voor visualisatie)\n",
    "axes[0].imshow(X_batch[:8, :100], cmap='gray', aspect='auto')\n",
    "axes[0].set_title(f'Input Batch\\n{X_batch.shape}\\n(eerste 8 rijen, 100 kolommen)')\n",
    "axes[0].set_xlabel('Pixel index')\n",
    "axes[0].set_ylabel('Afbeelding')\n",
    "\n",
    "# Gewichten (toon subset)\n",
    "axes[1].imshow(W[:100, :32], cmap='RdBu', aspect='auto')\n",
    "axes[1].set_title(f'Weights\\n{W.shape}\\n(eerste 100 rijen, 32 kolommen)')\n",
    "axes[1].set_xlabel('Neuron')\n",
    "axes[1].set_ylabel('Input')\n",
    "\n",
    "# Output batch\n",
    "axes[2].imshow(Y_batch[:8, :32], cmap='viridis', aspect='auto')\n",
    "axes[2].set_title(f'Output Batch\\n{Y_batch.shape}\\n(eerste 8 rijen, 32 kolommen)')\n",
    "axes[2].set_xlabel('Neuron')\n",
    "axes[2].set_ylabel('Afbeelding')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Een Layer class implementeren\n",
    "\n",
    "Laten we dit formaliseren in een Python class die we later kunnen hergebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    \"\"\"Een lineaire laag van een neuraal netwerk.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"Initialiseer de laag met random gewichten.\"\"\"\n",
    "        # Xavier/Glorot initialisatie voor betere training\n",
    "        scale = np.sqrt(2.0 / input_size)\n",
    "        self.W = np.random.randn(input_size, output_size) * scale\n",
    "        self.b = np.zeros(output_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Bereken de output voor input X.\"\"\"\n",
    "        return X @ self.W + self.b\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"LinearLayer({self.W.shape[0]} → {self.W.shape[1]})\"\n",
    "\n",
    "\n",
    "# Test de class\n",
    "layer = LinearLayer(784, 128)\n",
    "print(layer)\n",
    "print(f\"Parameters: {layer.W.size + layer.b.size:,}\")\n",
    "print()\n",
    "\n",
    "# Forward pass\n",
    "output = layer.forward(X_mnist[:10])\n",
    "print(f\"Input shape: (10, 784)\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Lineaire Transformaties\n",
    "\n",
    "Matrixvermenigvuldiging doet meer dan alleen getallen combineren. Geometrisch gezien transformeert een matrix vectoren van de ene ruimte naar de andere. Dit concept heet een lineaire transformatie.\n",
    "\n",
    "Een lineaire transformatie heeft twee belangrijke eigenschappen. Ten eerste behoudt het de oorsprong: als je de nulvector transformeert, krijg je weer de nulvector. Ten tweede behoudt het lineaire combinaties: als je twee vectoren optelt en dan transformeert, krijg je hetzelfde resultaat als wanneer je eerst transformeert en dan optelt.\n",
    "\n",
    "We kunnen lineaire transformaties het beste begrijpen door te kijken naar 2D-voorbeelden, die we gemakkelijk kunnen visualiseren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functie voor het visualiseren van 2D transformaties\n",
    "def plot_transformation(matrix, title, ax):\n",
    "    \"\"\"Visualiseer het effect van een matrix op een vierkant en grid.\"\"\"\n",
    "    # Maak een vierkant\n",
    "    square = np.array([\n",
    "        [0, 0], [1, 0], [1, 1], [0, 1], [0, 0]\n",
    "    ]).T\n",
    "    \n",
    "    # Maak een grid van punten\n",
    "    grid_x, grid_y = np.meshgrid(np.linspace(-0.2, 1.2, 8), np.linspace(-0.2, 1.2, 8))\n",
    "    grid_points = np.vstack([grid_x.ravel(), grid_y.ravel()])\n",
    "    \n",
    "    # Transformeer\n",
    "    square_transformed = matrix @ square\n",
    "    grid_transformed = matrix @ grid_points\n",
    "    \n",
    "    # Plot origineel (licht)\n",
    "    ax.plot(square[0], square[1], 'b-', alpha=0.3, linewidth=2)\n",
    "    ax.scatter(grid_points[0], grid_points[1], c='blue', alpha=0.2, s=20)\n",
    "    \n",
    "    # Plot getransformeerd (donker)\n",
    "    ax.plot(square_transformed[0], square_transformed[1], 'r-', linewidth=2)\n",
    "    ax.scatter(grid_transformed[0], grid_transformed[1], c='red', alpha=0.6, s=20)\n",
    "    \n",
    "    ax.set_xlim(-1.5, 2.5)\n",
    "    ax.set_ylim(-1.5, 2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.set_title(f'{title}\\n{matrix[0]}\\n{matrix[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verschillende 2D transformaties\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "\n",
    "# Identiteit (geen verandering)\n",
    "I = np.array([[1, 0], [0, 1]])\n",
    "plot_transformation(I, 'Identiteit', axes[0, 0])\n",
    "\n",
    "# Schaling (uniform)\n",
    "S = np.array([[1.5, 0], [0, 1.5]])\n",
    "plot_transformation(S, 'Schaling (1.5×)', axes[0, 1])\n",
    "\n",
    "# Schaling (niet-uniform)\n",
    "S2 = np.array([[2, 0], [0, 0.5]])\n",
    "plot_transformation(S2, 'Schaling (x:2, y:0.5)', axes[0, 2])\n",
    "\n",
    "# Rotatie (45 graden)\n",
    "theta = np.pi / 4  # 45 graden\n",
    "R = np.array([[np.cos(theta), -np.sin(theta)], \n",
    "              [np.sin(theta), np.cos(theta)]])\n",
    "plot_transformation(R, 'Rotatie (45°)', axes[1, 0])\n",
    "\n",
    "# Shear (horizontaal)\n",
    "Sh = np.array([[1, 0.5], [0, 1]])\n",
    "plot_transformation(Sh, 'Shear (horizontaal)', axes[1, 1])\n",
    "\n",
    "# Reflectie (over x-as)\n",
    "Ref = np.array([[1, 0], [0, -1]])\n",
    "plot_transformation(Ref, 'Reflectie (x-as)', axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Blauw/licht = origineel, Rood/donker = getransformeerd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samenstelling van transformaties\n",
    "\n",
    "Een van de krachtigste aspecten van lineaire transformaties is dat ze kunnen worden samengesteld door matrixvermenigvuldiging. Als je eerst transformatie A toepast en dan transformatie B, is het resultaat hetzelfde als de transformatie B × A toepassen.\n",
    "\n",
    "Let op de volgorde: de matrix die het laatst wordt toegepast staat vooraan in het product. Dit komt omdat we vectoren van rechts vermenigvuldigen: (B × A) × v = B × (A × v).\n",
    "\n",
    "Dit is precies wat er gebeurt in een neuraal netwerk: elke laag past een transformatie toe, en het totale effect is de samenstelling van al deze transformaties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samenstelling: eerst roteren, dan schalen\n",
    "theta = np.pi / 6  # 30 graden\n",
    "R = np.array([[np.cos(theta), -np.sin(theta)], \n",
    "              [np.sin(theta), np.cos(theta)]])\n",
    "S = np.array([[1.5, 0], [0, 1.5]])\n",
    "\n",
    "# Samengestelde transformatie: S × R (eerst R, dan S)\n",
    "combined = S @ R\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Origineel\n",
    "plot_transformation(np.eye(2), 'Origineel', axes[0])\n",
    "\n",
    "# Na rotatie\n",
    "plot_transformation(R, 'Na rotatie (30°)', axes[1])\n",
    "\n",
    "# Na rotatie + schaling (stapsgewijs)\n",
    "plot_transformation(combined, 'Na rotatie + schaling\\n(stapsgewijs)', axes[2])\n",
    "\n",
    "# Samengestelde matrix in één keer\n",
    "plot_transformation(combined, 'Samengestelde matrix\\nS @ R', axes[3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Rotatiematrix R:\\n{R.round(3)}\")\n",
    "print(f\"\\nSchalingsmatrix S:\\n{S}\")\n",
    "print(f\"\\nSamengesteld S @ R:\\n{combined.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link naar neurale netwerken\n",
    "\n",
    "In een neuraal netwerk is elke laag in essentie een lineaire transformatie (de matrixvermenigvuldiging met gewichten) gevolgd door een niet-lineaire activatiefunctie. De lineaire transformatie kan de data roteren, schalen, projecteren en op allerlei manieren vervormen.\n",
    "\n",
    "Het is de combinatie van lineaire transformaties met niet-lineaire activatiefuncties die neurale netwerken zo krachtig maakt. Zonder de niet-lineariteit zou een netwerk met meerdere lagen wiskundig equivalent zijn aan één enkele lineaire transformatie, omdat de samenstelling van lineaire functies weer lineair is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Speciale Matrices\n",
    "\n",
    "Sommige matrices hebben speciale eigenschappen die ze bijzonder nuttig maken. Het is belangrijk om deze te herkennen.\n",
    "\n",
    "### De identiteitsmatrix\n",
    "\n",
    "De identiteitsmatrix I is een vierkante matrix met enen op de diagonaal en nullen elders. Het is het equivalent van het getal 1 voor matrices: I × A = A × I = A. De identiteitsmatrix verandert niets aan een vector of matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identiteitsmatrix\n",
    "I = np.eye(3)  # 3×3 identiteitsmatrix\n",
    "print(\"Identiteitsmatrix I:\")\n",
    "print(I)\n",
    "print()\n",
    "\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "print(\"I @ A:\")\n",
    "print(I @ A)\n",
    "print()\n",
    "\n",
    "print(\"A @ I:\")\n",
    "print(A @ I)\n",
    "print()\n",
    "\n",
    "print(f\"I @ A == A? {np.allclose(I @ A, A)}\")\n",
    "print(f\"A @ I == A? {np.allclose(A @ I, A)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonaalmatrices\n",
    "\n",
    "Een diagonaalmatrix heeft alleen waarden op de hoofddiagonaal, de rest is nul. Als transformatie schaalt een diagonaalmatrix elke as onafhankelijk. De identiteitsmatrix is een speciaal geval van een diagonaalmatrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagonaalmatrix maken\n",
    "d = np.array([2, 3, 0.5])\n",
    "D = np.diag(d)\n",
    "print(\"Diagonaalmatrix D:\")\n",
    "print(D)\n",
    "print()\n",
    "\n",
    "# Effect op een vector\n",
    "v = np.array([1, 1, 1])\n",
    "print(f\"Vector v: {v}\")\n",
    "print(f\"D @ v: {D @ v}\")\n",
    "print()\n",
    "print(\"Elke component wordt geschaald met de corresponderende diagonaalwaarde.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De transpose\n",
    "\n",
    "De transpose van een matrix A, genoteerd als Aᵀ, is de matrix die je krijgt door rijen en kolommen te verwisselen. Element A[i,j] wordt Aᵀ[j,i].\n",
    "\n",
    "De transpose heeft enkele belangrijke eigenschappen. Ten eerste geldt (Aᵀ)ᵀ = A, de transpose van de transpose is de originele matrix. Ten tweede geldt (A × B)ᵀ = Bᵀ × Aᵀ, let op de omgekeerde volgorde. De transpose is cruciaal bij backpropagation in neurale netwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])  # 2×3\n",
    "\n",
    "print(f\"Matrix A (shape {A.shape}):\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "print(f\"Transpose Aᵀ (shape {A.T.shape}):\")\n",
    "print(A.T)\n",
    "print()\n",
    "\n",
    "print(\"Rijen worden kolommen en vice versa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belangrijke eigenschap: (A × B)ᵀ = Bᵀ × Aᵀ\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])  # 3×2\n",
    "B = np.array([[7, 8, 9], [10, 11, 12]])  # 2×3\n",
    "\n",
    "AB = A @ B  # 3×3\n",
    "AB_T = AB.T\n",
    "\n",
    "BT_AT = B.T @ A.T\n",
    "\n",
    "print(\"(A @ B)ᵀ:\")\n",
    "print(AB_T)\n",
    "print()\n",
    "\n",
    "print(\"Bᵀ @ Aᵀ:\")\n",
    "print(BT_AT)\n",
    "print()\n",
    "\n",
    "print(f\"Gelijk? {np.allclose(AB_T, BT_AT)}\")\n",
    "print()\n",
    "print(\"Let op de omgekeerde volgorde! Dit is cruciaal bij backpropagation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetrische matrices\n",
    "\n",
    "Een matrix A is symmetrisch als A = Aᵀ. Dit betekent dat de matrix gespiegeld is over de hoofddiagonaal. Symmetrische matrices komen veel voor in statistiek en machine learning, bijvoorbeeld covariantiematrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetrische matrix\n",
    "A_sym = np.array([\n",
    "    [1, 2, 3],\n",
    "    [2, 4, 5],\n",
    "    [3, 5, 6]\n",
    "])\n",
    "\n",
    "print(\"Symmetrische matrix A:\")\n",
    "print(A_sym)\n",
    "print()\n",
    "\n",
    "print(\"Transpose Aᵀ:\")\n",
    "print(A_sym.T)\n",
    "print()\n",
    "\n",
    "print(f\"A == Aᵀ? {np.allclose(A_sym, A_sym.T)}\")\n",
    "print()\n",
    "\n",
    "# Voorbeeld: covariantiematrix is altijd symmetrisch\n",
    "data = np.random.randn(100, 3)  # 100 samples, 3 features\n",
    "cov_matrix = np.cov(data.T)\n",
    "print(\"Covariantiematrix (altijd symmetrisch):\")\n",
    "print(cov_matrix.round(3))\n",
    "print(f\"Symmetrisch? {np.allclose(cov_matrix, cov_matrix.T)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Toepassing: Forward Pass van een Mini-Netwerk\n",
    "\n",
    "Laten we alles samenvoegen en een complete forward pass implementeren voor een klein neuraal netwerk. We bouwen een netwerk met drie lagen: Input (784) → Hidden (128) → Output (10).\n",
    "\n",
    "Dit netwerk zou in principe MNIST cijfers kunnen classificeren, al zal het met random gewichten alleen maar willekeurige outputs geven. Het punt is om te zien hoe de matrixoperaties samenwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"ReLU activatiefunctie: max(0, x)\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax: converteert naar kansverdeling.\"\"\"\n",
    "    # Numeriek stabiele versie\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "class MiniNetwork:\n",
    "    \"\"\"Een simpel 2-laags netwerk voor MNIST.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Laag 1: 784 → 128\n",
    "        self.W1 = np.random.randn(784, 128) * np.sqrt(2.0 / 784)\n",
    "        self.b1 = np.zeros(128)\n",
    "        \n",
    "        # Laag 2: 128 → 10\n",
    "        self.W2 = np.random.randn(128, 10) * np.sqrt(2.0 / 128)\n",
    "        self.b2 = np.zeros(10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass door het netwerk.\"\"\"\n",
    "        # Laag 1: linear + ReLU\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        \n",
    "        # Laag 2: linear + softmax\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.output = softmax(self.z2)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Geef de voorspelde klasse.\"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=-1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Tel het totaal aantal parameters.\"\"\"\n",
    "        return self.W1.size + self.b1.size + self.W2.size + self.b2.size\n",
    "\n",
    "\n",
    "# Maak het netwerk\n",
    "np.random.seed(42)\n",
    "network = MiniNetwork()\n",
    "\n",
    "print(\"Mini-netwerk architectuur:\")\n",
    "print(f\"  Input:  784 (28×28 pixels)\")\n",
    "print(f\"  Hidden: 128 (met ReLU)\")\n",
    "print(f\"  Output: 10  (met softmax)\")\n",
    "print()\n",
    "print(f\"Totaal parameters: {network.count_parameters():,}\")\n",
    "print(f\"  W1: {network.W1.shape} = {network.W1.size:,}\")\n",
    "print(f\"  b1: {network.b1.shape} = {network.b1.size}\")\n",
    "print(f\"  W2: {network.W2.shape} = {network.W2.size:,}\")\n",
    "print(f\"  b2: {network.b2.shape} = {network.b2.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass voor één afbeelding\n",
    "x = X_mnist[0:1]  # Shape (1, 784) - batch van 1\n",
    "true_label = y_mnist[0]\n",
    "\n",
    "output = network.forward(x)\n",
    "prediction = network.predict(x)[0]\n",
    "\n",
    "print(f\"Echte label: {true_label}\")\n",
    "print(f\"Voorspelling: {prediction}\")\n",
    "print()\n",
    "print(\"Output kansen per klasse:\")\n",
    "for i in range(10):\n",
    "    bar = \"█\" * int(output[0, i] * 50)\n",
    "    print(f\"  {i}: {output[0, i]:.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer de forward pass voor meerdere afbeeldingen\n",
    "n_samples = 5\n",
    "X_sample = X_mnist[:n_samples]\n",
    "y_sample = y_mnist[:n_samples]\n",
    "\n",
    "outputs = network.forward(X_sample)\n",
    "predictions = network.predict(X_sample)\n",
    "\n",
    "fig, axes = plt.subplots(2, n_samples, figsize=(15, 6))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Afbeelding\n",
    "    axes[0, i].imshow(X_sample[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title(f'Label: {y_sample[i]}\\nPred: {predictions[i]}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Output kansen\n",
    "    axes[1, i].bar(range(10), outputs[i])\n",
    "    axes[1, i].set_xticks(range(10))\n",
    "    axes[1, i].set_ylim(0, 0.5)\n",
    "    axes[1, i].set_xlabel('Cijfer')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Kans')\n",
    "\n",
    "plt.suptitle('Forward pass met random gewichten (ongetraind netwerk)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Met random gewichten zijn de voorspellingen willekeurig.\")\n",
    "print(\"Het netwerk moet nog getraind worden om te leren!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laten we de shapes door het netwerk volgen\n",
    "X_batch = X_mnist[:32]  # Batch van 32\n",
    "\n",
    "print(\"Data shapes tijdens forward pass:\")\n",
    "print(f\"\\nInput X:        {X_batch.shape}\")\n",
    "print(f\"                ↓\")\n",
    "print(f\"W1:             {network.W1.shape}\")\n",
    "print(f\"X @ W1:         {(X_batch @ network.W1).shape}\")\n",
    "print(f\"+ b1:           {network.b1.shape} (broadcast)\")\n",
    "print(f\"z1:             {(X_batch @ network.W1 + network.b1).shape}\")\n",
    "print(f\"a1 = ReLU(z1):  {relu(X_batch @ network.W1 + network.b1).shape}\")\n",
    "print(f\"                ↓\")\n",
    "a1 = relu(X_batch @ network.W1 + network.b1)\n",
    "print(f\"W2:             {network.W2.shape}\")\n",
    "print(f\"a1 @ W2:        {(a1 @ network.W2).shape}\")\n",
    "print(f\"+ b2:           {network.b2.shape} (broadcast)\")\n",
    "print(f\"z2:             {(a1 @ network.W2 + network.b2).shape}\")\n",
    "z2 = a1 @ network.W2 + network.b2\n",
    "print(f\"softmax(z2):    {softmax(z2).shape}\")\n",
    "print(f\"                ↓\")\n",
    "print(f\"Output:         (32, 10) - 32 kansverdelingen over 10 klassen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Samenvatting en Vooruitblik\n",
    "\n",
    "### Kernconcepten\n",
    "\n",
    "In deze les hebben we geleerd hoe matrixvermenigvuldiging werkt en waarom het zo fundamenteel is voor neurale netwerken. De kernformule (m × n) × (n × p) = (m × p) bepaalt wanneer matrices kunnen worden vermenigvuldigd en wat de resulterende dimensies zijn.\n",
    "\n",
    "We hebben gezien dat een complete netwerklaag kan worden berekend met één matrixoperatie: y = X @ W + b. Dit is niet alleen elegant maar ook efficiënt, omdat GPU's geoptimaliseerd zijn voor matrixoperaties.\n",
    "\n",
    "Geometrisch hebben we gezien dat matrixvermenigvuldiging lineaire transformaties uitvoert: rotaties, schalingen, reflecties en meer. Een neuraal netwerk is een reeks van zulke transformaties, afgewisseld met niet-lineaire activatiefuncties.\n",
    "\n",
    "We hebben speciale matrices leren kennen: de identiteitsmatrix die niets verandert, diagonaalmatrices die onafhankelijk schalen, en de transpose die rijen en kolommen verwisselt.\n",
    "\n",
    "### Link naar het neurale netwerk\n",
    "\n",
    "We hebben nu een volledig werkende forward pass voor een mini-MNIST-netwerk. De data stroomt als volgt: Input (784) → Hidden (128) via W1 → ReLU → Output (10) via W2 → Softmax. Met random gewichten zijn de outputs willekeurig, maar de structuur is er.\n",
    "\n",
    "### Volgende les\n",
    "\n",
    "In les 4 leren we over de inverse van een matrix en stelsels van vergelijkingen oplossen. Dit is het laatste onderdeel van lineaire algebra voordat we beginnen aan calculus, waar we leren hoe een netwerk daadwerkelijk leert door gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist\n",
    "\n",
    "Voordat je verdergaat, controleer of je het volgende begrijpt:\n",
    "\n",
    "1. Hoe bereken je het product van twee matrices?\n",
    "\n",
    "2. Wat is de dimensieregel voor matrixvermenigvuldiging?\n",
    "\n",
    "3. Waarom is A × B niet gelijk aan B × A?\n",
    "\n",
    "4. Hoe wordt een netwerklaag berekend met matrixvermenigvuldiging?\n",
    "\n",
    "5. Wat doet de transpose operatie?\n",
    "\n",
    "Als je deze vragen kunt beantwoorden, ben je klaar voor les 4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 3 van 12 | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
