{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 2: Vectoren en Matrices - Data als Wiskundige Objecten\n",
    "\n",
    "**Mathematical Foundations - IT & Artificial Intelligence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Recap en Overzicht\n",
    "\n",
    "In de vorige les hebben we gezien dat een neuraal netwerk gebouwd is op drie wiskundige pijlers: lineaire algebra, calculus en statistiek. Vandaag starten we met de eerste pijler, lineaire algebra, die ons de taal geeft om data te representeren en te manipuleren.\n",
    "\n",
    "Lineaire algebra is overal in Machine Learning. Wanneer je een afbeelding in een neuraal netwerk invoert, wordt die afbeelding voorgesteld als een verzameling getallen. Wanneer het netwerk berekeningen uitvoert, gebeurt dit via matrixoperaties. Wanneer we de gewichten van het netwerk opslaan, zijn dit matrices en vectoren.\n",
    "\n",
    "In deze les leren we de fundamentele bouwstenen: scalars, vectoren, matrices en tensors. We zien hoe verschillende soorten data, van spreadsheets tot afbeeldingen en video, worden omgezet naar deze wiskundige structuren. Tot slot leren we essentiële operaties zoals het dot product, dat de kern vormt van berekeningen in neurale netwerken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Leerdoelen van deze les\n",
    "\n",
    "Na het doorwerken van deze les ben je in staat om de begrippen scalar, vector, matrix en tensor te definiëren en van elkaar te onderscheiden. Je kan vectoren optellen, aftrekken en vermenigvuldigen met een scalar. Je kan de norm van een vector berekenen en begrijpt wat deze geometrisch betekent. Je kan het dot product berekenen en de geometrische betekenis ervan uitleggen. Je kan verschillende soorten data representeren als tensoren in NumPy. Tot slot kan je een MNIST-afbeelding transformeren tussen vector- en matrixvorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importeer de benodigde libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Stel enkele opties in voor mooiere output\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(\"Libraries geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Van Getallen naar Datastructuren\n",
    "\n",
    "### Scalars: het enkelvoudige getal\n",
    "\n",
    "Een scalar is het eenvoudigste wiskundige object: één enkel getal. Dit kan een geheel getal zijn, een kommagetal, of zelfs een complex getal. In de context van Machine Learning zijn scalars vaak meetwaarden zoals temperatuur, prijs, of de waarde van één enkele pixel.\n",
    "\n",
    "Hoewel een scalar simpel lijkt, speelt het een belangrijke rol in Machine Learning. De learning rate in gradient descent is een scalar die bepaalt hoe groot de stappen zijn die we nemen tijdens het optimaliseren. De output van een loss functie is een scalar die samenvat hoe goed of slecht het model presteert. Wanneer we zeggen dat een model een accuracy van 0.95 heeft, is dat een scalar.\n",
    "\n",
    "In wiskundige notatie gebruiken we vaak kleine letters voor scalars, zoals $a$, $b$, of $\\alpha$ (alpha). In Python is elk gewoon getal een scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalars in Python\n",
    "temperatuur = 21.5\n",
    "aantal_pixels = 784\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(f\"Temperatuur: {temperatuur} (type: {type(temperatuur).__name__})\")\n",
    "print(f\"Aantal pixels: {aantal_pixels} (type: {type(aantal_pixels).__name__})\")\n",
    "print(f\"Learning rate: {learning_rate} (type: {type(learning_rate).__name__})\")\n",
    "\n",
    "# In NumPy kunnen we expliciet een scalar maken met een specifiek datatype\n",
    "learning_rate_np = np.float64(0.01)\n",
    "print(f\"\\nNumPy scalar: {learning_rate_np} (type: {type(learning_rate_np).__name__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectoren: geordende lijsten van getallen\n",
    "\n",
    "Een vector is een geordende verzameling van getallen. De volgorde is belangrijk: de vector [1, 2, 3] is niet hetzelfde als [3, 2, 1]. Elk getal in de vector noemen we een element of component, en we kunnen elk element aanspreken via zijn index.\n",
    "\n",
    "Vectoren zijn overal in Machine Learning. Een MNIST-afbeelding van 784 pixels wordt voorgesteld als een vector met 784 elementen. De gewichten die één neuron verbinden met alle inputs vormen een vector. De output van een classificatienetwerk, met een kans per klasse, is een vector van 10 elementen.\n",
    "\n",
    "We kunnen vectoren op twee manieren visualiseren. Algebraïsch is een vector gewoon een lijst van getallen, bijvoorbeeld $\\mathbf{v} = [3, 1, 4]$. Geometrisch is een vector een pijl in een ruimte, met een richting en een lengte. Een 2D-vector kunnen we tekenen als een pijl in het platte vlak, een 3D-vector als een pijl in de ruimte. Deze dubbele interpretatie maakt vectoren zo krachtig.\n",
    "\n",
    "De dimensie van een vector is het aantal elementen dat hij bevat. Een vector met 784 elementen is een 784-dimensionale vector. In Machine Learning werken we routinematig met vectoren van honderden of duizenden dimensies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Een vector als NumPy array\n",
    "v = np.array([3, 1, 4, 1, 5])\n",
    "\n",
    "print(f\"Vector v: {v}\")\n",
    "print(f\"Aantal elementen (dimensie): {len(v)}\")\n",
    "print(f\"Shape: {v.shape}\")\n",
    "print()\n",
    "print(f\"Eerste element (index 0): {v[0]}\")\n",
    "print(f\"Derde element (index 2): {v[2]}\")\n",
    "print(f\"Laatste element (index -1): {v[-1]}\")\n",
    "print(f\"Elementen 1 tot 3: {v[1:4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometrische visualisatie van een 2D-vector\n",
    "v2d = np.array([3, 2])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.quiver(0, 0, v2d[0], v2d[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.02)\n",
    "plt.xlim(-1, 5)\n",
    "plt.ylim(-1, 4)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title(f'Vector v = [{v2d[0]}, {v2d[1]}] als pijl in 2D', fontsize=14)\n",
    "plt.text(v2d[0] + 0.1, v2d[1] + 0.1, f'v = ({v2d[0]}, {v2d[1]})', fontsize=12)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"De vector wijst van de oorsprong (0,0) naar het punt (3,2).\")\n",
    "print(\"De richting en lengte van de pijl zijn de essentiële eigenschappen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices: tabellen van getallen\n",
    "\n",
    "Een matrix is een rechthoekige tabel van getallen, georganiseerd in rijen en kolommen. We beschrijven de grootte van een matrix als \"m bij n\", waarbij m het aantal rijen is en n het aantal kolommen. Een matrix met 3 rijen en 4 kolommen noemen we een 3×4 matrix.\n",
    "\n",
    "In Machine Learning zijn matrices alomtegenwoordig. Een grijswaarden afbeelding is een matrix van pixelwaarden. De gewichten tussen twee lagen van een neuraal netwerk vormen een matrix. Een batch van trainingsvoorbeelden wordt vaak voorgesteld als een matrix waarbij elke rij één voorbeeld is.\n",
    "\n",
    "De relatie tussen vectoren en matrices is belangrijk om te begrijpen. Een vector kan worden gezien als een speciale matrix. Een kolomvector is een matrix met n rijen en 1 kolom. Een rijvector is een matrix met 1 rij en n kolommen. In NumPy maken we dit onderscheid meestal niet expliciet, maar het is wiskundig relevant.\n",
    "\n",
    "Om een element in een matrix aan te spreken, gebruiken we twee indices: de rij-index en de kolom-index. Het element op rij i en kolom j van matrix M noteren we als $M_{ij}$. In NumPy schrijven we dit als `M[i, j]}`, waarbij de indices beginnen bij 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Een 3x4 matrix\n",
    "M = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])\n",
    "\n",
    "print(\"Matrix M:\")\n",
    "print(M)\n",
    "print()\n",
    "print(f\"Shape: {M.shape}\")\n",
    "print(f\"Dit betekent: {M.shape[0]} rijen en {M.shape[1]} kolommen\")\n",
    "print()\n",
    "print(f\"Element op rij 0, kolom 0: {M[0, 0]}\")\n",
    "print(f\"Element op rij 1, kolom 2: {M[1, 2]}\")\n",
    "print(f\"Element op rij 2, kolom 3: {M[2, 3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rijen en kolommen selecteren\n",
    "print(\"Matrix M:\")\n",
    "print(M)\n",
    "print()\n",
    "\n",
    "print(f\"Eerste rij (index 0): {M[0]}\")\n",
    "print(f\"Tweede rij (index 1): {M[1]}\")\n",
    "print()\n",
    "\n",
    "print(f\"Eerste kolom (index 0): {M[:, 0]}\")\n",
    "print(f\"Derde kolom (index 2): {M[:, 2]}\")\n",
    "print()\n",
    "\n",
    "# Submatrix selecteren\n",
    "print(\"Submatrix (rijen 0-1, kolommen 1-2):\")\n",
    "print(M[0:2, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors: de generalisatie naar hogere dimensies\n",
    "\n",
    "Een tensor is de generalisatie van scalars, vectoren en matrices naar willekeurig veel dimensies. We kunnen dit systematisch bekijken: een scalar is een 0-dimensionale tensor (een enkel getal), een vector is een 1-dimensionale tensor (een rij getallen), een matrix is een 2-dimensionale tensor (een tabel van getallen). Een 3-dimensionale tensor kun je visualiseren als een \"kubus\" of \"stapel\" van matrices.\n",
    "\n",
    "In deep learning werken we vaak met tensors van hogere dimensies. Een kleurenafbeelding heeft drie dimensies: hoogte, breedte en kleurkanalen. Een batch van kleurenafbeeldingen heeft vier dimensies: batchgrootte, hoogte, breedte en kanalen. Een video voegt daar nog een tijdsdimensie aan toe.\n",
    "\n",
    "De naam \"TensorFlow\", een van de populairste deep learning frameworks, verwijst rechtstreeks naar deze datastructuur. De \"flow\" in TensorFlow beschrijft hoe tensors door het netwerk stromen en getransformeerd worden bij elke laag. PyTorch, een ander populair framework, heeft zelfs zijn basisdatatype \"torch.Tensor\" genoemd.\n",
    "\n",
    "Het is belangrijk om te begrijpen dat de term \"tensor\" in Machine Learning vaak iets losser wordt gebruikt dan in de wiskunde. In ML bedoelen we meestal gewoon een multidimensionale array van getallen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overzicht van tensor dimensies\n",
    "scalar = np.array(42)\n",
    "vector = np.array([1, 2, 3, 4])\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "print(\"Overzicht van tensor dimensies:\\n\")\n",
    "print(f\"Scalar (0D tensor): {scalar}\")\n",
    "print(f\"  Shape: {scalar.shape}, Dimensies: {scalar.ndim}\")\n",
    "print()\n",
    "print(f\"Vector (1D tensor): {vector}\")\n",
    "print(f\"  Shape: {vector.shape}, Dimensies: {vector.ndim}\")\n",
    "print()\n",
    "print(f\"Matrix (2D tensor):\")\n",
    "print(matrix)\n",
    "print(f\"  Shape: {matrix.shape}, Dimensies: {matrix.ndim}\")\n",
    "print()\n",
    "print(f\"3D Tensor:\")\n",
    "print(tensor_3d)\n",
    "print(f\"  Shape: {tensor_3d.shape}, Dimensies: {tensor_3d.ndim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Een 3D tensor visualiseren als \"stapel\" van matrices\n",
    "tensor_3d = np.array([\n",
    "    [[1, 2, 3, 4],\n",
    "     [5, 6, 7, 8],\n",
    "     [9, 10, 11, 12]],\n",
    "    \n",
    "    [[13, 14, 15, 16],\n",
    "     [17, 18, 19, 20],\n",
    "     [21, 22, 23, 24]]\n",
    "])\n",
    "\n",
    "print(f\"Shape van 3D tensor: {tensor_3d.shape}\")\n",
    "print(f\"Dit is een tensor met {tensor_3d.shape[0]} 'pagina's',\")\n",
    "print(f\"elk bestaande uit {tensor_3d.shape[1]} rijen en {tensor_3d.shape[2]} kolommen.\")\n",
    "print()\n",
    "\n",
    "print(\"Eerste 'pagina' (index 0):\")\n",
    "print(tensor_3d[0])\n",
    "print()\n",
    "\n",
    "print(\"Tweede 'pagina' (index 1):\")\n",
    "print(tensor_3d[1])\n",
    "print()\n",
    "\n",
    "print(f\"Element op pagina 1, rij 2, kolom 3: {tensor_3d[1, 2, 3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data als Tensoren: Van de Echte Wereld naar Getallen\n",
    "\n",
    "Voordat een Machine Learning model data kan verwerken, moet die data worden omgezet naar tensoren. Dit proces heet \"encoding\" of \"representatie\". Elk type data heeft zijn eigen manier om te worden voorgesteld als een tensor. In deze sectie bekijken we hoe verschillende veelvoorkomende datatypes worden omgezet.\n",
    "\n",
    "### Gestructureerde data: tabellen en spreadsheets\n",
    "\n",
    "Gestructureerde data is data die netjes georganiseerd is in rijen en kolommen, zoals je zou zien in een Excel-spreadsheet of een database. Elke rij is een observatie of voorbeeld, elke kolom is een kenmerk of feature.\n",
    "\n",
    "Dit type data wordt rechtstreeks voorgesteld als een 2D-matrix. Als je dataset 1000 huizen bevat met elk 5 kenmerken (oppervlakte, aantal kamers, bouwjaar, afstand tot centrum, prijs), dan krijg je een matrix van shape (1000, 5). Het eerste getal is altijd het aantal voorbeelden, het tweede is het aantal kenmerken.\n",
    "\n",
    "Een belangrijke uitdaging bij gestructureerde data is dat niet alle kolommen numeriek zijn. Categorische variabelen zoals \"type woning\" (appartement, rijhuis, vrijstaand) moeten worden omgezet naar getallen. Een veelgebruikte techniek hiervoor is one-hot encoding, waarbij elke categorie een aparte binaire kolom wordt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Voorbeeld: huizendata\n",
    "data = {\n",
    "    'oppervlakte_m2': [120, 85, 200, 95, 150],\n",
    "    'aantal_kamers': [4, 2, 5, 3, 4],\n",
    "    'bouwjaar': [1990, 2005, 1975, 2018, 2000],\n",
    "    'afstand_centrum_km': [5.2, 1.3, 12.0, 3.5, 7.8],\n",
    "    'prijs_euro': [320000, 195000, 425000, 285000, 375000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Oorspronkelijke data als DataFrame:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Converteer naar NumPy matrix (tensor)\n",
    "X = df.values\n",
    "print(f\"Als matrix (tensor): shape {X.shape}\")\n",
    "print(X)\n",
    "print()\n",
    "print(\"Elke rij is één huis, elke kolom is één kenmerk.\")\n",
    "print(\"Dit is een 2D tensor met shape (aantal_voorbeelden, aantal_kenmerken).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data met categorische variabele\n",
    "data_cat = {\n",
    "    'oppervlakte_m2': [120, 85, 200, 150],\n",
    "    'type_woning': ['appartement', 'appartement', 'vrijstaand', 'rijhuis'],\n",
    "    'prijs_euro': [320000, 195000, 425000, 375000]\n",
    "}\n",
    "df_cat = pd.DataFrame(data_cat)\n",
    "\n",
    "print(\"Data met categorische kolom:\")\n",
    "print(df_cat)\n",
    "print()\n",
    "\n",
    "# One-hot encoding voor 'type_woning'\n",
    "df_encoded = pd.get_dummies(df_cat, columns=['type_woning'])\n",
    "print(\"Na one-hot encoding:\")\n",
    "print(df_encoded)\n",
    "print()\n",
    "\n",
    "X_encoded = df_encoded.values\n",
    "print(f\"Als matrix: shape {X_encoded.shape}\")\n",
    "print(\"De categorische kolom is nu omgezet naar drie binaire kolommen.\")\n",
    "print(\"Elk huis heeft een 1 in precies één van deze kolommen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grijswaarden afbeeldingen: MNIST als matrix\n",
    "\n",
    "Een grijswaarden afbeelding is een natuurlijke matrix. Elke cel bevat de helderheid van één pixel, typisch een waarde tussen 0 (zwart) en 255 (wit). De positie in de matrix correspondeert met de positie van de pixel in de afbeelding.\n",
    "\n",
    "Voor MNIST is elke afbeelding een matrix van 28 rijen en 28 kolommen. Het element op rij i en kolom j is de helderheid van de pixel op die locatie. Deze matrixrepresentatie is intuïtief omdat het de ruimtelijke structuur van de afbeelding bewaart: pixels die naast elkaar liggen in de afbeelding, liggen ook naast elkaar in de matrix.\n",
    "\n",
    "Voor een neuraal netwerk is het vaak handiger om de afbeelding voor te stellen als een vector. We \"flatten\" de matrix door alle rijen achter elkaar te plaatsen. Een 28×28 matrix wordt zo een vector met 784 elementen. Deze transformatie verliest de expliciete 2D-structuur, maar behoudt alle informatie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad MNIST\n",
    "print(\"MNIST laden...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X_mnist, y_mnist = mnist.data, mnist.target.astype(int)\n",
    "print(f\"Geladen: {len(X_mnist)} afbeeldingen\")\n",
    "print()\n",
    "\n",
    "# Neem één afbeelding\n",
    "afbeelding_vector = X_mnist[0]\n",
    "afbeelding_matrix = afbeelding_vector.reshape(28, 28)\n",
    "\n",
    "print(f\"Als vector: shape {afbeelding_vector.shape}\")\n",
    "print(f\"Als matrix: shape {afbeelding_matrix.shape}\")\n",
    "print()\n",
    "print(\"De vector en matrix bevatten exact dezelfde informatie,\")\n",
    "print(\"alleen anders georganiseerd.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer de twee representaties\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Matrix weergave\n",
    "axes[0].imshow(afbeelding_matrix, cmap='gray')\n",
    "axes[0].set_title(f'Als matrix (28×28)\\nLabel: {y_mnist[0]}', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Vector weergave (als lange strip)\n",
    "axes[1].imshow(afbeelding_vector.reshape(1, -1), cmap='gray', aspect='auto')\n",
    "axes[1].set_title('Als vector (1×784)', fontsize=12)\n",
    "axes[1].set_xlabel('Index')\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "# Vector weergave als lijnplot\n",
    "axes[2].plot(afbeelding_vector, linewidth=0.5)\n",
    "axes[2].set_title('Pixelwaarden als functie van index', fontsize=12)\n",
    "axes[2].set_xlabel('Index')\n",
    "axes[2].set_ylabel('Pixelwaarde')\n",
    "axes[2].set_xlim(0, 784)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"De pieken in de lijnplot corresponderen met de witte pixels van het cijfer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversie tussen matrix en vector\n",
    "print(\"Conversie tussen matrix en vector:\\n\")\n",
    "\n",
    "# Van matrix naar vector: flatten\n",
    "matrix_origineel = afbeelding_matrix.copy()\n",
    "vector_flatten = matrix_origineel.flatten()\n",
    "print(f\"matrix.flatten() -> shape {vector_flatten.shape}\")\n",
    "\n",
    "# Of met reshape\n",
    "vector_reshape = matrix_origineel.reshape(-1)  # -1 betekent: bereken automatisch\n",
    "print(f\"matrix.reshape(-1) -> shape {vector_reshape.shape}\")\n",
    "\n",
    "vector_reshape2 = matrix_origineel.reshape(784)  # Expliciet 784\n",
    "print(f\"matrix.reshape(784) -> shape {vector_reshape2.shape}\")\n",
    "print()\n",
    "\n",
    "# Van vector naar matrix: reshape\n",
    "terug_naar_matrix = vector_flatten.reshape(28, 28)\n",
    "print(f\"vector.reshape(28, 28) -> shape {terug_naar_matrix.shape}\")\n",
    "print()\n",
    "\n",
    "# Verifieer dat de conversie geen data verliest\n",
    "print(f\"Zijn ze identiek? {np.allclose(terug_naar_matrix, matrix_origineel)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kleurenafbeeldingen: drie dimensies van kleur\n",
    "\n",
    "Een grijswaarden afbeelding zoals MNIST is een 2D-matrix. Een kleurenafbeelding heeft een extra dimensie: de kleurkanalen. De meeste kleurenafbeeldingen gebruiken het RGB-systeem met drie kanalen voor Rood, Groen en Blauw.\n",
    "\n",
    "Een kleurenafbeelding van 224×224 pixels wordt voorgesteld als een 3D-tensor met shape (224, 224, 3) of (3, 224, 224), afhankelijk van de conventie. De eerste conventie noemen we \"channels last\" en wordt gebruikt door TensorFlow en Keras. De tweede conventie noemen we \"channels first\" en wordt gebruikt door PyTorch.\n",
    "\n",
    "Elke pixel heeft nu drie waarden in plaats van één. De kleur geel is bijvoorbeeld een combinatie van veel rood, veel groen, en weinig blauw: (255, 255, 0). Zwart is (0, 0, 0) en wit is (255, 255, 255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een kleine kleurenafbeelding (8x8 pixels)\n",
    "hoogte, breedte = 8, 8\n",
    "afbeelding_rgb = np.zeros((hoogte, breedte, 3), dtype=np.uint8)\n",
    "\n",
    "# Vul met kleuren: links rood, midden groen, rechts blauw\n",
    "afbeelding_rgb[:, :3, 0] = 255      # Linker kolommen: rood kanaal hoog\n",
    "afbeelding_rgb[:, 3:5, 1] = 255     # Middelste kolommen: groen kanaal hoog  \n",
    "afbeelding_rgb[:, 5:, 2] = 255      # Rechter kolommen: blauw kanaal hoog\n",
    "\n",
    "print(f\"Shape van kleurenafbeelding: {afbeelding_rgb.shape}\")\n",
    "print(f\"Dit is een 3D tensor: (hoogte, breedte, RGB_kanalen)\")\n",
    "print()\n",
    "\n",
    "# Visualiseer\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "\n",
    "axes[0].imshow(afbeelding_rgb)\n",
    "axes[0].set_title('Volledige RGB afbeelding', fontsize=11)\n",
    "\n",
    "axes[1].imshow(afbeelding_rgb[:, :, 0], cmap='Reds', vmin=0, vmax=255)\n",
    "axes[1].set_title('Rood kanaal', fontsize=11)\n",
    "\n",
    "axes[2].imshow(afbeelding_rgb[:, :, 1], cmap='Greens', vmin=0, vmax=255)\n",
    "axes[2].set_title('Groen kanaal', fontsize=11)\n",
    "\n",
    "axes[3].imshow(afbeelding_rgb[:, :, 2], cmap='Blues', vmin=0, vmax=255)\n",
    "axes[3].set_title('Blauw kanaal', fontsize=11)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een kleurverloop als voorbeeld van een grotere afbeelding\n",
    "x = np.linspace(0, 1, 256)\n",
    "y = np.linspace(0, 1, 256)\n",
    "X_grid, Y_grid = np.meshgrid(x, y)\n",
    "\n",
    "# RGB kanalen gebaseerd op positie\n",
    "R = (X_grid * 255).astype(np.uint8)\n",
    "G = (Y_grid * 255).astype(np.uint8)\n",
    "B = ((1 - X_grid) * 255).astype(np.uint8)\n",
    "\n",
    "kleur_afbeelding = np.stack([R, G, B], axis=2)\n",
    "\n",
    "print(f\"Kleurenafbeelding shape: {kleur_afbeelding.shape}\")\n",
    "print(f\"Dit is een 3D tensor met:\")\n",
    "print(f\"  - {kleur_afbeelding.shape[0]} pixels hoog\")\n",
    "print(f\"  - {kleur_afbeelding.shape[1]} pixels breed\")\n",
    "print(f\"  - {kleur_afbeelding.shape[2]} kleurkanalen (RGB)\")\n",
    "print()\n",
    "\n",
    "# Eén specifieke pixel bekijken\n",
    "pixel_100_100 = kleur_afbeelding[100, 100]\n",
    "print(f\"Pixel op positie (100, 100): R={pixel_100_100[0]}, G={pixel_100_100[1]}, B={pixel_100_100[2]}\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(kleur_afbeelding)\n",
    "plt.title('Kleurenafbeelding: elke pixel is een vector van 3 waarden', fontsize=12)\n",
    "plt.xlabel('X positie (bepaalt R en B)')\n",
    "plt.ylabel('Y positie (bepaalt G)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch van afbeeldingen als 4D tensor\n",
    "# In de praktijk verwerken we meerdere afbeeldingen tegelijk (een \"batch\")\n",
    "\n",
    "batch_size = 32\n",
    "hoogte, breedte, kanalen = 224, 224, 3\n",
    "\n",
    "# Simuleer een batch van 32 kleurenafbeeldingen\n",
    "batch_afbeeldingen = np.random.randint(0, 256, size=(batch_size, hoogte, breedte, kanalen), dtype=np.uint8)\n",
    "\n",
    "print(f\"Batch shape: {batch_afbeeldingen.shape}\")\n",
    "print(f\"Dit is een 4D tensor: (batch_size, hoogte, breedte, kanalen)\")\n",
    "print()\n",
    "print(f\"De tensor bevat:\")\n",
    "print(f\"  - {batch_afbeeldingen.shape[0]} afbeeldingen in de batch\")\n",
    "print(f\"  - {batch_afbeeldingen.shape[1]} pixels hoog per afbeelding\")\n",
    "print(f\"  - {batch_afbeeldingen.shape[2]} pixels breed per afbeelding\")\n",
    "print(f\"  - {batch_afbeeldingen.shape[3]} kleurkanalen per pixel\")\n",
    "print()\n",
    "print(f\"Totaal aantal getallen in de batch: {batch_afbeeldingen.size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video: de tijd als extra dimensie\n",
    "\n",
    "Een video is een reeks van afbeeldingen die snel na elkaar worden getoond. Als elke frame een kleurenafbeelding is, dan is een video een 4D-tensor met dimensies voor tijd (aantal frames), hoogte, breedte en kleurkanalen.\n",
    "\n",
    "Een video van 10 seconden aan 30 frames per seconde, met een resolutie van 1920×1080 pixels, bevat 300 frames. Als 4D-tensor heeft dit shape (300, 1080, 1920, 3), wat neerkomt op meer dan 1.8 miljard getallen. Dit illustreert waarom videoprocessing zo rekenintensief is en waarom video's worden gecomprimeerd voor opslag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuleer een korte video met een bewegend vierkant\n",
    "frames = 30          # 1 seconde aan 30 fps\n",
    "hoogte = 64          # Kleine resolutie voor het voorbeeld\n",
    "breedte = 64\n",
    "kanalen = 3\n",
    "\n",
    "# Maak een video waar een vierkant beweegt\n",
    "video = np.zeros((frames, hoogte, breedte, kanalen), dtype=np.uint8)\n",
    "\n",
    "for t in range(frames):\n",
    "    # Achtergrond: donkerblauw\n",
    "    video[t, :, :, 2] = 50\n",
    "    \n",
    "    # Bewegend rood vierkant\n",
    "    x_pos = int(5 + t * 1.5)  # Beweegt naar rechts\n",
    "    if x_pos + 15 < breedte:\n",
    "        video[t, 20:35, x_pos:x_pos+15, 0] = 255  # Rood vierkant\n",
    "\n",
    "print(f\"Video tensor shape: {video.shape}\")\n",
    "print(f\"Dit is een 4D tensor: (frames, hoogte, breedte, kanalen)\")\n",
    "print()\n",
    "print(f\"De tensor bevat:\")\n",
    "print(f\"  - {video.shape[0]} frames\")\n",
    "print(f\"  - {video.shape[1]}×{video.shape[2]} pixels per frame\")\n",
    "print(f\"  - {video.shape[3]} kleurkanalen\")\n",
    "print()\n",
    "print(f\"Totaal aantal getallen: {video.size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer enkele frames\n",
    "fig, axes = plt.subplots(1, 6, figsize=(15, 3))\n",
    "frame_indices = [0, 5, 10, 15, 20, 25]\n",
    "\n",
    "for ax, idx in zip(axes, frame_indices):\n",
    "    ax.imshow(video[idx])\n",
    "    ax.set_title(f'Frame {idx}', fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Video: opeenvolgende frames tonen beweging', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typische video dimensies in de praktijk\n",
    "print(\"Video dimensies en geheugengebruik (ongecomprimeerd):\\n\")\n",
    "print(f\"{'Beschrijving':<30} {'Shape':<28} {'Geheugen':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "voorbeelden = [\n",
    "    (\"YouTube 1080p, 1 minuut\", 30*60, 1080, 1920, 3),\n",
    "    (\"TikTok video, 30 sec\", 30*30, 1920, 1080, 3),\n",
    "    (\"Webcam 720p, 10 sec\", 30*10, 720, 1280, 3),\n",
    "    (\"Surveillance 24 uur\", 10*3600*24, 480, 640, 3),\n",
    "]\n",
    "\n",
    "for beschrijving, num_frames, h, w, c in voorbeelden:\n",
    "    shape = f\"({num_frames}, {h}, {w}, {c})\"\n",
    "    totaal_bytes = num_frames * h * w * c\n",
    "    \n",
    "    if totaal_bytes > 1e9:\n",
    "        geheugen = f\"{totaal_bytes / 1e9:.1f} GB\"\n",
    "    else:\n",
    "        geheugen = f\"{totaal_bytes / 1e6:.1f} MB\"\n",
    "    \n",
    "    print(f\"{beschrijving:<30} {shape:<28} {geheugen:<12}\")\n",
    "\n",
    "print()\n",
    "print(\"Dit verklaart waarom video's altijd worden gecomprimeerd!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tekst: van woorden naar getallen\n",
    "\n",
    "Tekst is fundamenteel anders dan afbeeldingen omdat woorden geen natuurlijke numerieke representatie hebben. Het woord \"kat\" is niet inherent groter of kleiner dan het woord \"hond\". Er zijn verschillende strategieën om tekst om te zetten naar tensoren.\n",
    "\n",
    "De eenvoudigste methode is one-hot encoding op woordniveau. Elk woord krijgt een index in een vocabulaire, en wordt voorgesteld als een vector met een 1 op die positie en nullen elders. Een zin wordt dan een 2D-matrix waar elke rij een woord is.\n",
    "\n",
    "Het probleem met one-hot encoding is dat de vectoren enorm groot zijn (de grootte van het vocabulaire, vaak tienduizenden woorden) en dat ze geen semantische informatie bevatten: de vectoren voor \"koning\" en \"koningin\" lijken niet meer op elkaar dan \"koning\" en \"fiets\".\n",
    "\n",
    "Moderne systemen gebruiken daarom word embeddings: geleerde vectoren van typisch 100-300 dimensies die semantische relaties vastleggen. Woorden met vergelijkbare betekenis krijgen vergelijkbare vectoren. Dit is ook hoe grote taalmodellen zoals GPT werken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorbeeld zin\n",
    "zin = \"de kat zit op de mat\"\n",
    "woorden = zin.split()\n",
    "\n",
    "# Bouw vocabulaire\n",
    "vocabulaire = sorted(set(woorden))\n",
    "woord_naar_index = {woord: i for i, woord in enumerate(vocabulaire)}\n",
    "\n",
    "print(f\"Zin: '{zin}'\")\n",
    "print(f\"Woorden: {woorden}\")\n",
    "print(f\"Uniek vocabulaire: {vocabulaire}\")\n",
    "print(f\"Vocabulaire grootte: {len(vocabulaire)}\")\n",
    "print()\n",
    "print(\"Woord naar index mapping:\")\n",
    "for woord, idx in woord_naar_index.items():\n",
    "    print(f\"  '{woord}' -> {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "vocab_grootte = len(vocabulaire)\n",
    "zin_lengte = len(woorden)\n",
    "\n",
    "# Shape: (aantal_woorden, vocabulaire_grootte)\n",
    "one_hot_matrix = np.zeros((zin_lengte, vocab_grootte))\n",
    "\n",
    "for i, woord in enumerate(woorden):\n",
    "    j = woord_naar_index[woord]\n",
    "    one_hot_matrix[i, j] = 1\n",
    "\n",
    "print(f\"One-hot matrix shape: {one_hot_matrix.shape}\")\n",
    "print(f\"Dit is een 2D tensor: (zin_lengte, vocabulaire_grootte)\")\n",
    "print()\n",
    "print(\"One-hot matrix:\")\n",
    "print(f\"{'Woord':<10} {vocabulaire}\")\n",
    "print(\"-\" * 50)\n",
    "for i, woord in enumerate(woorden):\n",
    "    print(f\"{woord:<10} {one_hot_matrix[i].astype(int)}\")\n",
    "\n",
    "print()\n",
    "print(\"Elk woord is nu een vector met één 1 en verder nullen.\")\n",
    "print(\"Let op: 'de' komt twee keer voor maar heeft dezelfde representatie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probleem met one-hot: geen semantische informatie\n",
    "\n",
    "def one_hot_vector(woord, vocab_dict, vocab_size):\n",
    "    vec = np.zeros(vocab_size)\n",
    "    if woord in vocab_dict:\n",
    "        vec[vocab_dict[woord]] = 1\n",
    "    return vec\n",
    "\n",
    "# Groter vocabulaire voor demonstratie\n",
    "groot_vocab = ['kat', 'hond', 'dier', 'auto', 'fiets', 'voertuig', 'de', 'een']\n",
    "groot_vocab_dict = {w: i for i, w in enumerate(groot_vocab)}\n",
    "\n",
    "vec_kat = one_hot_vector('kat', groot_vocab_dict, len(groot_vocab))\n",
    "vec_hond = one_hot_vector('hond', groot_vocab_dict, len(groot_vocab))\n",
    "vec_auto = one_hot_vector('auto', groot_vocab_dict, len(groot_vocab))\n",
    "\n",
    "print(\"One-hot vectoren:\")\n",
    "print(f\"  'kat':  {vec_kat.astype(int)}\")\n",
    "print(f\"  'hond': {vec_hond.astype(int)}\")\n",
    "print(f\"  'auto': {vec_auto.astype(int)}\")\n",
    "print()\n",
    "\n",
    "# Bereken afstanden (dot products)\n",
    "print(\"Dot products (maat voor gelijkenis):\")\n",
    "print(f\"  'kat' · 'hond' = {vec_kat @ vec_hond} (beide dieren, maar dot product is 0!)\")\n",
    "print(f\"  'kat' · 'auto' = {vec_kat @ vec_auto} (niet gerelateerd, ook 0)\")\n",
    "print()\n",
    "print(\"One-hot encoding bevat geen semantische informatie.\")\n",
    "print(\"'kat' en 'hond' lijken niet meer op elkaar dan 'kat' en 'auto'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embeddings: dense vectoren met semantische betekenis\n",
    "# In de praktijk worden deze geleerd, hier simuleren we ze\n",
    "\n",
    "embedding_dim = 4\n",
    "\n",
    "# Gesimuleerde embeddings (in werkelijkheid komen deze uit training)\n",
    "embeddings = {\n",
    "    'kat': np.array([0.8, -0.5, 0.6, 0.1]),      # Dier, huisdier\n",
    "    'hond': np.array([0.7, -0.4, 0.5, 0.2]),     # Ook een dier - lijkt op kat!\n",
    "    'auto': np.array([-0.6, 0.8, -0.3, 0.5]),    # Voertuig\n",
    "    'fiets': np.array([-0.5, 0.7, -0.2, 0.4]),   # Ook voertuig - lijkt op auto!\n",
    "    'de': np.array([0.1, 0.1, 0.0, -0.1]),       # Functiewoord\n",
    "    'zit': np.array([0.2, 0.3, 0.8, -0.2]),      # Werkwoord\n",
    "}\n",
    "\n",
    "print(\"Word embeddings (dense vectoren met 4 dimensies):\")\n",
    "print()\n",
    "for woord, vector in embeddings.items():\n",
    "    print(f\"  '{woord}': {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nu kunnen we semantische gelijkenis meten met cosine similarity\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    return (u @ v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "print(\"Cosine similarity met word embeddings:\")\n",
    "print()\n",
    "\n",
    "# Dieren onderling\n",
    "sim_kat_hond = cosine_similarity(embeddings['kat'], embeddings['hond'])\n",
    "print(f\"  'kat' en 'hond':  {sim_kat_hond:.3f} (beide huisdieren - hoge gelijkenis!)\")\n",
    "\n",
    "# Voertuigen onderling\n",
    "sim_auto_fiets = cosine_similarity(embeddings['auto'], embeddings['fiets'])\n",
    "print(f\"  'auto' en 'fiets': {sim_auto_fiets:.3f} (beide voertuigen - hoge gelijkenis!)\")\n",
    "\n",
    "# Dier vs voertuig\n",
    "sim_kat_auto = cosine_similarity(embeddings['kat'], embeddings['auto'])\n",
    "print(f\"  'kat' en 'auto':  {sim_kat_auto:.3f} (niet gerelateerd - lage/negatieve gelijkenis)\")\n",
    "\n",
    "print()\n",
    "print(\"Embeddings vangen semantische relaties: vergelijkbare woorden hebben vergelijkbare vectoren.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zin als matrix van embeddings\n",
    "zin_woorden = ['de', 'kat', 'zit']\n",
    "zin_embeddings = np.array([embeddings[w] for w in zin_woorden])\n",
    "\n",
    "print(f\"Zin: {zin_woorden}\")\n",
    "print(f\"Als embedding matrix: shape {zin_embeddings.shape}\")\n",
    "print(f\"Dit is een 2D tensor: (zin_lengte, embedding_dimensie)\")\n",
    "print()\n",
    "print(\"Matrix:\")\n",
    "print(zin_embeddings)\n",
    "print()\n",
    "print(\"Elke rij is het embedding van één woord.\")\n",
    "print(\"Dit is de input voor moderne taalmodellen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overzicht van alle datatypes en hun tensor shapes\n",
    "print(\"OVERZICHT: Datatypes en hun tensor representaties\")\n",
    "print(\"=\" * 75)\n",
    "print()\n",
    "print(f\"{'Datatype':<25} {'Tensor dimensies':<20} {'Voorbeeld shape':<20}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "datatypes = [\n",
    "    (\"Gestructureerde data\", \"2D\", \"(1000, 10)\"),\n",
    "    (\"Grijswaarden afbeelding\", \"2D\", \"(28, 28)\"),\n",
    "    (\"MNIST als vector\", \"1D\", \"(784,)\"),\n",
    "    (\"Kleurenafbeelding\", \"3D\", \"(224, 224, 3)\"),\n",
    "    (\"Batch afbeeldingen\", \"4D\", \"(32, 224, 224, 3)\"),\n",
    "    (\"Video\", \"4D\", \"(300, 1080, 1920, 3)\"),\n",
    "    (\"Batch video's\", \"5D\", \"(8, 300, 224, 224, 3)\"),\n",
    "    (\"Tekst (one-hot)\", \"2D\", \"(100, 10000)\"),\n",
    "    (\"Tekst (embeddings)\", \"2D\", \"(100, 256)\"),\n",
    "    (\"Batch tekst\", \"3D\", \"(32, 100, 256)\"),\n",
    "]\n",
    "\n",
    "for dtype, dims, shape in datatypes:\n",
    "    print(f\"{dtype:<25} {dims:<20} {shape:<20}\")\n",
    "\n",
    "print()\n",
    "print(\"Let op: bij batches wordt altijd een extra dimensie toegevoegd vooraan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Operaties op Vectoren\n",
    "\n",
    "Nu we weten hoe data wordt voorgesteld als vectoren en matrices, leren we de fundamentele operaties die we kunnen uitvoeren. Deze operaties vormen de bouwstenen van alle berekeningen in neurale netwerken.\n",
    "\n",
    "### Vectoroptelling\n",
    "\n",
    "Twee vectoren van dezelfde lengte kunnen worden opgeteld door de overeenkomstige elementen op te tellen. Als u = [1, 2, 3] en v = [4, 5, 6], dan is u + v = [1+4, 2+5, 3+6] = [5, 7, 9]. De vectoren moeten dezelfde dimensie hebben, anders is de optelling niet gedefinieerd.\n",
    "\n",
    "Geometrisch kun je vectoroptelling visualiseren met de \"kop-aan-staart\" methode. Plaats de staart van de tweede vector aan de kop van de eerste, en het resultaat is de vector van de oorsprong naar de kop van de tweede. Dit is intuïtief als je vectoren ziet als verplaatsingen: eerst ga je in de richting van u, dan in de richting van v.\n",
    "\n",
    "In Machine Learning gebruiken we vectoroptelling wanneer we een bias toevoegen aan de output van een laag. De bias is een vector die bij elke output wordt opgeteld, waardoor het netwerk de output kan verschuiven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([1, 2, 3])\n",
    "v = np.array([4, 5, 6])\n",
    "\n",
    "som = u + v\n",
    "\n",
    "print(f\"u = {u}\")\n",
    "print(f\"v = {v}\")\n",
    "print(f\"u + v = {som}\")\n",
    "print()\n",
    "print(\"Element voor element: [1+4, 2+5, 3+6] = [5, 7, 9]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometrische visualisatie van vectoroptelling in 2D\n",
    "u_2d = np.array([3, 1])\n",
    "v_2d = np.array([1, 2])\n",
    "som_2d = u_2d + v_2d\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Teken vector u (blauw)\n",
    "plt.quiver(0, 0, u_2d[0], u_2d[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='blue', width=0.015, label=f'u = {u_2d}')\n",
    "\n",
    "# Teken vector v vanaf de kop van u (rood)\n",
    "plt.quiver(u_2d[0], u_2d[1], v_2d[0], v_2d[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='red', width=0.015, label=f'v = {v_2d}')\n",
    "\n",
    "# Teken de som vector (groen)\n",
    "plt.quiver(0, 0, som_2d[0], som_2d[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='green', width=0.015, label=f'u + v = {som_2d}')\n",
    "\n",
    "plt.xlim(-0.5, 5.5)\n",
    "plt.ylim(-0.5, 4.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.legend(fontsize=11)\n",
    "plt.title('Vectoroptelling: kop-aan-staart methode', fontsize=14)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"De groene vector (som) gaat van de oorsprong naar waar v eindigt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalaire vermenigvuldiging\n",
    "\n",
    "Een vector kan worden vermenigvuldigd met een scalar door elk element met die scalar te vermenigvuldigen. Als v = [1, 2, 3] en c = 2, dan is c·v = [2, 4, 6].\n",
    "\n",
    "Geometrisch verandert scalaire vermenigvuldiging de lengte van de vector zonder de richting te veranderen (tenzij de scalar negatief is). Een scalar groter dan 1 maakt de vector langer, een scalar tussen 0 en 1 maakt hem korter. Een negatieve scalar keert de richting om.\n",
    "\n",
    "In neurale netwerken zijn de gewichten scalars die de input signalen schalen. Een groot gewicht versterkt een signaal, een klein gewicht verzwakt het, en een negatief gewicht keert het om."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([1, 2, 3])\n",
    "\n",
    "print(f\"Originele vector v = {v}\")\n",
    "print()\n",
    "print(f\"2 × v = {2 * v}  (verdubbeld)\")\n",
    "print(f\"0.5 × v = {0.5 * v}  (gehalveerd)\")\n",
    "print(f\"-1 × v = {-1 * v}  (omgekeerd)\")\n",
    "print(f\"0 × v = {0 * v}  (nulvector)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometrische visualisatie van scalaire vermenigvuldiging\n",
    "v = np.array([2, 1])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "scalars = [2, 0.5, -1]\n",
    "titles = ['c = 2 (verlengen)', 'c = 0.5 (verkorten)', 'c = -1 (omkeren)']\n",
    "colors = ['red', 'orange', 'purple']\n",
    "\n",
    "for ax, c, title, color in zip(axes, scalars, titles, colors):\n",
    "    # Originele vector\n",
    "    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color='blue', width=0.02, label=f'v = {v}')\n",
    "    \n",
    "    # Geschaalde vector\n",
    "    scaled = c * v\n",
    "    ax.quiver(0, 0, scaled[0], scaled[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color=color, width=0.02, label=f'{c}·v = {scaled}')\n",
    "    \n",
    "    ax.set_xlim(-3, 5)\n",
    "    ax.set_ylim(-2, 3)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De norm: lengte van een vector\n",
    "\n",
    "De norm van een vector is een maat voor de \"grootte\" of lengte van de vector. De meest gebruikte norm is de Euclidische norm of L2-norm, die overeenkomt met de gewone afstand in de ruimte.\n",
    "\n",
    "Voor een vector v = [v₁, v₂, ..., vₙ] is de L2-norm: ||v|| = √(v₁² + v₂² + ... + vₙ²)\n",
    "\n",
    "Dit is een generalisatie van de stelling van Pythagoras naar hogere dimensies. In 2D is de norm de lengte van de schuine zijde van een rechthoekige driehoek. In 3D is het de afstand van de oorsprong tot een punt in de ruimte.\n",
    "\n",
    "De norm is belangrijk in Machine Learning voor meerdere doeleinden. We gebruiken het om vectoren te normaliseren, zodat ze lengte 1 hebben. We gebruiken het voor regularisatie, waarbij we grote gewichten bestraffen. We gebruiken het om afstanden tussen punten te berekenen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Het bekende 3-4-5 driehoek voorbeeld\n",
    "v = np.array([3, 4])\n",
    "\n",
    "# Handmatige berekening\n",
    "norm_handmatig = np.sqrt(v[0]**2 + v[1]**2)\n",
    "\n",
    "# Met NumPy\n",
    "norm_numpy = np.linalg.norm(v)\n",
    "\n",
    "print(f\"Vector v = {v}\")\n",
    "print()\n",
    "print(\"Berekening van de L2-norm:\")\n",
    "print(f\"||v|| = √(3² + 4²) = √(9 + 16) = √25 = {norm_handmatig}\")\n",
    "print()\n",
    "print(f\"Met NumPy: np.linalg.norm(v) = {norm_numpy}\")\n",
    "print()\n",
    "print(\"Dit is het bekende 3-4-5 rechthoekige driehoek!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisatie van de norm als lengte\n",
    "v = np.array([3, 4])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Teken de vector\n",
    "plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='blue', width=0.02)\n",
    "\n",
    "# Teken de rechthoekige driehoek\n",
    "plt.plot([0, v[0]], [0, 0], 'r--', linewidth=2, label=f'horizontaal = {v[0]}')\n",
    "plt.plot([v[0], v[0]], [0, v[1]], 'g--', linewidth=2, label=f'verticaal = {v[1]}')\n",
    "\n",
    "# Markeer de hoek\n",
    "angle = np.arctan2(v[1], v[0])\n",
    "theta = np.linspace(0, angle, 20)\n",
    "r = 0.5\n",
    "plt.plot(r * np.cos(theta), r * np.sin(theta), 'k-', linewidth=1)\n",
    "\n",
    "plt.xlim(-0.5, 5)\n",
    "plt.ylim(-0.5, 5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.legend(fontsize=11)\n",
    "plt.title(f'Stelling van Pythagoras: ||v|| = √(3² + 4²) = 5', fontsize=14)\n",
    "plt.text(1.5, 2.2, f'||v|| = {np.linalg.norm(v):.0f}', fontsize=14, color='blue')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm in hogere dimensies\n",
    "v_3d = np.array([1, 2, 2])\n",
    "v_hoog = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])  # 10 dimensies\n",
    "\n",
    "print(\"De norm werkt in elke dimensie:\")\n",
    "print()\n",
    "print(f\"3D vector: {v_3d}\")\n",
    "print(f\"||v|| = √(1² + 2² + 2²) = √(1 + 4 + 4) = √9 = {np.linalg.norm(v_3d)}\")\n",
    "print()\n",
    "print(f\"10D vector: {v_hoog}\")\n",
    "print(f\"||v|| = √(1² + 1² + ... + 1²) = √10 = {np.linalg.norm(v_hoog):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisatie: vectoren met lengte 1\n",
    "\n",
    "Een genormaliseerde vector of eenheidsvector is een vector met norm 1. We kunnen elke niet-nul vector normaliseren door hem te delen door zijn norm: û = v / ||v||\n",
    "\n",
    "Het resultaat is een vector die dezelfde richting heeft als de originele vector, maar met lengte 1. Genormaliseerde vectoren zijn handig wanneer we alleen geïnteresseerd zijn in de richting, niet in de grootte.\n",
    "\n",
    "In word embeddings vergelijken we vaak de richtingen van vectoren om semantische gelijkenis te meten, ongeacht hoe \"sterk\" de embeddings zijn. In neurale netwerken wordt batch normalization gebruikt om de activaties te normaliseren, wat het trainen stabieler maakt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([3, 4])\n",
    "norm_v = np.linalg.norm(v)\n",
    "v_genormaliseerd = v / norm_v\n",
    "\n",
    "print(f\"Originele vector: v = {v}\")\n",
    "print(f\"Norm van v: ||v|| = {norm_v}\")\n",
    "print()\n",
    "print(f\"Genormaliseerde vector: v / ||v|| = {v} / {norm_v} = {v_genormaliseerd}\")\n",
    "print()\n",
    "print(f\"Norm van genormaliseerde vector: {np.linalg.norm(v_genormaliseerd):.10f}\")\n",
    "print()\n",
    "print(\"De genormaliseerde vector heeft exact lengte 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisatie van normalisatie\n",
    "vectoren = [\n",
    "    np.array([3, 4]),\n",
    "    np.array([1, 1]),\n",
    "    np.array([4, 1]),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for v, color in zip(vectoren, colors):\n",
    "    # Originele vector (licht)\n",
    "    plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "               color=color, alpha=0.3, width=0.015)\n",
    "    \n",
    "    # Genormaliseerde vector (donker)\n",
    "    v_norm = v / np.linalg.norm(v)\n",
    "    plt.quiver(0, 0, v_norm[0], v_norm[1], angles='xy', scale_units='xy', scale=1, \n",
    "               color=color, width=0.015, label=f'{v} → {v_norm.round(2)}')\n",
    "\n",
    "# Teken eenheidscirkel\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "plt.plot(np.cos(theta), np.sin(theta), 'k--', linewidth=1, alpha=0.5, label='Eenheidscirkel')\n",
    "\n",
    "plt.xlim(-1.5, 5)\n",
    "plt.ylim(-1.5, 5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.legend(fontsize=10, loc='upper right')\n",
    "plt.title('Normalisatie: alle vectoren worden teruggebracht tot lengte 1', fontsize=14)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"De lichte pijlen zijn de originele vectoren.\")\n",
    "print(\"De donkere pijlen zijn de genormaliseerde vectoren (allemaal op de eenheidscirkel).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Het Dot Product\n",
    "\n",
    "### Definitie en berekening\n",
    "\n",
    "Het dot product (ook wel inwendig product of scalair product genoemd) is een fundamentele operatie die twee vectoren combineert tot één enkel getal (een scalar). Voor vectoren u = [u₁, u₂, ..., uₙ] en v = [v₁, v₂, ..., vₙ] is het dot product:\n",
    "\n",
    "u · v = u₁v₁ + u₂v₂ + ... + uₙvₙ\n",
    "\n",
    "Je vermenigvuldigt de overeenkomstige elementen en telt ze op. Het resultaat is één enkel getal. Dit lijkt simpel, maar het dot product is misschien wel de belangrijkste operatie in Machine Learning. Vrijwel elke berekening in een neuraal netwerk is uiteindelijk gebaseerd op dot producten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([1, 2, 3])\n",
    "v = np.array([4, 5, 6])\n",
    "\n",
    "# Handmatige berekening\n",
    "dot_handmatig = u[0]*v[0] + u[1]*v[1] + u[2]*v[2]\n",
    "\n",
    "# Met NumPy functie\n",
    "dot_numpy = np.dot(u, v)\n",
    "\n",
    "# Met de @ operator (aanbevolen in moderne Python)\n",
    "dot_operator = u @ v\n",
    "\n",
    "print(f\"u = {u}\")\n",
    "print(f\"v = {v}\")\n",
    "print()\n",
    "print(\"Dot product berekening:\")\n",
    "print(f\"u · v = 1×4 + 2×5 + 3×6 = 4 + 10 + 18 = {dot_handmatig}\")\n",
    "print()\n",
    "print(f\"Met np.dot(u, v): {dot_numpy}\")\n",
    "print(f\"Met u @ v: {dot_operator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometrische interpretatie\n",
    "\n",
    "Het dot product heeft een prachtige geometrische betekenis. Het is gelijk aan het product van de lengtes van de vectoren en de cosinus van de hoek θ tussen hen:\n",
    "\n",
    "u · v = ||u|| × ||v|| × cos(θ)\n",
    "\n",
    "Dit leidt tot belangrijke inzichten over de relatie tussen twee vectoren:\n",
    "\n",
    "Als de vectoren in dezelfde richting wijzen (θ = 0°), is cos(θ) = 1, en het dot product is maximaal positief. Als de vectoren loodrecht op elkaar staan (θ = 90°), is cos(θ) = 0, dus het dot product is 0. Als de vectoren in tegengestelde richting wijzen (θ = 180°), is cos(θ) = -1, en het dot product is maximaal negatief.\n",
    "\n",
    "Het dot product meet dus in zekere zin hoeveel twee vectoren \"op elkaar lijken\" qua richting. Dit is precies waarom het zo nuttig is in Machine Learning: we kunnen het gebruiken om de gelijkenis tussen datapunten te meten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstratie van de geometrische interpretatie\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "scenarios = [\n",
    "    (np.array([1, 0]), np.array([1, 0]), 'Zelfde richting (θ = 0°)'),\n",
    "    (np.array([1, 0]), np.array([0, 1]), 'Loodrecht (θ = 90°)'),\n",
    "    (np.array([1, 0]), np.array([-1, 0]), 'Tegengesteld (θ = 180°)')\n",
    "]\n",
    "\n",
    "for ax, (u, v, title) in zip(axes, scenarios):\n",
    "    # Bereken dot product\n",
    "    dot = u @ v\n",
    "    \n",
    "    # Teken vectoren\n",
    "    ax.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color='blue', width=0.03, label='u')\n",
    "    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color='red', width=0.03, label='v')\n",
    "    \n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_title(f'{title}\\nu · v = {dot}', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Het teken van het dot product vertelt ons over de hoek tussen de vectoren:\")\n",
    "print(\"  - Positief: hoek < 90° (vectoren wijzen min of meer dezelfde kant op)\")\n",
    "print(\"  - Nul: hoek = 90° (vectoren staan loodrecht op elkaar)\")\n",
    "print(\"  - Negatief: hoek > 90° (vectoren wijzen min of meer tegengesteld)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De hoek berekenen uit het dot product\n",
    "u = np.array([1, 2])\n",
    "v = np.array([3, 1])\n",
    "\n",
    "# Bereken dot product en normen\n",
    "dot_product = u @ v\n",
    "norm_u = np.linalg.norm(u)\n",
    "norm_v = np.linalg.norm(v)\n",
    "\n",
    "# cos(θ) = (u · v) / (||u|| × ||v||)\n",
    "cos_theta = dot_product / (norm_u * norm_v)\n",
    "\n",
    "# θ = arccos(cos(θ))\n",
    "theta_radialen = np.arccos(cos_theta)\n",
    "theta_graden = np.degrees(theta_radialen)\n",
    "\n",
    "print(f\"u = {u}, ||u|| = {norm_u:.4f}\")\n",
    "print(f\"v = {v}, ||v|| = {norm_v:.4f}\")\n",
    "print()\n",
    "print(f\"u · v = {dot_product}\")\n",
    "print(f\"cos(θ) = {dot_product} / ({norm_u:.4f} × {norm_v:.4f}) = {cos_theta:.4f}\")\n",
    "print(f\"θ = arccos({cos_theta:.4f}) = {theta_graden:.2f}°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product in neurale netwerken\n",
    "\n",
    "In een neuraal netwerk is het dot product de kernoperatie. Wanneer een neuron zijn output berekent, neemt het het dot product van de input vector met zijn gewichten vector, en telt daar een bias bij op:\n",
    "\n",
    "output = (inputs · weights) + bias\n",
    "\n",
    "Dit betekent dat elk gewicht bepaalt hoeveel de corresponderende input bijdraagt aan de output. Een groot positief gewicht versterkt die input, een groot negatief gewicht keert het effect om, en een gewicht dicht bij nul negeert die input grotendeels.\n",
    "\n",
    "Als de input vector en de gewichten vector in dezelfde \"richting\" wijzen (positief dot product), wordt de output groot. Als ze tegengesteld zijn (negatief dot product), wordt de output klein of negatief. Het neuron \"activeert\" dus wanneer de input lijkt op wat de gewichten coderen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulatie van één neuron\n",
    "inputs = np.array([0.5, 0.3, 0.8, 0.1])   # 4 input waarden\n",
    "weights = np.array([0.2, -0.5, 0.9, 0.1])  # 4 gewichten\n",
    "bias = 0.1\n",
    "\n",
    "# De output van het neuron (voor activatie)\n",
    "weighted_sum = inputs @ weights\n",
    "output = weighted_sum + bias\n",
    "\n",
    "print(\"Neuron berekening:\")\n",
    "print()\n",
    "print(f\"Inputs:  {inputs}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "print(f\"Bias:    {bias}\")\n",
    "print()\n",
    "print(\"Stap 1: Dot product van inputs en weights\")\n",
    "print(f\"  {inputs[0]}×{weights[0]} + {inputs[1]}×{weights[1]} + {inputs[2]}×{weights[2]} + {inputs[3]}×{weights[3]}\")\n",
    "print(f\"  = {inputs[0]*weights[0]:.2f} + {inputs[1]*weights[1]:.2f} + {inputs[2]*weights[2]:.2f} + {inputs[3]*weights[3]:.2f}\")\n",
    "print(f\"  = {weighted_sum:.4f}\")\n",
    "print()\n",
    "print(f\"Stap 2: Tel bias op\")\n",
    "print(f\"  {weighted_sum:.4f} + {bias} = {output:.4f}\")\n",
    "print()\n",
    "print(f\"Output van het neuron: {output:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstratie: het neuron reageert op bepaalde patronen\n",
    "weights = np.array([1.0, 1.0, 0.0, 0.0])  # Reageert op de eerste twee inputs\n",
    "bias = -0.5\n",
    "\n",
    "test_inputs = [\n",
    "    np.array([1.0, 1.0, 0.0, 0.0]),  # Matcht de gewichten\n",
    "    np.array([0.0, 0.0, 1.0, 1.0]),  # Tegengesteld aan de gewichten\n",
    "    np.array([0.5, 0.5, 0.5, 0.5]),  # Neutraal\n",
    "    np.array([1.0, 0.0, 1.0, 0.0]),  # Gedeeltelijke match\n",
    "]\n",
    "\n",
    "print(f\"Gewichten: {weights}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "print()\n",
    "print(\"Dit neuron 'zoekt' naar activatie in de eerste twee inputs.\\n\")\n",
    "\n",
    "for inp in test_inputs:\n",
    "    output = inp @ weights + bias\n",
    "    print(f\"Input: {inp} → Output: {output:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Toepassing: Gelijkenis tussen Afbeeldingen\n",
    "\n",
    "We kunnen het dot product en cosine similarity gebruiken om te meten hoe \"gelijkend\" twee MNIST-afbeeldingen zijn. Dit is een vereenvoudigde versie van wat een neuraal netwerk doet: de gewichten van een getraind netwerk bevatten \"templates\" van kenmerken, en het netwerk berekent hoe goed de input overeenkomt met deze templates via dot producten.\n",
    "\n",
    "### Dot product als gelijkenismaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecteer voorbeelden van verschillende cijfers\n",
    "idx_3_a = np.where(y_mnist == 3)[0][0]  # Eerste 3\n",
    "idx_3_b = np.where(y_mnist == 3)[0][1]  # Tweede 3\n",
    "idx_8 = np.where(y_mnist == 8)[0][0]    # Eerste 8\n",
    "idx_1 = np.where(y_mnist == 1)[0][0]    # Eerste 1\n",
    "\n",
    "afb_3_a = X_mnist[idx_3_a]\n",
    "afb_3_b = X_mnist[idx_3_b]\n",
    "afb_8 = X_mnist[idx_8]\n",
    "afb_1 = X_mnist[idx_1]\n",
    "\n",
    "# Visualiseer de afbeeldingen\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "images = [(afb_3_a, '3 (a)'), (afb_3_b, '3 (b)'), (afb_8, '8'), (afb_1, '1')]\n",
    "\n",
    "for ax, (img, label) in zip(axes, images):\n",
    "    ax.imshow(img.reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f'Cijfer {label}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereken dot products\n",
    "print(\"Dot products tussen MNIST afbeeldingen:\")\n",
    "print()\n",
    "\n",
    "dot_3a_3a = afb_3_a @ afb_3_a\n",
    "dot_3a_3b = afb_3_a @ afb_3_b\n",
    "dot_3a_8 = afb_3_a @ afb_8\n",
    "dot_3a_1 = afb_3_a @ afb_1\n",
    "\n",
    "print(f\"3(a) met zichzelf: {dot_3a_3a:,.0f}\")\n",
    "print(f\"3(a) met 3(b):     {dot_3a_3b:,.0f}\")\n",
    "print(f\"3(a) met 8:        {dot_3a_8:,.0f}\")\n",
    "print(f\"3(a) met 1:        {dot_3a_1:,.0f}\")\n",
    "print()\n",
    "print(\"Het dot product met zichzelf is het grootst (maximale gelijkenis).\")\n",
    "print(\"De dot producten zijn moeilijk te vergelijken omdat de afbeeldingen\")\n",
    "print(\"verschillende 'helderheden' kunnen hebben.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity: genormaliseerde gelijkenis\n",
    "\n",
    "Om een eerlijkere vergelijking te maken, gebruiken we cosine similarity. Dit normaliseert voor de lengte van de vectoren en geeft een waarde tussen -1 en 1:\n",
    "\n",
    "cosine_similarity(u, v) = (u · v) / (||u|| × ||v||)\n",
    "\n",
    "Een waarde van 1 betekent dat de vectoren in exact dezelfde richting wijzen, 0 betekent dat ze loodrecht staan, en -1 betekent dat ze in tegengestelde richting wijzen. Voor afbeeldingen met alleen positieve pixelwaarden zal de cosine similarity altijd tussen 0 en 1 liggen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"Bereken de cosine similarity tussen twee vectoren.\"\"\"\n",
    "    return (u @ v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "print(\"Cosine similarity tussen MNIST afbeeldingen:\")\n",
    "print()\n",
    "\n",
    "sim_3a_3a = cosine_similarity(afb_3_a, afb_3_a)\n",
    "sim_3a_3b = cosine_similarity(afb_3_a, afb_3_b)\n",
    "sim_3a_8 = cosine_similarity(afb_3_a, afb_8)\n",
    "sim_3a_1 = cosine_similarity(afb_3_a, afb_1)\n",
    "\n",
    "print(f\"3(a) met zichzelf: {sim_3a_3a:.4f} (perfecte gelijkenis)\")\n",
    "print(f\"3(a) met 3(b):     {sim_3a_3b:.4f} (beide drieën)\")\n",
    "print(f\"3(a) met 8:        {sim_3a_8:.4f} (verschillende cijfers)\")\n",
    "print(f\"3(a) met 1:        {sim_3a_1:.4f} (zeer verschillend)\")\n",
    "print()\n",
    "print(\"De 3 lijkt meer op een andere 3 dan op een 8 of 1!\")\n",
    "print(\"Dit is de basis van patroonherkenning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een heatmap van gelijkenis tussen gemiddelde cijfers\n",
    "print(\"Berekenen van gemiddelde afbeelding per cijfer...\")\n",
    "\n",
    "gemiddelde_cijfers = []\n",
    "for cijfer in range(10):\n",
    "    mask = y_mnist == cijfer\n",
    "    gemiddelde = X_mnist[mask].mean(axis=0)\n",
    "    gemiddelde_cijfers.append(gemiddelde)\n",
    "\n",
    "gemiddelde_cijfers = np.array(gemiddelde_cijfers)\n",
    "print(f\"Shape: {gemiddelde_cijfers.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereken similarity matrix\n",
    "similarity_matrix = np.zeros((10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        similarity_matrix[i, j] = cosine_similarity(gemiddelde_cijfers[i], gemiddelde_cijfers[j])\n",
    "\n",
    "# Visualiseer\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gemiddelde cijfers\n",
    "for i in range(10):\n",
    "    ax_sub = fig.add_axes([0.02 + i*0.045, 0.55, 0.04, 0.35])\n",
    "    ax_sub.imshow(gemiddelde_cijfers[i].reshape(28, 28), cmap='gray')\n",
    "    ax_sub.set_title(str(i), fontsize=10)\n",
    "    ax_sub.axis('off')\n",
    "\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Gemiddelde afbeelding per cijfer', fontsize=14, pad=80)\n",
    "\n",
    "# Heatmap\n",
    "im = axes[1].imshow(similarity_matrix, cmap='YlOrRd', vmin=0.5, vmax=1)\n",
    "axes[1].set_xticks(range(10))\n",
    "axes[1].set_yticks(range(10))\n",
    "axes[1].set_xlabel('Cijfer', fontsize=12)\n",
    "axes[1].set_ylabel('Cijfer', fontsize=12)\n",
    "axes[1].set_title('Cosine similarity tussen gemiddelde cijfers', fontsize=14)\n",
    "plt.colorbar(im, ax=axes[1], label='Similarity')\n",
    "\n",
    "# Voeg waarden toe aan de cellen\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        color = 'white' if similarity_matrix[i, j] > 0.75 else 'black'\n",
    "        axes[1].text(j, i, f'{similarity_matrix[i, j]:.2f}', \n",
    "                    ha='center', va='center', color=color, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vind de meest en minst gelijkende paren\n",
    "print(\"Analyse van de similarity matrix:\\n\")\n",
    "\n",
    "# Maak een lijst van alle paren (zonder diagonaal en duplicaten)\n",
    "paren = []\n",
    "for i in range(10):\n",
    "    for j in range(i+1, 10):\n",
    "        paren.append((i, j, similarity_matrix[i, j]))\n",
    "\n",
    "# Sorteer op similarity\n",
    "paren_gesorteerd = sorted(paren, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"Meest gelijkende paren:\")\n",
    "for i, j, sim in paren_gesorteerd[:5]:\n",
    "    print(f\"  {i} en {j}: {sim:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Minst gelijkende paren:\")\n",
    "for i, j, sim in paren_gesorteerd[-5:]:\n",
    "    print(f\"  {i} en {j}: {sim:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Dit verklaart waarom sommige cijfers vaker worden verward dan andere!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Samenvatting en Vooruitblik\n",
    "\n",
    "### Kernconcepten van deze les\n",
    "\n",
    "We hebben de fundamentele datastructuren van lineaire algebra verkend. Een scalar is een enkel getal, een vector is een geordende lijst van getallen, een matrix is een 2D-tabel, en een tensor generaliseert naar hogere dimensies. Deze structuren stellen ons in staat om data zoals tabellen, afbeeldingen, video's en tekst wiskundig te representeren.\n",
    "\n",
    "We hebben gezien hoe verschillende soorten data worden omgezet naar tensoren. Gestructureerde data wordt een 2D-matrix met rijen voor voorbeelden en kolommen voor kenmerken. Grijswaarden afbeeldingen zijn 2D-matrices, kleurenafbeeldingen zijn 3D-tensors, en video's zijn 4D-tensors. Tekst wordt via one-hot encoding of word embeddings omgezet naar matrices.\n",
    "\n",
    "Vectoroperaties zoals optelling, scalaire vermenigvuldiging en het berekenen van de norm geven ons tools om met deze data te werken. Het dot product is bijzonder belangrijk: het combineert twee vectoren tot een scalar en vormt de basis van berekeningen in neurale netwerken. De geometrische interpretatie van het dot product als maat voor gelijkenis tussen richtingen is essentieel voor het begrijpen van hoe neurale netwerken patronen herkennen.\n",
    "\n",
    "### Link naar het neurale netwerk\n",
    "\n",
    "De input van ons MNIST-netwerk is een vector van 784 pixels. Elk neuron in de eerste laag heeft 784 gewichten, ook een vector. De output van dat neuron is het dot product van input en gewichten, plus een bias. Dit proces herhaalt zich door alle lagen van het netwerk. In essentie is elk neuron een patroonherkennner die meet hoe goed de input overeenkomt met de geleerde gewichten.\n",
    "\n",
    "### Volgende les\n",
    "\n",
    "In les 3 breiden we uit naar matrixoperaties. We leren hoe matrixvermenigvuldiging werkt en hoe dit ons in staat stelt om een hele laag van het netwerk in één operatie te berekenen. We introduceren ook het concept van lineaire transformaties en bekijken speciale matrixeigenschappen zoals de inverse en determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist leerdoelen\n",
    "\n",
    "Neem even de tijd om te reflecteren over wat je hebt geleerd. Kan je de volgende vragen beantwoorden?\n",
    "\n",
    "1. Wat is het verschil tussen een scalar, vector, matrix en tensor?\n",
    "\n",
    "2. Hoe wordt een kleurenafbeelding voorgesteld als tensor? En een video?\n",
    "\n",
    "3. Wat is de geometrische betekenis van de norm van een vector?\n",
    "\n",
    "4. Wat is het dot product en waarom is het belangrijk in neurale netwerken?\n",
    "\n",
    "5. Hoe meet cosine similarity de gelijkenis tussen twee vectoren?\n",
    "\n",
    "Als je deze vragen kan beantwoorden, ben je klaar voor de volgende les!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mathematical Foundations** | Les 2 van 12 | IT & Artificial Intelligence\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
